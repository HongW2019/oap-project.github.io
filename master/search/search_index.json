{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OAP Project Overview Optimized Analytics Package (OAP) for Spark platform is a open source project for multiple Spark optimizations and features driving by Intel and the community. Spark is powerful and well optimized in a lot of aspects. But we still face a few challenges for Spark to the next level performance. The JVM and row-based computing engine preventing Spark to be fully optimized for Intel hardware, for example AVX/AVX512, GPU The current implementation of key aspects, such as memory management & shuffle didn\u2019t consider the latest technology advancements, like PMEM The batch processing engine a lot of times cannot satisfy the need of queries with high performance requirement. OAP Project is targeted to optimize Spark in these aspects above, now it has 8 components including SQL DS Cache, Native SQL Engine, Arrow Data Source, OAP MLlib, PMem Spill, PMem Common, PMem Shuffle and Remote Shuffle. Guide Please refer to the total OAP project installation and development guide below. OAP Installation Guide OAP Developer Guide You can get more detailed information from each module web page of OAP Project below, or from the left sidebar navigation. SQL DS Cache Native SQL Engine Arrow Data Source OAP MLlib PMem Shuffle Remote Shuffle PMem Spill PMem Common","title":"Home"},{"location":"#oap-project-overview","text":"Optimized Analytics Package (OAP) for Spark platform is a open source project for multiple Spark optimizations and features driving by Intel and the community. Spark is powerful and well optimized in a lot of aspects. But we still face a few challenges for Spark to the next level performance. The JVM and row-based computing engine preventing Spark to be fully optimized for Intel hardware, for example AVX/AVX512, GPU The current implementation of key aspects, such as memory management & shuffle didn\u2019t consider the latest technology advancements, like PMEM The batch processing engine a lot of times cannot satisfy the need of queries with high performance requirement. OAP Project is targeted to optimize Spark in these aspects above, now it has 8 components including SQL DS Cache, Native SQL Engine, Arrow Data Source, OAP MLlib, PMem Spill, PMem Common, PMem Shuffle and Remote Shuffle.","title":"OAP Project Overview"},{"location":"#guide","text":"Please refer to the total OAP project installation and development guide below. OAP Installation Guide OAP Developer Guide You can get more detailed information from each module web page of OAP Project below, or from the left sidebar navigation. SQL DS Cache Native SQL Engine Arrow Data Source OAP MLlib PMem Shuffle Remote Shuffle PMem Spill PMem Common","title":"Guide"},{"location":"OAP-Developer-Guide/","text":"OAP Developer Guide This document contains the instructions & scripts on installing necessary dependencies and building OAP. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Arrow Data Source Native SQL Engine Building OAP Prerequisites for Building OAP is built with Apache Maven and Oracle Java 8, and mainly required tools to install on your cluster are listed below. Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. We provide scripts below to help automatically install dependencies above except RDMA , need change to root account, run: # git clone -b <version> https://github.com/oap-project/oap-tools.git # cd oap-tools # sh dev/install-compile-time-dependencies.sh Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven Building To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building Specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache","title":"OAP Developer Guide"},{"location":"OAP-Developer-Guide/#oap-developer-guide","text":"This document contains the instructions & scripts on installing necessary dependencies and building OAP. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Arrow Data Source Native SQL Engine","title":"OAP Developer Guide"},{"location":"OAP-Developer-Guide/#building-oap","text":"","title":"Building OAP"},{"location":"OAP-Developer-Guide/#prerequisites-for-building","text":"OAP is built with Apache Maven and Oracle Java 8, and mainly required tools to install on your cluster are listed below. Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. We provide scripts below to help automatically install dependencies above except RDMA , need change to root account, run: # git clone -b <version> https://github.com/oap-project/oap-tools.git # cd oap-tools # sh dev/install-compile-time-dependencies.sh Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven","title":"Prerequisites for Building"},{"location":"OAP-Developer-Guide/#building","text":"To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building Specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache","title":"Building"},{"location":"OAP-Installation-Guide/","text":"OAP Installation Guide This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine. Contents Prerequisites Installing OAP Configuration Prerequisites OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly. Installing OAP Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Create a conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.0.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Extra Steps for Shuffle Remote PMem Extension If you use one of OAP features -- PMmem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details. Configuration Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar And then you can follow the corresponding feature documents for more details to use them.","title":"OAP Installation Guide"},{"location":"OAP-Installation-Guide/#oap-installation-guide","text":"This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine.","title":"OAP Installation Guide"},{"location":"OAP-Installation-Guide/#contents","text":"Prerequisites Installing OAP Configuration","title":"Contents"},{"location":"OAP-Installation-Guide/#prerequisites","text":"OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly.","title":"Prerequisites"},{"location":"OAP-Installation-Guide/#installing-oap","text":"Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Create a conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.0.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars","title":"Installing OAP"},{"location":"OAP-Installation-Guide/#extra-steps-for-shuffle-remote-pmem-extension","text":"If you use one of OAP features -- PMmem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details.","title":"Extra Steps for Shuffle Remote PMem Extension"},{"location":"OAP-Installation-Guide/#configuration","text":"Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar And then you can follow the corresponding feature documents for more details to use them.","title":"Configuration"},{"location":"ArrowDataSource/","text":"Arrow Data Source A Spark DataSource implementation for reading files into Arrow compatible columnar vectors. Note The development of this library is still in progress. As a result some of the functionality may not be constantly stable for being used in production environments that have not been fully considered due to the limited testing capabilities so far. Build Prerequisite There are some requirements before you build the project. Please make sure you have already installed the software in your system. gcc 9.3 or higher version java8 OpenJDK -> yum install java-1.8.0-openjdk cmake 3.2 or higher version maven 3.1.1 or higher version Hadoop 2.7.5 or higher version Spark 3.0.0 or higher version Intel Optimized Arrow 0.17.0 Building by Conda If you already have a working Hadoop Spark Cluster, we provide a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. Once finished OAP-Installation-Guide , you can find built spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar under $HOME/miniconda2/envs/oapenv/oap_jars . Then you can just skip steps below and jump to Get Started . cmake installation If you are facing some trouble when installing cmake, please follow below steps to install cmake. // installing cmake 3.2 sudo yum install cmake3 // If you have an existing cmake, you can use below command to set it as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake 10 --slave /usr/local/bin/ctest ctest /usr/bin/ctest --slave /usr/local/bin/cpack cpack /usr/bin/cpack --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake --family cmake // Set cmake3 as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake3 20 --slave /usr/local/bin/ctest ctest /usr/bin/ctest3 --slave /usr/local/bin/cpack cpack /usr/bin/cpack3 --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake3 --family cmake // Use alternatives to choose cmake version sudo alternatives --config cmake maven installation If you are facing some trouble when installing maven, please follow below steps to install maven // installing maven 3.6.3 Go to https://maven.apache.org/download.cgi and download the specific version of maven // Below command use maven 3.6.3 as an example wget https://ftp.wayne.edu/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz tar xzf apache-maven-3.6.3-bin.tar.gz mkdir /usr/local/maven mv apache-maven-3.6.3/ /usr/local/maven/ // Set maven 3.6.3 as an option within alternatives command sudo alternatives --install /usr/bin/mvn mvn /usr/local/maven/apache-maven-3.6.3/bin/mvn 1 // Use alternatives to choose mvn version sudo alternatives --config mvn Hadoop Native Library(Default) Please make sure you have set up Hadoop directory properly with Hadoop Native Libraries By default, Apache Arrow would scan $HADOOP_HOME and find the native Hadoop library libhdfs.so (under $HADOOP_HOME/lib/native directory) to be used for Hadoop client. You can also use ARROW_LIBHDFS_DIR to configure the location of libhdfs.so if it is installed in other directory than $HADOOP_HOME/lib/native If your SPARK and HADOOP are separated in different nodes, please find libhdfs.so in your Hadoop cluster and copy it to SPARK cluster, then use one of the above methods to set it properly. For more information, please check Arrow HDFS interface documentation Hadoop Native Library, please read the official Hadoop website documentation Use libhdfs3 library for better performance(Optional) For better performance ArrowDataSource reads HDFS files using the third-party library libhdfs3 . The library must be pre-installed on machines Spark Executor nodes are running on. To install the library, use of Conda is recommended. // installing libhdfs3 conda install -c conda-forge libhdfs3 // check the installed library file ll ~/miniconda/envs/$(YOUR_ENV_NAME)/lib/libhdfs3.so To set up libhdfs3, there are two different ways: Option1: Overwrite the soft link for libhdfs.so To install libhdfs3.so, you have to create a soft link for libhdfs.so in your Hadoop directory( $HADOOP_HOME/lib/native by default). ln -f -s libhdfs3.so libhdfs.so Option2: Add env variable to the system export ARROW_LIBHDFS3_DIR=\"PATH_TO_LIBHDFS3_DIR/\" Add following Spark configuration options before running the DataSource to make the library to be recognized: * spark.executorEnv.ARROW_LIBHDFS3_DIR = \"PATH_TO_LIBHDFS3_DIR/\" * spark.executorEnv.LD_LIBRARY_PATH = \"PATH_TO_LIBHDFS3_DEPENDENCIES_DIR/\" Please notes: If you choose to use libhdfs3.so, there are some other dependency libraries you have to installed such as libprotobuf or libcrypto. Build and install Intel\u00ae Optimized Arrow with Datasets Java API You have to use a customized Arrow to support for our datasets Java API. // build arrow-cpp git clone -b <version> https://github.com/Intel-bigdata/arrow.git cd arrow/cpp mkdir build cd build cmake -DARROW_DEPENDENCY_SOURCE=BUNDLED -DARROW_GANDIVA_JAVA=ON -DARROW_GANDIVA=ON -DARROW_PARQUET=ON -DARROW_HDFS=ON -DARROW_BOOST_USE_SHARED=ON -DARROW_JNI=ON -DARROW_DATASET=ON -DARROW_WITH_PROTOBUF=ON -DARROW_WITH_SNAPPY=ON -DARROW_WITH_LZ4=ON -DARROW_FILESYSTEM=ON -DARROW_JSON=ON .. make // build and install arrow jvm library cd ../../java mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=$PATH_TO_ARROW_SOURCE_CODE/arrow/cpp/build/release Build Arrow Data Source Library // Download Arrow Data Source Code git clone -b <version> https://github.com/oap-project/arrow-data-source.git // Go to the directory cd arrow-data-source // build mvn clean -DskipTests package // check built jar library readlink -f standard/target/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar Download Spark 3.0.0 Currently ArrowDataSource works on the Spark 3.0.0 version. wget http://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz tar -xf ./spark-3.0.0-bin-hadoop2.7.tgz export SPARK_HOME=`pwd`/spark-3.0.0-bin-hadoop2.7 If you are new to Apache Spark, please go though Spark's official deploying guide before getting started with ArrowDataSource. Get started Add extra class pathes to Spark To enable ArrowDataSource, the previous built jar spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar should be added to Spark configuration. Typically the options are: spark.driver.extraClassPath : Set to load jar file to driver. spark.executor.extraClassPath : Set to load jar file to executor. jars : Set to copy jar file to the executors when using yarn cluster mode. spark.executorEnv.ARROW_LIBHDFS3_DIR : Optional if you are using a custom libhdfs3.so. spark.executorEnv.LD_LIBRARY_PATH : Optional if you are using a custom libhdfs3.so. For Spark Standalone Mode, please set the above value as relative path to the jar file. For Spark Yarn Cluster Mode, please set the above value as absolute path to the jar file. Example to run Spark Shell with ArrowDataSource jar file ${SPARK_HOME}/bin/spark-shell \\ --verbose \\ --master yarn \\ --driver-memory 10G \\ --conf spark.driver.extraClassPath=$PATH_TO_DATASOURCE_DIR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar \\ --conf spark.executor.extraClassPath=$PATH_TO_DATASOURCE_DIR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar \\ --conf spark.driver.cores=1 \\ --conf spark.executor.instances=12 \\ --conf spark.executor.cores=6 \\ --conf spark.executor.memory=20G \\ --conf spark.memory.offHeap.size=80G \\ --conf spark.task.cpus=1 \\ --conf spark.locality.wait=0s \\ --conf spark.sql.shuffle.partitions=72 \\ --conf spark.executorEnv.ARROW_LIBHDFS3_DIR=\"$PATH_TO_LIBHDFS3_DIR/\" \\ --conf spark.executorEnv.LD_LIBRARY_PATH=\"$PATH_TO_LIBHDFS3_DEPENDENCIES_DIR\" For more information about these options, please read the official Spark documentation . Run a query with ArrowDataSource (Scala) val path = \"${PATH_TO_YOUR_PARQUET_FILE}\" val df = spark.read .option(ArrowOptions.KEY_ORIGINAL_FORMAT, \"parquet\") .option(ArrowOptions.KEY_FILESYSTEM, \"hdfs\") .format(\"arrow\") .load(path) df.createOrReplaceTempView(\"my_temp_view\") spark.sql(\"SELECT * FROM my_temp_view LIMIT 10\").show(10) To validate if ArrowDataSource works properly To validate if ArrowDataSource works, you can go to the DAG to check if ArrowScan has been used from the above example query. Work together with ParquetDataSource (experimental) We provide a customized replacement of Spark's built-in ParquetFileFormat. By so users don't have to change existing Parquet-based SQL/code and will be able to read Arrow data from Parquet directly. More importantly, sometimes the feature could be extremely helpful to make ArrowDataSource work correctly with some 3rd-party storage tools (e.g. Delta Lake ) that are built on top of ParquetDataSource. To replace built-in ParquetDataSource, the only thing has to be done is to place compiled jar spark-arrow-datasource-parquet-<version>.jar into Spark's library folder. If you'd like to verify that ParquetDataSource is successfully overwritten by the jar, run following code before executing SQL job: ServiceLoaderUtil.ensureParquetFileFormatOverwritten(); Note the whole feature is currently experimental and only DataSource v1 is supported. V2 support is being planned.","title":"Arrow Data Source"},{"location":"ArrowDataSource/#arrow-data-source","text":"A Spark DataSource implementation for reading files into Arrow compatible columnar vectors.","title":"Arrow Data Source"},{"location":"ArrowDataSource/#note","text":"The development of this library is still in progress. As a result some of the functionality may not be constantly stable for being used in production environments that have not been fully considered due to the limited testing capabilities so far.","title":"Note"},{"location":"ArrowDataSource/#build","text":"","title":"Build"},{"location":"ArrowDataSource/#prerequisite","text":"There are some requirements before you build the project. Please make sure you have already installed the software in your system. gcc 9.3 or higher version java8 OpenJDK -> yum install java-1.8.0-openjdk cmake 3.2 or higher version maven 3.1.1 or higher version Hadoop 2.7.5 or higher version Spark 3.0.0 or higher version Intel Optimized Arrow 0.17.0","title":"Prerequisite"},{"location":"ArrowDataSource/#building-by-conda","text":"If you already have a working Hadoop Spark Cluster, we provide a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. Once finished OAP-Installation-Guide , you can find built spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar under $HOME/miniconda2/envs/oapenv/oap_jars . Then you can just skip steps below and jump to Get Started .","title":"Building by Conda"},{"location":"ArrowDataSource/#cmake-installation","text":"If you are facing some trouble when installing cmake, please follow below steps to install cmake. // installing cmake 3.2 sudo yum install cmake3 // If you have an existing cmake, you can use below command to set it as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake 10 --slave /usr/local/bin/ctest ctest /usr/bin/ctest --slave /usr/local/bin/cpack cpack /usr/bin/cpack --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake --family cmake // Set cmake3 as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake3 20 --slave /usr/local/bin/ctest ctest /usr/bin/ctest3 --slave /usr/local/bin/cpack cpack /usr/bin/cpack3 --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake3 --family cmake // Use alternatives to choose cmake version sudo alternatives --config cmake","title":"cmake installation"},{"location":"ArrowDataSource/#maven-installation","text":"If you are facing some trouble when installing maven, please follow below steps to install maven // installing maven 3.6.3 Go to https://maven.apache.org/download.cgi and download the specific version of maven // Below command use maven 3.6.3 as an example wget https://ftp.wayne.edu/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz tar xzf apache-maven-3.6.3-bin.tar.gz mkdir /usr/local/maven mv apache-maven-3.6.3/ /usr/local/maven/ // Set maven 3.6.3 as an option within alternatives command sudo alternatives --install /usr/bin/mvn mvn /usr/local/maven/apache-maven-3.6.3/bin/mvn 1 // Use alternatives to choose mvn version sudo alternatives --config mvn","title":"maven installation"},{"location":"ArrowDataSource/#hadoop-native-librarydefault","text":"Please make sure you have set up Hadoop directory properly with Hadoop Native Libraries By default, Apache Arrow would scan $HADOOP_HOME and find the native Hadoop library libhdfs.so (under $HADOOP_HOME/lib/native directory) to be used for Hadoop client. You can also use ARROW_LIBHDFS_DIR to configure the location of libhdfs.so if it is installed in other directory than $HADOOP_HOME/lib/native If your SPARK and HADOOP are separated in different nodes, please find libhdfs.so in your Hadoop cluster and copy it to SPARK cluster, then use one of the above methods to set it properly. For more information, please check Arrow HDFS interface documentation Hadoop Native Library, please read the official Hadoop website documentation","title":"Hadoop Native Library(Default)"},{"location":"ArrowDataSource/#use-libhdfs3-library-for-better-performanceoptional","text":"For better performance ArrowDataSource reads HDFS files using the third-party library libhdfs3 . The library must be pre-installed on machines Spark Executor nodes are running on. To install the library, use of Conda is recommended. // installing libhdfs3 conda install -c conda-forge libhdfs3 // check the installed library file ll ~/miniconda/envs/$(YOUR_ENV_NAME)/lib/libhdfs3.so To set up libhdfs3, there are two different ways: Option1: Overwrite the soft link for libhdfs.so To install libhdfs3.so, you have to create a soft link for libhdfs.so in your Hadoop directory( $HADOOP_HOME/lib/native by default). ln -f -s libhdfs3.so libhdfs.so Option2: Add env variable to the system export ARROW_LIBHDFS3_DIR=\"PATH_TO_LIBHDFS3_DIR/\" Add following Spark configuration options before running the DataSource to make the library to be recognized: * spark.executorEnv.ARROW_LIBHDFS3_DIR = \"PATH_TO_LIBHDFS3_DIR/\" * spark.executorEnv.LD_LIBRARY_PATH = \"PATH_TO_LIBHDFS3_DEPENDENCIES_DIR/\" Please notes: If you choose to use libhdfs3.so, there are some other dependency libraries you have to installed such as libprotobuf or libcrypto.","title":"Use libhdfs3 library for better performance(Optional)"},{"location":"ArrowDataSource/#build-and-install-intel-optimized-arrow-with-datasets-java-api","text":"You have to use a customized Arrow to support for our datasets Java API. // build arrow-cpp git clone -b <version> https://github.com/Intel-bigdata/arrow.git cd arrow/cpp mkdir build cd build cmake -DARROW_DEPENDENCY_SOURCE=BUNDLED -DARROW_GANDIVA_JAVA=ON -DARROW_GANDIVA=ON -DARROW_PARQUET=ON -DARROW_HDFS=ON -DARROW_BOOST_USE_SHARED=ON -DARROW_JNI=ON -DARROW_DATASET=ON -DARROW_WITH_PROTOBUF=ON -DARROW_WITH_SNAPPY=ON -DARROW_WITH_LZ4=ON -DARROW_FILESYSTEM=ON -DARROW_JSON=ON .. make // build and install arrow jvm library cd ../../java mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=$PATH_TO_ARROW_SOURCE_CODE/arrow/cpp/build/release","title":"Build and install Intel\u00ae Optimized Arrow with Datasets Java API"},{"location":"ArrowDataSource/#build-arrow-data-source-library","text":"// Download Arrow Data Source Code git clone -b <version> https://github.com/oap-project/arrow-data-source.git // Go to the directory cd arrow-data-source // build mvn clean -DskipTests package // check built jar library readlink -f standard/target/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar","title":"Build Arrow Data Source Library"},{"location":"ArrowDataSource/#download-spark-300","text":"Currently ArrowDataSource works on the Spark 3.0.0 version. wget http://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz tar -xf ./spark-3.0.0-bin-hadoop2.7.tgz export SPARK_HOME=`pwd`/spark-3.0.0-bin-hadoop2.7 If you are new to Apache Spark, please go though Spark's official deploying guide before getting started with ArrowDataSource.","title":"Download Spark 3.0.0"},{"location":"ArrowDataSource/#get-started","text":"","title":"Get started"},{"location":"ArrowDataSource/#add-extra-class-pathes-to-spark","text":"To enable ArrowDataSource, the previous built jar spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar should be added to Spark configuration. Typically the options are: spark.driver.extraClassPath : Set to load jar file to driver. spark.executor.extraClassPath : Set to load jar file to executor. jars : Set to copy jar file to the executors when using yarn cluster mode. spark.executorEnv.ARROW_LIBHDFS3_DIR : Optional if you are using a custom libhdfs3.so. spark.executorEnv.LD_LIBRARY_PATH : Optional if you are using a custom libhdfs3.so. For Spark Standalone Mode, please set the above value as relative path to the jar file. For Spark Yarn Cluster Mode, please set the above value as absolute path to the jar file. Example to run Spark Shell with ArrowDataSource jar file ${SPARK_HOME}/bin/spark-shell \\ --verbose \\ --master yarn \\ --driver-memory 10G \\ --conf spark.driver.extraClassPath=$PATH_TO_DATASOURCE_DIR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar \\ --conf spark.executor.extraClassPath=$PATH_TO_DATASOURCE_DIR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar \\ --conf spark.driver.cores=1 \\ --conf spark.executor.instances=12 \\ --conf spark.executor.cores=6 \\ --conf spark.executor.memory=20G \\ --conf spark.memory.offHeap.size=80G \\ --conf spark.task.cpus=1 \\ --conf spark.locality.wait=0s \\ --conf spark.sql.shuffle.partitions=72 \\ --conf spark.executorEnv.ARROW_LIBHDFS3_DIR=\"$PATH_TO_LIBHDFS3_DIR/\" \\ --conf spark.executorEnv.LD_LIBRARY_PATH=\"$PATH_TO_LIBHDFS3_DEPENDENCIES_DIR\" For more information about these options, please read the official Spark documentation .","title":"Add extra class pathes to Spark"},{"location":"ArrowDataSource/#run-a-query-with-arrowdatasource-scala","text":"val path = \"${PATH_TO_YOUR_PARQUET_FILE}\" val df = spark.read .option(ArrowOptions.KEY_ORIGINAL_FORMAT, \"parquet\") .option(ArrowOptions.KEY_FILESYSTEM, \"hdfs\") .format(\"arrow\") .load(path) df.createOrReplaceTempView(\"my_temp_view\") spark.sql(\"SELECT * FROM my_temp_view LIMIT 10\").show(10)","title":"Run a query with ArrowDataSource (Scala)"},{"location":"ArrowDataSource/#to-validate-if-arrowdatasource-works-properly","text":"To validate if ArrowDataSource works, you can go to the DAG to check if ArrowScan has been used from the above example query.","title":"To validate if ArrowDataSource works properly"},{"location":"ArrowDataSource/#work-together-with-parquetdatasource-experimental","text":"We provide a customized replacement of Spark's built-in ParquetFileFormat. By so users don't have to change existing Parquet-based SQL/code and will be able to read Arrow data from Parquet directly. More importantly, sometimes the feature could be extremely helpful to make ArrowDataSource work correctly with some 3rd-party storage tools (e.g. Delta Lake ) that are built on top of ParquetDataSource. To replace built-in ParquetDataSource, the only thing has to be done is to place compiled jar spark-arrow-datasource-parquet-<version>.jar into Spark's library folder. If you'd like to verify that ParquetDataSource is successfully overwritten by the jar, run following code before executing SQL job: ServiceLoaderUtil.ensureParquetFileFormatOverwritten(); Note the whole feature is currently experimental and only DataSource v1 is supported. V2 support is being planned.","title":"Work together with ParquetDataSource (experimental)"},{"location":"ArrowDataSource/OAP-Developer-Guide/","text":"OAP Developer Guide This document contains the instructions & scripts on installing necessary dependencies and building OAP. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Arrow Data Source Native SQL Engine Building OAP Prerequisites for Building OAP is built with Apache Maven and Oracle Java 8, and mainly required tools to install on your cluster are listed below. Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. We provide scripts below to help automatically install dependencies above except RDMA , need change to root account, run: # git clone -b <tag-version> # cd oap-tools # sh dev/install-compile-time-dependencies.sh Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven Building To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building Specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache","title":"OAP Developer Guide"},{"location":"ArrowDataSource/OAP-Developer-Guide/#oap-developer-guide","text":"This document contains the instructions & scripts on installing necessary dependencies and building OAP. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Arrow Data Source Native SQL Engine","title":"OAP Developer Guide"},{"location":"ArrowDataSource/OAP-Developer-Guide/#building-oap","text":"","title":"Building OAP"},{"location":"ArrowDataSource/OAP-Developer-Guide/#prerequisites-for-building","text":"OAP is built with Apache Maven and Oracle Java 8, and mainly required tools to install on your cluster are listed below. Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. We provide scripts below to help automatically install dependencies above except RDMA , need change to root account, run: # git clone -b <tag-version> # cd oap-tools # sh dev/install-compile-time-dependencies.sh Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven","title":"Prerequisites for Building"},{"location":"ArrowDataSource/OAP-Developer-Guide/#building","text":"To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building Specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache","title":"Building"},{"location":"ArrowDataSource/OAP-Installation-Guide/","text":"OAP Installation Guide This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine. Contents Prerequisites Installing OAP Configuration Prerequisites OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly. Installing OAP Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Create a conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.0.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Extra Steps for Shuffle Remote PMem Extension If you use one of OAP features -- PMmem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details. Configuration Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar And then you can follow the corresponding feature documents for more details to use them.","title":"OAP Installation Guide"},{"location":"ArrowDataSource/OAP-Installation-Guide/#oap-installation-guide","text":"This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine.","title":"OAP Installation Guide"},{"location":"ArrowDataSource/OAP-Installation-Guide/#contents","text":"Prerequisites Installing OAP Configuration","title":"Contents"},{"location":"ArrowDataSource/OAP-Installation-Guide/#prerequisites","text":"OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly.","title":"Prerequisites"},{"location":"ArrowDataSource/OAP-Installation-Guide/#installing-oap","text":"Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Create a conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.0.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars","title":"Installing OAP"},{"location":"ArrowDataSource/OAP-Installation-Guide/#extra-steps-for-shuffle-remote-pmem-extension","text":"If you use one of OAP features -- PMmem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details.","title":"Extra Steps for Shuffle Remote PMem Extension"},{"location":"ArrowDataSource/OAP-Installation-Guide/#configuration","text":"Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar And then you can follow the corresponding feature documents for more details to use them.","title":"Configuration"},{"location":"ArrowDataSource/User-Guide/","text":"Arrow Data Source A Spark DataSource implementation for reading files into Arrow compatible columnar vectors. Note The development of this library is still in progress. As a result some of the functionality may not be constantly stable for being used in production environments that have not been fully considered due to the limited testing capabilities so far. Build Prerequisite There are some requirements before you build the project. Please make sure you have already installed the software in your system. gcc 9.3 or higher version java8 OpenJDK -> yum install java-1.8.0-openjdk cmake 3.2 or higher version maven 3.1.1 or higher version Hadoop 2.7.5 or higher version Spark 3.0.0 or higher version Intel Optimized Arrow 0.17.0 Building by Conda If you already have a working Hadoop Spark Cluster, we provide a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. Once finished OAP-Installation-Guide , you can find built spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar under $HOME/miniconda2/envs/oapenv/oap_jars . Then you can just skip steps below and jump to Get Started . cmake installation If you are facing some trouble when installing cmake, please follow below steps to install cmake. // installing cmake 3.2 sudo yum install cmake3 // If you have an existing cmake, you can use below command to set it as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake 10 --slave /usr/local/bin/ctest ctest /usr/bin/ctest --slave /usr/local/bin/cpack cpack /usr/bin/cpack --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake --family cmake // Set cmake3 as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake3 20 --slave /usr/local/bin/ctest ctest /usr/bin/ctest3 --slave /usr/local/bin/cpack cpack /usr/bin/cpack3 --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake3 --family cmake // Use alternatives to choose cmake version sudo alternatives --config cmake maven installation If you are facing some trouble when installing maven, please follow below steps to install maven // installing maven 3.6.3 Go to https://maven.apache.org/download.cgi and download the specific version of maven // Below command use maven 3.6.3 as an example wget https://ftp.wayne.edu/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz tar xzf apache-maven-3.6.3-bin.tar.gz mkdir /usr/local/maven mv apache-maven-3.6.3/ /usr/local/maven/ // Set maven 3.6.3 as an option within alternatives command sudo alternatives --install /usr/bin/mvn mvn /usr/local/maven/apache-maven-3.6.3/bin/mvn 1 // Use alternatives to choose mvn version sudo alternatives --config mvn Hadoop Native Library(Default) Please make sure you have set up Hadoop directory properly with Hadoop Native Libraries By default, Apache Arrow would scan $HADOOP_HOME and find the native Hadoop library libhdfs.so (under $HADOOP_HOME/lib/native directory) to be used for Hadoop client. You can also use ARROW_LIBHDFS_DIR to configure the location of libhdfs.so if it is installed in other directory than $HADOOP_HOME/lib/native If your SPARK and HADOOP are separated in different nodes, please find libhdfs.so in your Hadoop cluster and copy it to SPARK cluster, then use one of the above methods to set it properly. For more information, please check Arrow HDFS interface documentation Hadoop Native Library, please read the official Hadoop website documentation Use libhdfs3 library for better performance(Optional) For better performance ArrowDataSource reads HDFS files using the third-party library libhdfs3 . The library must be pre-installed on machines Spark Executor nodes are running on. To install the library, use of Conda is recommended. // installing libhdfs3 conda install -c conda-forge libhdfs3 // check the installed library file ll ~/miniconda/envs/$(YOUR_ENV_NAME)/lib/libhdfs3.so To set up libhdfs3, there are two different ways: Option1: Overwrite the soft link for libhdfs.so To install libhdfs3.so, you have to create a soft link for libhdfs.so in your Hadoop directory( $HADOOP_HOME/lib/native by default). ln -f -s libhdfs3.so libhdfs.so Option2: Add env variable to the system export ARROW_LIBHDFS3_DIR=\"PATH_TO_LIBHDFS3_DIR/\" Add following Spark configuration options before running the DataSource to make the library to be recognized: * spark.executorEnv.ARROW_LIBHDFS3_DIR = \"PATH_TO_LIBHDFS3_DIR/\" * spark.executorEnv.LD_LIBRARY_PATH = \"PATH_TO_LIBHDFS3_DEPENDENCIES_DIR/\" Please notes: If you choose to use libhdfs3.so, there are some other dependency libraries you have to installed such as libprotobuf or libcrypto. Build and install Intel\u00ae Optimized Arrow with Datasets Java API You have to use a customized Arrow to support for our datasets Java API. // build arrow-cpp git clone -b <version> https://github.com/Intel-bigdata/arrow.git cd arrow/cpp mkdir build cd build cmake -DARROW_DEPENDENCY_SOURCE=BUNDLED -DARROW_GANDIVA_JAVA=ON -DARROW_GANDIVA=ON -DARROW_PARQUET=ON -DARROW_HDFS=ON -DARROW_BOOST_USE_SHARED=ON -DARROW_JNI=ON -DARROW_DATASET=ON -DARROW_WITH_PROTOBUF=ON -DARROW_WITH_SNAPPY=ON -DARROW_WITH_LZ4=ON -DARROW_FILESYSTEM=ON -DARROW_JSON=ON .. make // build and install arrow jvm library cd ../../java mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=$PATH_TO_ARROW_SOURCE_CODE/arrow/cpp/build/release Build Arrow Data Source Library // Download Arrow Data Source Code git clone -b <version> https://github.com/oap-project/arrow-data-source.git // Go to the directory cd arrow-data-source // build mvn clean -DskipTests package // check built jar library readlink -f standard/target/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar Download Spark 3.0.0 Currently ArrowDataSource works on the Spark 3.0.0 version. wget http://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz tar -xf ./spark-3.0.0-bin-hadoop2.7.tgz export SPARK_HOME=`pwd`/spark-3.0.0-bin-hadoop2.7 If you are new to Apache Spark, please go though Spark's official deploying guide before getting started with ArrowDataSource. Get started Add extra class pathes to Spark To enable ArrowDataSource, the previous built jar spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar should be added to Spark configuration. Typically the options are: spark.driver.extraClassPath : Set to load jar file to driver. spark.executor.extraClassPath : Set to load jar file to executor. jars : Set to copy jar file to the executors when using yarn cluster mode. spark.executorEnv.ARROW_LIBHDFS3_DIR : Optional if you are using a custom libhdfs3.so. spark.executorEnv.LD_LIBRARY_PATH : Optional if you are using a custom libhdfs3.so. For Spark Standalone Mode, please set the above value as relative path to the jar file. For Spark Yarn Cluster Mode, please set the above value as absolute path to the jar file. Example to run Spark Shell with ArrowDataSource jar file ${SPARK_HOME}/bin/spark-shell \\ --verbose \\ --master yarn \\ --driver-memory 10G \\ --conf spark.driver.extraClassPath=$PATH_TO_DATASOURCE_DIR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar \\ --conf spark.executor.extraClassPath=$PATH_TO_DATASOURCE_DIR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar \\ --conf spark.driver.cores=1 \\ --conf spark.executor.instances=12 \\ --conf spark.executor.cores=6 \\ --conf spark.executor.memory=20G \\ --conf spark.memory.offHeap.size=80G \\ --conf spark.task.cpus=1 \\ --conf spark.locality.wait=0s \\ --conf spark.sql.shuffle.partitions=72 \\ --conf spark.executorEnv.ARROW_LIBHDFS3_DIR=\"$PATH_TO_LIBHDFS3_DIR/\" \\ --conf spark.executorEnv.LD_LIBRARY_PATH=\"$PATH_TO_LIBHDFS3_DEPENDENCIES_DIR\" For more information about these options, please read the official Spark documentation . Run a query with ArrowDataSource (Scala) val path = \"${PATH_TO_YOUR_PARQUET_FILE}\" val df = spark.read .option(ArrowOptions.KEY_ORIGINAL_FORMAT, \"parquet\") .option(ArrowOptions.KEY_FILESYSTEM, \"hdfs\") .format(\"arrow\") .load(path) df.createOrReplaceTempView(\"my_temp_view\") spark.sql(\"SELECT * FROM my_temp_view LIMIT 10\").show(10) To validate if ArrowDataSource works properly To validate if ArrowDataSource works, you can go to the DAG to check if ArrowScan has been used from the above example query. Work together with ParquetDataSource (experimental) We provide a customized replacement of Spark's built-in ParquetFileFormat. By so users don't have to change existing Parquet-based SQL/code and will be able to read Arrow data from Parquet directly. More importantly, sometimes the feature could be extremely helpful to make ArrowDataSource work correctly with some 3rd-party storage tools (e.g. Delta Lake ) that are built on top of ParquetDataSource. To replace built-in ParquetDataSource, the only thing has to be done is to place compiled jar spark-arrow-datasource-parquet-<version>.jar into Spark's library folder. If you'd like to verify that ParquetDataSource is successfully overwritten by the jar, run following code before executing SQL job: ServiceLoaderUtil.ensureParquetFileFormatOverwritten(); Note the whole feature is currently experimental and only DataSource v1 is supported. V2 support is being planned.","title":"User Guide"},{"location":"ArrowDataSource/User-Guide/#arrow-data-source","text":"A Spark DataSource implementation for reading files into Arrow compatible columnar vectors.","title":"Arrow Data Source"},{"location":"ArrowDataSource/User-Guide/#note","text":"The development of this library is still in progress. As a result some of the functionality may not be constantly stable for being used in production environments that have not been fully considered due to the limited testing capabilities so far.","title":"Note"},{"location":"ArrowDataSource/User-Guide/#build","text":"","title":"Build"},{"location":"ArrowDataSource/User-Guide/#prerequisite","text":"There are some requirements before you build the project. Please make sure you have already installed the software in your system. gcc 9.3 or higher version java8 OpenJDK -> yum install java-1.8.0-openjdk cmake 3.2 or higher version maven 3.1.1 or higher version Hadoop 2.7.5 or higher version Spark 3.0.0 or higher version Intel Optimized Arrow 0.17.0","title":"Prerequisite"},{"location":"ArrowDataSource/User-Guide/#building-by-conda","text":"If you already have a working Hadoop Spark Cluster, we provide a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. Once finished OAP-Installation-Guide , you can find built spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar under $HOME/miniconda2/envs/oapenv/oap_jars . Then you can just skip steps below and jump to Get Started .","title":"Building by Conda"},{"location":"ArrowDataSource/User-Guide/#cmake-installation","text":"If you are facing some trouble when installing cmake, please follow below steps to install cmake. // installing cmake 3.2 sudo yum install cmake3 // If you have an existing cmake, you can use below command to set it as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake 10 --slave /usr/local/bin/ctest ctest /usr/bin/ctest --slave /usr/local/bin/cpack cpack /usr/bin/cpack --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake --family cmake // Set cmake3 as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake3 20 --slave /usr/local/bin/ctest ctest /usr/bin/ctest3 --slave /usr/local/bin/cpack cpack /usr/bin/cpack3 --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake3 --family cmake // Use alternatives to choose cmake version sudo alternatives --config cmake","title":"cmake installation"},{"location":"ArrowDataSource/User-Guide/#maven-installation","text":"If you are facing some trouble when installing maven, please follow below steps to install maven // installing maven 3.6.3 Go to https://maven.apache.org/download.cgi and download the specific version of maven // Below command use maven 3.6.3 as an example wget https://ftp.wayne.edu/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz tar xzf apache-maven-3.6.3-bin.tar.gz mkdir /usr/local/maven mv apache-maven-3.6.3/ /usr/local/maven/ // Set maven 3.6.3 as an option within alternatives command sudo alternatives --install /usr/bin/mvn mvn /usr/local/maven/apache-maven-3.6.3/bin/mvn 1 // Use alternatives to choose mvn version sudo alternatives --config mvn","title":"maven installation"},{"location":"ArrowDataSource/User-Guide/#hadoop-native-librarydefault","text":"Please make sure you have set up Hadoop directory properly with Hadoop Native Libraries By default, Apache Arrow would scan $HADOOP_HOME and find the native Hadoop library libhdfs.so (under $HADOOP_HOME/lib/native directory) to be used for Hadoop client. You can also use ARROW_LIBHDFS_DIR to configure the location of libhdfs.so if it is installed in other directory than $HADOOP_HOME/lib/native If your SPARK and HADOOP are separated in different nodes, please find libhdfs.so in your Hadoop cluster and copy it to SPARK cluster, then use one of the above methods to set it properly. For more information, please check Arrow HDFS interface documentation Hadoop Native Library, please read the official Hadoop website documentation","title":"Hadoop Native Library(Default)"},{"location":"ArrowDataSource/User-Guide/#use-libhdfs3-library-for-better-performanceoptional","text":"For better performance ArrowDataSource reads HDFS files using the third-party library libhdfs3 . The library must be pre-installed on machines Spark Executor nodes are running on. To install the library, use of Conda is recommended. // installing libhdfs3 conda install -c conda-forge libhdfs3 // check the installed library file ll ~/miniconda/envs/$(YOUR_ENV_NAME)/lib/libhdfs3.so To set up libhdfs3, there are two different ways: Option1: Overwrite the soft link for libhdfs.so To install libhdfs3.so, you have to create a soft link for libhdfs.so in your Hadoop directory( $HADOOP_HOME/lib/native by default). ln -f -s libhdfs3.so libhdfs.so Option2: Add env variable to the system export ARROW_LIBHDFS3_DIR=\"PATH_TO_LIBHDFS3_DIR/\" Add following Spark configuration options before running the DataSource to make the library to be recognized: * spark.executorEnv.ARROW_LIBHDFS3_DIR = \"PATH_TO_LIBHDFS3_DIR/\" * spark.executorEnv.LD_LIBRARY_PATH = \"PATH_TO_LIBHDFS3_DEPENDENCIES_DIR/\" Please notes: If you choose to use libhdfs3.so, there are some other dependency libraries you have to installed such as libprotobuf or libcrypto.","title":"Use libhdfs3 library for better performance(Optional)"},{"location":"ArrowDataSource/User-Guide/#build-and-install-intel-optimized-arrow-with-datasets-java-api","text":"You have to use a customized Arrow to support for our datasets Java API. // build arrow-cpp git clone -b <version> https://github.com/Intel-bigdata/arrow.git cd arrow/cpp mkdir build cd build cmake -DARROW_DEPENDENCY_SOURCE=BUNDLED -DARROW_GANDIVA_JAVA=ON -DARROW_GANDIVA=ON -DARROW_PARQUET=ON -DARROW_HDFS=ON -DARROW_BOOST_USE_SHARED=ON -DARROW_JNI=ON -DARROW_DATASET=ON -DARROW_WITH_PROTOBUF=ON -DARROW_WITH_SNAPPY=ON -DARROW_WITH_LZ4=ON -DARROW_FILESYSTEM=ON -DARROW_JSON=ON .. make // build and install arrow jvm library cd ../../java mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=$PATH_TO_ARROW_SOURCE_CODE/arrow/cpp/build/release","title":"Build and install Intel\u00ae Optimized Arrow with Datasets Java API"},{"location":"ArrowDataSource/User-Guide/#build-arrow-data-source-library","text":"// Download Arrow Data Source Code git clone -b <version> https://github.com/oap-project/arrow-data-source.git // Go to the directory cd arrow-data-source // build mvn clean -DskipTests package // check built jar library readlink -f standard/target/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar","title":"Build Arrow Data Source Library"},{"location":"ArrowDataSource/User-Guide/#download-spark-300","text":"Currently ArrowDataSource works on the Spark 3.0.0 version. wget http://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz tar -xf ./spark-3.0.0-bin-hadoop2.7.tgz export SPARK_HOME=`pwd`/spark-3.0.0-bin-hadoop2.7 If you are new to Apache Spark, please go though Spark's official deploying guide before getting started with ArrowDataSource.","title":"Download Spark 3.0.0"},{"location":"ArrowDataSource/User-Guide/#get-started","text":"","title":"Get started"},{"location":"ArrowDataSource/User-Guide/#add-extra-class-pathes-to-spark","text":"To enable ArrowDataSource, the previous built jar spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar should be added to Spark configuration. Typically the options are: spark.driver.extraClassPath : Set to load jar file to driver. spark.executor.extraClassPath : Set to load jar file to executor. jars : Set to copy jar file to the executors when using yarn cluster mode. spark.executorEnv.ARROW_LIBHDFS3_DIR : Optional if you are using a custom libhdfs3.so. spark.executorEnv.LD_LIBRARY_PATH : Optional if you are using a custom libhdfs3.so. For Spark Standalone Mode, please set the above value as relative path to the jar file. For Spark Yarn Cluster Mode, please set the above value as absolute path to the jar file. Example to run Spark Shell with ArrowDataSource jar file ${SPARK_HOME}/bin/spark-shell \\ --verbose \\ --master yarn \\ --driver-memory 10G \\ --conf spark.driver.extraClassPath=$PATH_TO_DATASOURCE_DIR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar \\ --conf spark.executor.extraClassPath=$PATH_TO_DATASOURCE_DIR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar \\ --conf spark.driver.cores=1 \\ --conf spark.executor.instances=12 \\ --conf spark.executor.cores=6 \\ --conf spark.executor.memory=20G \\ --conf spark.memory.offHeap.size=80G \\ --conf spark.task.cpus=1 \\ --conf spark.locality.wait=0s \\ --conf spark.sql.shuffle.partitions=72 \\ --conf spark.executorEnv.ARROW_LIBHDFS3_DIR=\"$PATH_TO_LIBHDFS3_DIR/\" \\ --conf spark.executorEnv.LD_LIBRARY_PATH=\"$PATH_TO_LIBHDFS3_DEPENDENCIES_DIR\" For more information about these options, please read the official Spark documentation .","title":"Add extra class pathes to Spark"},{"location":"ArrowDataSource/User-Guide/#run-a-query-with-arrowdatasource-scala","text":"val path = \"${PATH_TO_YOUR_PARQUET_FILE}\" val df = spark.read .option(ArrowOptions.KEY_ORIGINAL_FORMAT, \"parquet\") .option(ArrowOptions.KEY_FILESYSTEM, \"hdfs\") .format(\"arrow\") .load(path) df.createOrReplaceTempView(\"my_temp_view\") spark.sql(\"SELECT * FROM my_temp_view LIMIT 10\").show(10)","title":"Run a query with ArrowDataSource (Scala)"},{"location":"ArrowDataSource/User-Guide/#to-validate-if-arrowdatasource-works-properly","text":"To validate if ArrowDataSource works, you can go to the DAG to check if ArrowScan has been used from the above example query.","title":"To validate if ArrowDataSource works properly"},{"location":"ArrowDataSource/User-Guide/#work-together-with-parquetdatasource-experimental","text":"We provide a customized replacement of Spark's built-in ParquetFileFormat. By so users don't have to change existing Parquet-based SQL/code and will be able to read Arrow data from Parquet directly. More importantly, sometimes the feature could be extremely helpful to make ArrowDataSource work correctly with some 3rd-party storage tools (e.g. Delta Lake ) that are built on top of ParquetDataSource. To replace built-in ParquetDataSource, the only thing has to be done is to place compiled jar spark-arrow-datasource-parquet-<version>.jar into Spark's library folder. If you'd like to verify that ParquetDataSource is successfully overwritten by the jar, run following code before executing SQL job: ServiceLoaderUtil.ensureParquetFileFormatOverwritten(); Note the whole feature is currently experimental and only DataSource v1 is supported. V2 support is being planned.","title":"Work together with ParquetDataSource (experimental)"},{"location":"NativeSQLEngine/","text":"Spark Native SQL Engine A Native Engine for Spark SQL with vectorized SIMD optimizations Introduction Spark SQL works very well with structured row-based data. It used WholeStageCodeGen to improve the performance by Java JIT code. However Java JIT is usually not working very well on utilizing latest SIMD instructions, especially under complicated queries. Apache Arrow provided CPU-cache friendly columnar in-memory layout, its SIMD optimized kernels and LLVM based SQL engine Gandiva are also very efficient. Native SQL Engine used these technoligies and brought better performance to Spark SQL. Key Features Apache Arrow formatted intermediate data among Spark operator With Spark 27396 its possible to pass a RDD of Columnarbatch to operators. We implemented this API with Arrow columnar format. Apache Arrow based Native Readers for Parquet and other formats A native parquet reader was developed to speed up the data loading. it's based on Apache Arrow Dataset. For details please check Arrow Data Source Apache Arrow Compute/Gandiva based operators We implemented common operators based on Apache Arrow Compute and Gandiva. The SQL expression was compiled to one expression tree with protobuf and passed to native kernels. The native kernels will then evaluate the these expressions based on the input columnar batch. Native Columnar Shuffle Operator with efficient compression support We implemented columnar shuffle to improve the shuffle performance. With the columnar layout we could do very efficient data compression for different data format. Build the Plugin Building by Conda If you already have a working Hadoop Spark Cluster, we provide a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. Once finished OAP-Installation-Guide , you can find built spark-columnar-core-1.0.0-jar-with-dependencies.jar under $HOME/miniconda2/envs/oapenv/oap_jars . Then you can just skip below steps and jump to Getting Started Get Started . Building by yourself If you prefer to build from the source code on your hand, please follow below steps to set up your environment. Prerequisite There are some requirements before you build the project. Please check the document Prerequisite and make sure you have already installed the software in your system. If you are running a SPARK Cluster, please make sure all the software are installed in every single node. Installation Please check the document Installation Guide Configuration & Testing Please check the document Configuration Guide Get started To enable OAP NativeSQL Engine, the previous built jar spark-columnar-core-<version>-jar-with-dependencies.jar should be added to Spark configuration. We also recommend to use spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar . We will demonstrate an example by using both jar files. SPARK related options are: spark.driver.extraClassPath : Set to load jar file to driver. spark.executor.extraClassPath : Set to load jar file to executor. jars : Set to copy jar file to the executors when using yarn cluster mode. spark.executorEnv.ARROW_LIBHDFS3_DIR : Optional if you are using a custom libhdfs3.so. spark.executorEnv.LD_LIBRARY_PATH : Optional if you are using a custom libhdfs3.so. For Spark Standalone Mode, please set the above value as relative path to the jar file. For Spark Yarn Cluster Mode, please set the above value as absolute path to the jar file. Example to run Spark Shell with ArrowDataSource jar file ${SPARK_HOME}/bin/spark-shell \\ --verbose \\ --master yarn \\ --driver-memory 10G \\ --conf spark.driver.extraClassPath=$PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar:$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar \\ --conf spark.executor.extraClassPath=$PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar:$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar \\ --conf spark.driver.cores=1 \\ --conf spark.executor.instances=12 \\ --conf spark.executor.cores=6 \\ --conf spark.executor.memory=20G \\ --conf spark.memory.offHeap.size=80G \\ --conf spark.task.cpus=1 \\ --conf spark.locality.wait=0s \\ --conf spark.sql.shuffle.partitions=72 \\ --conf spark.executorEnv.ARROW_LIBHDFS3_DIR=\"$PATH_TO_LIBHDFS3_DIR/\" \\ --conf spark.executorEnv.LD_LIBRARY_PATH=\"$PATH_TO_LIBHDFS3_DEPENDENCIES_DIR\" --jars $PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar,$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar Here is one example to verify if native sql engine works, make sure you have TPC-H dataset. We could do a simple projection on one parquet table. For detailed testing scripts, please refer to Solution Guide . val orders = spark.read.format(\"arrow\").load(\"hdfs:////user/root/date_tpch_10/orders\") orders.createOrReplaceTempView(\"orders\") spark.sql(\"select * from orders where o_orderdate > date '1998-07-26'\").show(20000, false) The result should show up on Spark console and you can check the DAG diagram with some Columnar Processing stage. Performance data For initial microbenchmark performance, we add 10 fields up with spark, data size is 200G data Coding Style For Java code, we used google-java-format For Scala code, we used Spark Scala Format , please use scalafmt or run ./scalafmt for scala codes format For Cpp codes, we used Clang-Format, check on this link google-vim-codefmt for details. Contact chendi.xue@intel.com binwei.yang@intel.com","title":"Spark Native SQL Engine"},{"location":"NativeSQLEngine/#spark-native-sql-engine","text":"A Native Engine for Spark SQL with vectorized SIMD optimizations","title":"Spark Native SQL Engine"},{"location":"NativeSQLEngine/#introduction","text":"Spark SQL works very well with structured row-based data. It used WholeStageCodeGen to improve the performance by Java JIT code. However Java JIT is usually not working very well on utilizing latest SIMD instructions, especially under complicated queries. Apache Arrow provided CPU-cache friendly columnar in-memory layout, its SIMD optimized kernels and LLVM based SQL engine Gandiva are also very efficient. Native SQL Engine used these technoligies and brought better performance to Spark SQL.","title":"Introduction"},{"location":"NativeSQLEngine/#key-features","text":"","title":"Key Features"},{"location":"NativeSQLEngine/#apache-arrow-formatted-intermediate-data-among-spark-operator","text":"With Spark 27396 its possible to pass a RDD of Columnarbatch to operators. We implemented this API with Arrow columnar format.","title":"Apache Arrow formatted intermediate data among Spark operator"},{"location":"NativeSQLEngine/#apache-arrow-based-native-readers-for-parquet-and-other-formats","text":"A native parquet reader was developed to speed up the data loading. it's based on Apache Arrow Dataset. For details please check Arrow Data Source","title":"Apache Arrow based Native Readers for Parquet and other formats"},{"location":"NativeSQLEngine/#apache-arrow-computegandiva-based-operators","text":"We implemented common operators based on Apache Arrow Compute and Gandiva. The SQL expression was compiled to one expression tree with protobuf and passed to native kernels. The native kernels will then evaluate the these expressions based on the input columnar batch.","title":"Apache Arrow Compute/Gandiva based operators"},{"location":"NativeSQLEngine/#native-columnar-shuffle-operator-with-efficient-compression-support","text":"We implemented columnar shuffle to improve the shuffle performance. With the columnar layout we could do very efficient data compression for different data format.","title":"Native Columnar Shuffle Operator with efficient compression support"},{"location":"NativeSQLEngine/#build-the-plugin","text":"","title":"Build the Plugin"},{"location":"NativeSQLEngine/#building-by-conda","text":"If you already have a working Hadoop Spark Cluster, we provide a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. Once finished OAP-Installation-Guide , you can find built spark-columnar-core-1.0.0-jar-with-dependencies.jar under $HOME/miniconda2/envs/oapenv/oap_jars . Then you can just skip below steps and jump to Getting Started Get Started .","title":"Building by Conda"},{"location":"NativeSQLEngine/#building-by-yourself","text":"If you prefer to build from the source code on your hand, please follow below steps to set up your environment.","title":"Building by yourself"},{"location":"NativeSQLEngine/#prerequisite","text":"There are some requirements before you build the project. Please check the document Prerequisite and make sure you have already installed the software in your system. If you are running a SPARK Cluster, please make sure all the software are installed in every single node.","title":"Prerequisite"},{"location":"NativeSQLEngine/#installation","text":"Please check the document Installation Guide","title":"Installation"},{"location":"NativeSQLEngine/#configuration-testing","text":"Please check the document Configuration Guide","title":"Configuration &amp; Testing"},{"location":"NativeSQLEngine/#get-started","text":"To enable OAP NativeSQL Engine, the previous built jar spark-columnar-core-<version>-jar-with-dependencies.jar should be added to Spark configuration. We also recommend to use spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar . We will demonstrate an example by using both jar files. SPARK related options are: spark.driver.extraClassPath : Set to load jar file to driver. spark.executor.extraClassPath : Set to load jar file to executor. jars : Set to copy jar file to the executors when using yarn cluster mode. spark.executorEnv.ARROW_LIBHDFS3_DIR : Optional if you are using a custom libhdfs3.so. spark.executorEnv.LD_LIBRARY_PATH : Optional if you are using a custom libhdfs3.so. For Spark Standalone Mode, please set the above value as relative path to the jar file. For Spark Yarn Cluster Mode, please set the above value as absolute path to the jar file. Example to run Spark Shell with ArrowDataSource jar file ${SPARK_HOME}/bin/spark-shell \\ --verbose \\ --master yarn \\ --driver-memory 10G \\ --conf spark.driver.extraClassPath=$PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar:$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar \\ --conf spark.executor.extraClassPath=$PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar:$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar \\ --conf spark.driver.cores=1 \\ --conf spark.executor.instances=12 \\ --conf spark.executor.cores=6 \\ --conf spark.executor.memory=20G \\ --conf spark.memory.offHeap.size=80G \\ --conf spark.task.cpus=1 \\ --conf spark.locality.wait=0s \\ --conf spark.sql.shuffle.partitions=72 \\ --conf spark.executorEnv.ARROW_LIBHDFS3_DIR=\"$PATH_TO_LIBHDFS3_DIR/\" \\ --conf spark.executorEnv.LD_LIBRARY_PATH=\"$PATH_TO_LIBHDFS3_DEPENDENCIES_DIR\" --jars $PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar,$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar Here is one example to verify if native sql engine works, make sure you have TPC-H dataset. We could do a simple projection on one parquet table. For detailed testing scripts, please refer to Solution Guide . val orders = spark.read.format(\"arrow\").load(\"hdfs:////user/root/date_tpch_10/orders\") orders.createOrReplaceTempView(\"orders\") spark.sql(\"select * from orders where o_orderdate > date '1998-07-26'\").show(20000, false) The result should show up on Spark console and you can check the DAG diagram with some Columnar Processing stage.","title":"Get started"},{"location":"NativeSQLEngine/#performance-data","text":"For initial microbenchmark performance, we add 10 fields up with spark, data size is 200G data","title":"Performance data"},{"location":"NativeSQLEngine/#coding-style","text":"For Java code, we used google-java-format For Scala code, we used Spark Scala Format , please use scalafmt or run ./scalafmt for scala codes format For Cpp codes, we used Clang-Format, check on this link google-vim-codefmt for details.","title":"Coding Style"},{"location":"NativeSQLEngine/#contact","text":"chendi.xue@intel.com binwei.yang@intel.com","title":"Contact"},{"location":"NativeSQLEngine/ApacheArrowInstallation/","text":"llvm-7.0: Arrow Gandiva depends on LLVM, and I noticed current version strictly depends on llvm7.0 if you installed any other version rather than 7.0, it will fail. wget http://releases.llvm.org/7.0.1/llvm-7.0.1.src.tar.xz tar xf llvm-7.0.1.src.tar.xz cd llvm-7.0.1.src/ cd tools wget http://releases.llvm.org/7.0.1/cfe-7.0.1.src.tar.xz tar xf cfe-7.0.1.src.tar.xz mv cfe-7.0.1.src clang cd .. mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE=Release cmake --build . -j cmake --build . --target install # check if clang has also been compiled, if no cd tools/clang mkdir build cd build cmake .. make -j make install cmake: Arrow will download package during compiling, in order to support SSL in cmake, build cmake is optional. wget https://github.com/Kitware/CMake/releases/download/v3.15.0-rc4/cmake-3.15.0-rc4.tar.gz tar xf cmake-3.15.0-rc4.tar.gz cd cmake-3.15.0-rc4/ ./bootstrap --system-curl --parallel=64 #parallel num depends on your server core number make -j make install cmake --version cmake version 3.15.0-rc4 Apache Arrow git clone https://github.com/Intel-bigdata/arrow.git cd arrow && git checkout branch-0.17.0-oap-1.0 mkdir -p arrow/cpp/release-build cd arrow/cpp/release-build cmake -DARROW_DEPENDENCY_SOURCE=BUNDLED -DARROW_GANDIVA_JAVA=ON -DARROW_GANDIVA=ON -DARROW_PARQUET=ON -DARROW_HDFS=ON -DARROW_BOOST_USE_SHARED=ON -DARROW_JNI=ON -DARROW_DATASET=ON -DARROW_WITH_PROTOBUF=ON -DARROW_WITH_SNAPPY=ON -DARROW_WITH_LZ4=ON -DARROW_FILESYSTEM=ON -DARROW_JSON=ON .. make -j make install # build java cd ../../java # change property 'arrow.cpp.build.dir' to the relative path of cpp build dir in gandiva/pom.xml mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=../cpp/release-build/release/ -DskipTests # if you are behine proxy, please also add proxy for socks mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=../cpp/release-build/release/ -DskipTests -DsocksProxyHost=${proxyHost} -DsocksProxyPort=1080 run test mvn test -pl adapter/parquet -P arrow-jni mvn test -pl gandiva -P arrow-jni Copy binary files to oap-native-sql resources directory Because oap-native-sql plugin will build a stand-alone jar file with arrow dependency, if you choose to build Arrow by yourself, you have to copy below files as a replacement from the original one. You can find those files in Apache Arrow installation directory or release directory. Below example assume Apache Arrow has been installed on /usr/local/lib64 cp /usr/local/lib64/libarrow.so.17 $native-sql-engine-dir/cpp/src/resources cp /usr/local/lib64/libgandiva.so.17 $native-sql-engine-dir/cpp/src/resources cp /usr/local/lib64/libparquet.so.17 $native-sql-engine-dir/cpp/src/resources","title":"ApacheArrowInstallation"},{"location":"NativeSQLEngine/ApacheArrowInstallation/#llvm-70","text":"Arrow Gandiva depends on LLVM, and I noticed current version strictly depends on llvm7.0 if you installed any other version rather than 7.0, it will fail. wget http://releases.llvm.org/7.0.1/llvm-7.0.1.src.tar.xz tar xf llvm-7.0.1.src.tar.xz cd llvm-7.0.1.src/ cd tools wget http://releases.llvm.org/7.0.1/cfe-7.0.1.src.tar.xz tar xf cfe-7.0.1.src.tar.xz mv cfe-7.0.1.src clang cd .. mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE=Release cmake --build . -j cmake --build . --target install # check if clang has also been compiled, if no cd tools/clang mkdir build cd build cmake .. make -j make install","title":"llvm-7.0:"},{"location":"NativeSQLEngine/ApacheArrowInstallation/#cmake","text":"Arrow will download package during compiling, in order to support SSL in cmake, build cmake is optional. wget https://github.com/Kitware/CMake/releases/download/v3.15.0-rc4/cmake-3.15.0-rc4.tar.gz tar xf cmake-3.15.0-rc4.tar.gz cd cmake-3.15.0-rc4/ ./bootstrap --system-curl --parallel=64 #parallel num depends on your server core number make -j make install cmake --version cmake version 3.15.0-rc4","title":"cmake:"},{"location":"NativeSQLEngine/ApacheArrowInstallation/#apache-arrow","text":"git clone https://github.com/Intel-bigdata/arrow.git cd arrow && git checkout branch-0.17.0-oap-1.0 mkdir -p arrow/cpp/release-build cd arrow/cpp/release-build cmake -DARROW_DEPENDENCY_SOURCE=BUNDLED -DARROW_GANDIVA_JAVA=ON -DARROW_GANDIVA=ON -DARROW_PARQUET=ON -DARROW_HDFS=ON -DARROW_BOOST_USE_SHARED=ON -DARROW_JNI=ON -DARROW_DATASET=ON -DARROW_WITH_PROTOBUF=ON -DARROW_WITH_SNAPPY=ON -DARROW_WITH_LZ4=ON -DARROW_FILESYSTEM=ON -DARROW_JSON=ON .. make -j make install # build java cd ../../java # change property 'arrow.cpp.build.dir' to the relative path of cpp build dir in gandiva/pom.xml mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=../cpp/release-build/release/ -DskipTests # if you are behine proxy, please also add proxy for socks mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=../cpp/release-build/release/ -DskipTests -DsocksProxyHost=${proxyHost} -DsocksProxyPort=1080 run test mvn test -pl adapter/parquet -P arrow-jni mvn test -pl gandiva -P arrow-jni","title":"Apache Arrow"},{"location":"NativeSQLEngine/ApacheArrowInstallation/#copy-binary-files-to-oap-native-sql-resources-directory","text":"Because oap-native-sql plugin will build a stand-alone jar file with arrow dependency, if you choose to build Arrow by yourself, you have to copy below files as a replacement from the original one. You can find those files in Apache Arrow installation directory or release directory. Below example assume Apache Arrow has been installed on /usr/local/lib64 cp /usr/local/lib64/libarrow.so.17 $native-sql-engine-dir/cpp/src/resources cp /usr/local/lib64/libgandiva.so.17 $native-sql-engine-dir/cpp/src/resources cp /usr/local/lib64/libparquet.so.17 $native-sql-engine-dir/cpp/src/resources","title":"Copy binary files to oap-native-sql resources directory"},{"location":"NativeSQLEngine/Configuration/","text":"Spark Configurations for Native SQL Engine Add below configuration to spark-defaults.conf ##### Columnar Process Configuration spark.sql.sources.useV1SourceList avro spark.sql.join.preferSortMergeJoin false spark.sql.extensions com.intel.oap.ColumnarPlugin spark.shuffle.manager org.apache.spark.shuffle.sort.ColumnarShuffleManager # note native sql engine depends on arrow data source spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/spark-columnar-core-<version>-jar-with-dependencies.jar:$HOME/miniconda2/envs/oapenv/oap_jars/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/spark-columnar-core-<version>-jar-with-dependencies.jar:$HOME/miniconda2/envs/oapenv/oap_jars/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar spark.executorEnv.LIBARROW_DIR $HOME/miniconda2/envs/oapenv spark.executorEnv.CC $HOME/miniconda2/envs/oapenv/bin/gcc ###### Before you start spark, you must use below command to add some environment variables. export CC=$HOME/miniconda2/envs/oapenv/bin/gcc export LIBARROW_DIR=$HOME/miniconda2/envs/oapenv/ About arrow-data-source.jar, you can refer Unified Arrow Data Source .","title":"Configuration"},{"location":"NativeSQLEngine/Configuration/#spark-configurations-for-native-sql-engine","text":"Add below configuration to spark-defaults.conf ##### Columnar Process Configuration spark.sql.sources.useV1SourceList avro spark.sql.join.preferSortMergeJoin false spark.sql.extensions com.intel.oap.ColumnarPlugin spark.shuffle.manager org.apache.spark.shuffle.sort.ColumnarShuffleManager # note native sql engine depends on arrow data source spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/spark-columnar-core-<version>-jar-with-dependencies.jar:$HOME/miniconda2/envs/oapenv/oap_jars/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/spark-columnar-core-<version>-jar-with-dependencies.jar:$HOME/miniconda2/envs/oapenv/oap_jars/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar spark.executorEnv.LIBARROW_DIR $HOME/miniconda2/envs/oapenv spark.executorEnv.CC $HOME/miniconda2/envs/oapenv/bin/gcc ###### Before you start spark, you must use below command to add some environment variables. export CC=$HOME/miniconda2/envs/oapenv/bin/gcc export LIBARROW_DIR=$HOME/miniconda2/envs/oapenv/ About arrow-data-source.jar, you can refer Unified Arrow Data Source .","title":"Spark Configurations for Native SQL Engine"},{"location":"NativeSQLEngine/Installation/","text":"Spark Native SQL Engine Installation For detailed testing scripts, please refer to solution guide Install Googletest and Googlemock yum install gtest-devel yum install gmock Build Native SQL Engine git clone -b ${version} https://github.com/oap-project/native-sql-engine.git cd oap-native-sql cd cpp/ mkdir build/ cd build/ cmake .. -DTESTS=ON make -j cd ../../core/ mvn clean package -DskipTests Additonal Notes Notes for Installation Issues","title":"Installation"},{"location":"NativeSQLEngine/Installation/#spark-native-sql-engine-installation","text":"For detailed testing scripts, please refer to solution guide","title":"Spark Native SQL Engine Installation"},{"location":"NativeSQLEngine/Installation/#install-googletest-and-googlemock","text":"yum install gtest-devel yum install gmock","title":"Install Googletest and Googlemock"},{"location":"NativeSQLEngine/Installation/#build-native-sql-engine","text":"git clone -b ${version} https://github.com/oap-project/native-sql-engine.git cd oap-native-sql cd cpp/ mkdir build/ cd build/ cmake .. -DTESTS=ON make -j cd ../../core/ mvn clean package -DskipTests","title":"Build Native SQL Engine"},{"location":"NativeSQLEngine/Installation/#additonal-notes","text":"Notes for Installation Issues","title":"Additonal Notes"},{"location":"NativeSQLEngine/InstallationNotes/","text":"Notes for Installation Issues Before the Installation, if you have installed other version of oap-native-sql, remove all installed lib and include from system path: libarrow libgandiva libspark-columnar-jni* libgandiva_jni.so was not found inside JAR change property 'arrow.cpp.build.dir' to $ARROW_DIR/cpp/release-build/release/ in gandiva/pom.xml. If you do not want to change the contents of pom.xml, specify it like this: mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=/root/git/t/arrow/cpp/release-build/release/ -DskipTests -Dcheckstyle.skip No rule to make target '../src/protobuf_ep', needed by `src/proto/Exprs.pb.cc' remove the existing libprotobuf installation, then the script for find_package() will be able to download protobuf. can't find the libprotobuf.so.13 in the shared lib copy the libprotobuf.so.13 from $OAP_DIR/oap-native-sql/cpp/src/resources to /usr/lib64/ unable to load libhdfs: libgsasl.so.7: cannot open shared object file libgsasl is missing, run yum install libgsasl CentOS 7.7 looks like didn't provide the glibc we required, so binaries packaged on F30 won't work. 20/04/21 17:46:17 WARN TaskSetManager: Lost task 0.1 in stage 1.0 (TID 2, 10.0.0.143, executor 6): java.lang.UnsatisfiedLinkError: /tmp/libgandiva_jni.sobe729912-3bbe-4bd0-bb96-4c7ce2e62336: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by /tmp/libgandiva_jni.sobe729912-3bbe-4bd0-bb96-4c7ce2e62336) Missing symbols due to old GCC version. [root@vsr243 release-build]# nm /usr/local/lib64/libparquet.so | grep ZN5boost16re_detail_10710012perl_matcherIN9__gnu_cxx17__normal_iteratorIPKcSsEESaINS_9sub_matchIS6_EEENS_12regex_traitsIcNS_16cpp_regex_traitsIcEEEEE14construct_initERKNS_11basic_regexIcSD_EENS_15regex_constants12_match_flagsE _ZN5boost16re_detail_10710012perl_matcherIN9__gnu_cxx17__normal_iteratorIPKcSsEESaINS_9sub_matchIS6_EEENS_12regex_traitsIcNS_16cpp_regex_traitsIcEEEEE14construct_initERKNS_11basic_regexIcSD_EENS_15regex_constants12_match_flagsE Need to compile all packags with newer GCC: [root@vsr243 ~]# export CXX=/usr/local/bin/g++ [root@vsr243 ~]# export CC=/usr/local/bin/gcc Can not connect to hdfs @sr602 vsr606, vsr243 are both not able to connect to hdfs @sr602, need to skipTests to generate the jar","title":"InstallationNotes"},{"location":"NativeSQLEngine/InstallationNotes/#notes-for-installation-issues","text":"Before the Installation, if you have installed other version of oap-native-sql, remove all installed lib and include from system path: libarrow libgandiva libspark-columnar-jni* libgandiva_jni.so was not found inside JAR change property 'arrow.cpp.build.dir' to $ARROW_DIR/cpp/release-build/release/ in gandiva/pom.xml. If you do not want to change the contents of pom.xml, specify it like this: mvn clean install -P arrow-jni -am -Darrow.cpp.build.dir=/root/git/t/arrow/cpp/release-build/release/ -DskipTests -Dcheckstyle.skip No rule to make target '../src/protobuf_ep', needed by `src/proto/Exprs.pb.cc' remove the existing libprotobuf installation, then the script for find_package() will be able to download protobuf. can't find the libprotobuf.so.13 in the shared lib copy the libprotobuf.so.13 from $OAP_DIR/oap-native-sql/cpp/src/resources to /usr/lib64/ unable to load libhdfs: libgsasl.so.7: cannot open shared object file libgsasl is missing, run yum install libgsasl CentOS 7.7 looks like didn't provide the glibc we required, so binaries packaged on F30 won't work. 20/04/21 17:46:17 WARN TaskSetManager: Lost task 0.1 in stage 1.0 (TID 2, 10.0.0.143, executor 6): java.lang.UnsatisfiedLinkError: /tmp/libgandiva_jni.sobe729912-3bbe-4bd0-bb96-4c7ce2e62336: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by /tmp/libgandiva_jni.sobe729912-3bbe-4bd0-bb96-4c7ce2e62336) Missing symbols due to old GCC version. [root@vsr243 release-build]# nm /usr/local/lib64/libparquet.so | grep ZN5boost16re_detail_10710012perl_matcherIN9__gnu_cxx17__normal_iteratorIPKcSsEESaINS_9sub_matchIS6_EEENS_12regex_traitsIcNS_16cpp_regex_traitsIcEEEEE14construct_initERKNS_11basic_regexIcSD_EENS_15regex_constants12_match_flagsE _ZN5boost16re_detail_10710012perl_matcherIN9__gnu_cxx17__normal_iteratorIPKcSsEESaINS_9sub_matchIS6_EEENS_12regex_traitsIcNS_16cpp_regex_traitsIcEEEEE14construct_initERKNS_11basic_regexIcSD_EENS_15regex_constants12_match_flagsE Need to compile all packags with newer GCC: [root@vsr243 ~]# export CXX=/usr/local/bin/g++ [root@vsr243 ~]# export CC=/usr/local/bin/gcc Can not connect to hdfs @sr602 vsr606, vsr243 are both not able to connect to hdfs @sr602, need to skipTests to generate the jar","title":"Notes for Installation Issues"},{"location":"NativeSQLEngine/OAP-Developer-Guide/","text":"OAP Developer Guide This document contains the instructions & scripts on installing necessary dependencies and building OAP. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Arrow Data Source Native SQL Engine Building OAP Prerequisites for Building OAP is built with Apache Maven and Oracle Java 8, and mainly required tools to install on your cluster are listed below. Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. We provide scripts below to help automatically install dependencies above except RDMA , need change to root account, run: # git clone -b <tag-version> # cd oap-tools # sh dev/install-compile-time-dependencies.sh Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven Building To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building Specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache","title":"OAP Developer Guide"},{"location":"NativeSQLEngine/OAP-Developer-Guide/#oap-developer-guide","text":"This document contains the instructions & scripts on installing necessary dependencies and building OAP. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Arrow Data Source Native SQL Engine","title":"OAP Developer Guide"},{"location":"NativeSQLEngine/OAP-Developer-Guide/#building-oap","text":"","title":"Building OAP"},{"location":"NativeSQLEngine/OAP-Developer-Guide/#prerequisites-for-building","text":"OAP is built with Apache Maven and Oracle Java 8, and mainly required tools to install on your cluster are listed below. Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. We provide scripts below to help automatically install dependencies above except RDMA , need change to root account, run: # git clone -b <tag-version> # cd oap-tools # sh dev/install-compile-time-dependencies.sh Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven","title":"Prerequisites for Building"},{"location":"NativeSQLEngine/OAP-Developer-Guide/#building","text":"To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building Specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache","title":"Building"},{"location":"NativeSQLEngine/OAP-Installation-Guide/","text":"OAP Installation Guide This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine. Contents Prerequisites Installing OAP Configuration Prerequisites OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly. Installing OAP Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Create a conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.0.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Extra Steps for Shuffle Remote PMem Extension If you use one of OAP features -- PMmem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details. Configuration Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar And then you can follow the corresponding feature documents for more details to use them.","title":"OAP Installation Guide"},{"location":"NativeSQLEngine/OAP-Installation-Guide/#oap-installation-guide","text":"This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine.","title":"OAP Installation Guide"},{"location":"NativeSQLEngine/OAP-Installation-Guide/#contents","text":"Prerequisites Installing OAP Configuration","title":"Contents"},{"location":"NativeSQLEngine/OAP-Installation-Guide/#prerequisites","text":"OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly.","title":"Prerequisites"},{"location":"NativeSQLEngine/OAP-Installation-Guide/#installing-oap","text":"Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Create a conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.0.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars","title":"Installing OAP"},{"location":"NativeSQLEngine/OAP-Installation-Guide/#extra-steps-for-shuffle-remote-pmem-extension","text":"If you use one of OAP features -- PMmem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details.","title":"Extra Steps for Shuffle Remote PMem Extension"},{"location":"NativeSQLEngine/OAP-Installation-Guide/#configuration","text":"Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar And then you can follow the corresponding feature documents for more details to use them.","title":"Configuration"},{"location":"NativeSQLEngine/Prerequisite/","text":"Prerequisite There are some requirements before you build the project. Please make sure you have already installed the software in your system. gcc 9.3 or higher version java8 OpenJDK -> yum install java-1.8.0-openjdk cmake 3.2 or higher version maven 3.1.1 or higher version Hadoop 2.7.5 or higher version Spark 3.0.0 or higher version Intel Optimized Arrow 0.17.0 gcc installation // installing gcc 9.3 or higher version Please notes for better performance support, gcc 9.3 is a minimal requirement with Intel Microarchitecture such as SKYLAKE, CASCADELAKE, ICELAKE. https://gcc.gnu.org/install/index.html Follow the above website to download gcc. C++ library may ask a certain version, if you are using gcc 9.3 the version would be libstdc++.so.6.0.28. You may have to launch ./contrib/download_prerequisites command to install all the prerequisites for gcc. If you are facing downloading issue in download_prerequisites command, you can try to change ftp to http. //Follow the steps to configure gcc https://gcc.gnu.org/install/configure.html If you are facing a multilib issue, you can try to add --disable-multilib parameter in ../configure //Follow the steps to build gc https://gcc.gnu.org/install/build.html //Follow the steps to install gcc https://gcc.gnu.org/install/finalinstall.html //Set up Environment for new gcc export PATH=$YOUR_GCC_INSTALLATION_DIR/bin:$PATH export LD_LIBRARY_PATH=$YOUR_GCC_INSTALLATION_DIR/lib64:$LD_LIBRARY_PATH Please remember to add and source the setup in your environment files such as /etc/profile or /etc/bashrc //Verify if gcc has been installation Use gcc -v command to verify if your gcc version is correct.(Must larger than 9.3) cmake installation If you are facing some trouble when installing cmake, please follow below steps to install cmake. // installing cmake 3.2 sudo yum install cmake3 // If you have an existing cmake, you can use below command to set it as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake 10 --slave /usr/local/bin/ctest ctest /usr/bin/ctest --slave /usr/local/bin/cpack cpack /usr/bin/cpack --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake --family cmake // Set cmake3 as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake3 20 --slave /usr/local/bin/ctest ctest /usr/bin/ctest3 --slave /usr/local/bin/cpack cpack /usr/bin/cpack3 --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake3 --family cmake // Use alternatives to choose cmake version sudo alternatives --config cmake maven installation If you are facing some trouble when installing maven, please follow below steps to install maven // installing maven 3.6.3 Go to https://maven.apache.org/download.cgi and download the specific version of maven // Below command use maven 3.6.3 as an example wget htps://ftp.wayne.edu/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz wget https://ftp.wayne.edu/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz tar xzf apache-maven-3.6.3-bin.tar.gz mkdir /usr/local/maven mv apache-maven-3.6.3/ /usr/local/maven/ // Set maven 3.6.3 as an option within alternatives command sudo alternatives --install /usr/bin/mvn mvn /usr/local/maven/apache-maven-3.6.3/bin/mvn 1 // Use alternatives to choose mvn version sudo alternatives --config mvn HADOOP/SPARK Installation If there is no existing Hadoop/Spark installed, Please follow the guide to install your Hadoop/Spark SPARK/HADOOP Installation Hadoop Native Library(Default) Please make sure you have set up Hadoop directory properly with Hadoop Native Libraries By default, Apache Arrow would scan $HADOOP_HOME and find the native Hadoop library libhdfs.so (under $HADOOP_HOME/lib/native directory) to be used for Hadoop client. You can also use ARROW_LIBHDFS_DIR to configure the location of libhdfs.so if it is installed in other directory than $HADOOP_HOME/lib/native If your SPARK and HADOOP are separated in different nodes, please find libhdfs.so in your Hadoop cluster and copy it to SPARK cluster, then use one of the above methods to set it properly. For more information, please check Arrow HDFS interface documentation Hadoop Native Library, please read the official Hadoop website documentation Use libhdfs3 library for better performance(Optional) For better performance ArrowDataSource reads HDFS files using the third-party library libhdfs3 . The library must be pre-installed on machines Spark Executor nodes are running on. To install the library, use of Conda is recommended. // installing libhdfs3 conda install -c conda-forge libhdfs3 // check the installed library file ll ~/miniconda/envs/$(YOUR_ENV_NAME)/lib/libhdfs3.so We also provide a libhdfs3 binary in cpp/src/resources directory. To set up libhdfs3, there are two different ways: Option1: Overwrite the soft link for libhdfs.so To install libhdfs3.so, you have to create a soft link for libhdfs.so in your Hadoop directory( $HADOOP_HOME/lib/native by default). ln -f -s libhdfs3.so libhdfs.so Option2: Add env variable to the system export ARROW_LIBHDFS3_DIR=\"PATH_TO_LIBHDFS3_DIR/\" Add following Spark configuration options before running the DataSource to make the library to be recognized: spark.executorEnv.ARROW_LIBHDFS3_DIR = \"PATH_TO_LIBHDFS3_DIR/\" spark.executorEnv.LD_LIBRARY_PATH = \"PATH_TO_LIBHDFS3_DEPENDENCIES_DIR/\" Please notes: If you choose to use libhdfs3.so, there are some other dependency libraries you have to installed such as libprotobuf or libcrypto. Intel Optimized Apache Arrow Installation Intel Optimized Apache Arrow is MANDATORY to be used. However, we have a bundle a compiled arrow libraries(libarrow, libgandiva, libparquet) built by GCC9.3 included in the cpp/src/resources directory. If you wish to build Apache Arrow by yourself, please follow the guide to build and install Apache Arrow ArrowInstallation","title":"Prerequisite"},{"location":"NativeSQLEngine/Prerequisite/#prerequisite","text":"There are some requirements before you build the project. Please make sure you have already installed the software in your system. gcc 9.3 or higher version java8 OpenJDK -> yum install java-1.8.0-openjdk cmake 3.2 or higher version maven 3.1.1 or higher version Hadoop 2.7.5 or higher version Spark 3.0.0 or higher version Intel Optimized Arrow 0.17.0","title":"Prerequisite"},{"location":"NativeSQLEngine/Prerequisite/#gcc-installation","text":"// installing gcc 9.3 or higher version Please notes for better performance support, gcc 9.3 is a minimal requirement with Intel Microarchitecture such as SKYLAKE, CASCADELAKE, ICELAKE. https://gcc.gnu.org/install/index.html Follow the above website to download gcc. C++ library may ask a certain version, if you are using gcc 9.3 the version would be libstdc++.so.6.0.28. You may have to launch ./contrib/download_prerequisites command to install all the prerequisites for gcc. If you are facing downloading issue in download_prerequisites command, you can try to change ftp to http. //Follow the steps to configure gcc https://gcc.gnu.org/install/configure.html If you are facing a multilib issue, you can try to add --disable-multilib parameter in ../configure //Follow the steps to build gc https://gcc.gnu.org/install/build.html //Follow the steps to install gcc https://gcc.gnu.org/install/finalinstall.html //Set up Environment for new gcc export PATH=$YOUR_GCC_INSTALLATION_DIR/bin:$PATH export LD_LIBRARY_PATH=$YOUR_GCC_INSTALLATION_DIR/lib64:$LD_LIBRARY_PATH Please remember to add and source the setup in your environment files such as /etc/profile or /etc/bashrc //Verify if gcc has been installation Use gcc -v command to verify if your gcc version is correct.(Must larger than 9.3)","title":"gcc installation"},{"location":"NativeSQLEngine/Prerequisite/#cmake-installation","text":"If you are facing some trouble when installing cmake, please follow below steps to install cmake. // installing cmake 3.2 sudo yum install cmake3 // If you have an existing cmake, you can use below command to set it as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake 10 --slave /usr/local/bin/ctest ctest /usr/bin/ctest --slave /usr/local/bin/cpack cpack /usr/bin/cpack --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake --family cmake // Set cmake3 as an option within alternatives command sudo alternatives --install /usr/local/bin/cmake cmake /usr/bin/cmake3 20 --slave /usr/local/bin/ctest ctest /usr/bin/ctest3 --slave /usr/local/bin/cpack cpack /usr/bin/cpack3 --slave /usr/local/bin/ccmake ccmake /usr/bin/ccmake3 --family cmake // Use alternatives to choose cmake version sudo alternatives --config cmake","title":"cmake installation"},{"location":"NativeSQLEngine/Prerequisite/#maven-installation","text":"If you are facing some trouble when installing maven, please follow below steps to install maven // installing maven 3.6.3 Go to https://maven.apache.org/download.cgi and download the specific version of maven // Below command use maven 3.6.3 as an example wget htps://ftp.wayne.edu/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz wget https://ftp.wayne.edu/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz tar xzf apache-maven-3.6.3-bin.tar.gz mkdir /usr/local/maven mv apache-maven-3.6.3/ /usr/local/maven/ // Set maven 3.6.3 as an option within alternatives command sudo alternatives --install /usr/bin/mvn mvn /usr/local/maven/apache-maven-3.6.3/bin/mvn 1 // Use alternatives to choose mvn version sudo alternatives --config mvn","title":"maven installation"},{"location":"NativeSQLEngine/Prerequisite/#hadoopspark-installation","text":"If there is no existing Hadoop/Spark installed, Please follow the guide to install your Hadoop/Spark SPARK/HADOOP Installation","title":"HADOOP/SPARK Installation"},{"location":"NativeSQLEngine/Prerequisite/#hadoop-native-librarydefault","text":"Please make sure you have set up Hadoop directory properly with Hadoop Native Libraries By default, Apache Arrow would scan $HADOOP_HOME and find the native Hadoop library libhdfs.so (under $HADOOP_HOME/lib/native directory) to be used for Hadoop client. You can also use ARROW_LIBHDFS_DIR to configure the location of libhdfs.so if it is installed in other directory than $HADOOP_HOME/lib/native If your SPARK and HADOOP are separated in different nodes, please find libhdfs.so in your Hadoop cluster and copy it to SPARK cluster, then use one of the above methods to set it properly. For more information, please check Arrow HDFS interface documentation Hadoop Native Library, please read the official Hadoop website documentation","title":"Hadoop Native Library(Default)"},{"location":"NativeSQLEngine/Prerequisite/#use-libhdfs3-library-for-better-performanceoptional","text":"For better performance ArrowDataSource reads HDFS files using the third-party library libhdfs3 . The library must be pre-installed on machines Spark Executor nodes are running on. To install the library, use of Conda is recommended. // installing libhdfs3 conda install -c conda-forge libhdfs3 // check the installed library file ll ~/miniconda/envs/$(YOUR_ENV_NAME)/lib/libhdfs3.so We also provide a libhdfs3 binary in cpp/src/resources directory. To set up libhdfs3, there are two different ways: Option1: Overwrite the soft link for libhdfs.so To install libhdfs3.so, you have to create a soft link for libhdfs.so in your Hadoop directory( $HADOOP_HOME/lib/native by default). ln -f -s libhdfs3.so libhdfs.so Option2: Add env variable to the system export ARROW_LIBHDFS3_DIR=\"PATH_TO_LIBHDFS3_DIR/\" Add following Spark configuration options before running the DataSource to make the library to be recognized: spark.executorEnv.ARROW_LIBHDFS3_DIR = \"PATH_TO_LIBHDFS3_DIR/\" spark.executorEnv.LD_LIBRARY_PATH = \"PATH_TO_LIBHDFS3_DEPENDENCIES_DIR/\" Please notes: If you choose to use libhdfs3.so, there are some other dependency libraries you have to installed such as libprotobuf or libcrypto.","title":"Use libhdfs3 library for better performance(Optional)"},{"location":"NativeSQLEngine/Prerequisite/#intel-optimized-apache-arrow-installation","text":"Intel Optimized Apache Arrow is MANDATORY to be used. However, we have a bundle a compiled arrow libraries(libarrow, libgandiva, libparquet) built by GCC9.3 included in the cpp/src/resources directory. If you wish to build Apache Arrow by yourself, please follow the guide to build and install Apache Arrow ArrowInstallation","title":"Intel Optimized Apache Arrow Installation"},{"location":"NativeSQLEngine/SparkInstallation/","text":"Download Spark 3.0.1 Currently Native SQL Engine works on the Spark 3.0.1 version. wget http://archive.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz sudo mkdir -p /opt/spark && sudo mv spark-3.0.1-bin-hadoop3.2.tgz /opt/spark sudo cd /opt/spark && sudo tar -xf spark-3.0.1-bin-hadoop3.2.tgz export SPARK_HOME=/opt/spark/spark-3.0.1-bin-hadoop3.2/ Or building Spark from source git clone https://github.com/intel-bigdata/spark.git cd spark && git checkout native-sql-engine-clean # check spark supported hadoop version grep \\<hadoop\\.version\\> -r pom.xml <hadoop.version>2.7.4</hadoop.version> <hadoop.version>3.2.0</hadoop.version> # so we should build spark specifying hadoop version as 3.2 ./build/mvn -Pyarn -Phadoop-3.2 -Dhadoop.version=3.2.0 -DskipTests clean install Specify SPARK_HOME to spark path export SPARK_HOME=${HADOOP_PATH} Hadoop building from source git clone https://github.com/apache/hadoop.git cd hadoop git checkout rel/release-3.2.0 # only build binary for hadoop mvn clean install -Pdist -DskipTests -Dtar # build binary and native library such as libhdfs.so for hadoop # mvn clean install -Pdist,native -DskipTests -Dtar export HADOOP_HOME=${HADOOP_PATH}/hadoop-dist/target/hadoop-3.2.0/","title":"SparkInstallation"},{"location":"NativeSQLEngine/SparkInstallation/#download-spark-301","text":"Currently Native SQL Engine works on the Spark 3.0.1 version. wget http://archive.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz sudo mkdir -p /opt/spark && sudo mv spark-3.0.1-bin-hadoop3.2.tgz /opt/spark sudo cd /opt/spark && sudo tar -xf spark-3.0.1-bin-hadoop3.2.tgz export SPARK_HOME=/opt/spark/spark-3.0.1-bin-hadoop3.2/","title":"Download Spark 3.0.1"},{"location":"NativeSQLEngine/SparkInstallation/#or-building-spark-from-source","text":"git clone https://github.com/intel-bigdata/spark.git cd spark && git checkout native-sql-engine-clean # check spark supported hadoop version grep \\<hadoop\\.version\\> -r pom.xml <hadoop.version>2.7.4</hadoop.version> <hadoop.version>3.2.0</hadoop.version> # so we should build spark specifying hadoop version as 3.2 ./build/mvn -Pyarn -Phadoop-3.2 -Dhadoop.version=3.2.0 -DskipTests clean install Specify SPARK_HOME to spark path export SPARK_HOME=${HADOOP_PATH}","title":"Or building Spark from source"},{"location":"NativeSQLEngine/SparkInstallation/#hadoop-building-from-source","text":"git clone https://github.com/apache/hadoop.git cd hadoop git checkout rel/release-3.2.0 # only build binary for hadoop mvn clean install -Pdist -DskipTests -Dtar # build binary and native library such as libhdfs.so for hadoop # mvn clean install -Pdist,native -DskipTests -Dtar export HADOOP_HOME=${HADOOP_PATH}/hadoop-dist/target/hadoop-3.2.0/","title":"Hadoop building from source"},{"location":"NativeSQLEngine/User-Guide/","text":"Spark Native SQL Engine A Native Engine for Spark SQL with vectorized SIMD optimizations Introduction Spark SQL works very well with structured row-based data. It used WholeStageCodeGen to improve the performance by Java JIT code. However Java JIT is usually not working very well on utilizing latest SIMD instructions, especially under complicated queries. Apache Arrow provided CPU-cache friendly columnar in-memory layout, its SIMD optimized kernels and LLVM based SQL engine Gandiva are also very efficient. Native SQL Engine used these technoligies and brought better performance to Spark SQL. Key Features Apache Arrow formatted intermediate data among Spark operator With Spark 27396 its possible to pass a RDD of Columnarbatch to operators. We implemented this API with Arrow columnar format. Apache Arrow based Native Readers for Parquet and other formats A native parquet reader was developed to speed up the data loading. it's based on Apache Arrow Dataset. For details please check Arrow Data Source Apache Arrow Compute/Gandiva based operators We implemented common operators based on Apache Arrow Compute and Gandiva. The SQL expression was compiled to one expression tree with protobuf and passed to native kernels. The native kernels will then evaluate the these expressions based on the input columnar batch. Native Columnar Shuffle Operator with efficient compression support We implemented columnar shuffle to improve the shuffle performance. With the columnar layout we could do very efficient data compression for different data format. Build the Plugin Building by Conda If you already have a working Hadoop Spark Cluster, we provide a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. Once finished OAP-Installation-Guide , you can find built spark-columnar-core-<version>-jar-with-dependencies.jar under $HOME/miniconda2/envs/oapenv/oap_jars . Then you can just skip below steps and jump to Getting Started Get Started . Building by yourself If you prefer to build from the source code on your hand, please follow below steps to set up your environment. Prerequisite There are some requirements before you build the project. Please check the document Prerequisite and make sure you have already installed the software in your system. If you are running a SPARK Cluster, please make sure all the software are installed in every single node. Installation Please check the document Installation Guide Configuration & Testing Please check the document Configuration Guide Get started To enable OAP NativeSQL Engine, the previous built jar spark-columnar-core-<version>-jar-with-dependencies.jar should be added to Spark configuration. We also recommend to use spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar . We will demonstrate an example by using both jar files. SPARK related options are: spark.driver.extraClassPath : Set to load jar file to driver. spark.executor.extraClassPath : Set to load jar file to executor. jars : Set to copy jar file to the executors when using yarn cluster mode. spark.executorEnv.ARROW_LIBHDFS3_DIR : Optional if you are using a custom libhdfs3.so. spark.executorEnv.LD_LIBRARY_PATH : Optional if you are using a custom libhdfs3.so. For Spark Standalone Mode, please set the above value as relative path to the jar file. For Spark Yarn Cluster Mode, please set the above value as absolute path to the jar file. Example to run Spark Shell with ArrowDataSource jar file ${SPARK_HOME}/bin/spark-shell \\ --verbose \\ --master yarn \\ --driver-memory 10G \\ --conf spark.driver.extraClassPath=$PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar:$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar \\ --conf spark.executor.extraClassPath=$PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar:$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar \\ --conf spark.driver.cores=1 \\ --conf spark.executor.instances=12 \\ --conf spark.executor.cores=6 \\ --conf spark.executor.memory=20G \\ --conf spark.memory.offHeap.size=80G \\ --conf spark.task.cpus=1 \\ --conf spark.locality.wait=0s \\ --conf spark.sql.shuffle.partitions=72 \\ --conf spark.executorEnv.ARROW_LIBHDFS3_DIR=\"$PATH_TO_LIBHDFS3_DIR/\" \\ --conf spark.executorEnv.LD_LIBRARY_PATH=\"$PATH_TO_LIBHDFS3_DEPENDENCIES_DIR\" --jars $PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar,$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar Here is one example to verify if native sql engine works, make sure you have TPC-H dataset. We could do a simple projection on one parquet table. For detailed testing scripts, please refer to Solution Guide . val orders = spark.read.format(\"arrow\").load(\"hdfs:////user/root/date_tpch_10/orders\") orders.createOrReplaceTempView(\"orders\") spark.sql(\"select * from orders where o_orderdate > date '1998-07-26'\").show(20000, false) The result should show up on Spark console and you can check the DAG diagram with some Columnar Processing stage. Performance data For initial microbenchmark performance, we add 10 fields up with spark, data size is 200G data Coding Style For Java code, we used google-java-format For Scala code, we used Spark Scala Format , please use scalafmt or run ./scalafmt for scala codes format For Cpp codes, we used Clang-Format, check on this link google-vim-codefmt for details. Contact chendi.xue@intel.com binwei.yang@intel.com","title":"User Guide"},{"location":"NativeSQLEngine/User-Guide/#spark-native-sql-engine","text":"A Native Engine for Spark SQL with vectorized SIMD optimizations","title":"Spark Native SQL Engine"},{"location":"NativeSQLEngine/User-Guide/#introduction","text":"Spark SQL works very well with structured row-based data. It used WholeStageCodeGen to improve the performance by Java JIT code. However Java JIT is usually not working very well on utilizing latest SIMD instructions, especially under complicated queries. Apache Arrow provided CPU-cache friendly columnar in-memory layout, its SIMD optimized kernels and LLVM based SQL engine Gandiva are also very efficient. Native SQL Engine used these technoligies and brought better performance to Spark SQL.","title":"Introduction"},{"location":"NativeSQLEngine/User-Guide/#key-features","text":"","title":"Key Features"},{"location":"NativeSQLEngine/User-Guide/#apache-arrow-formatted-intermediate-data-among-spark-operator","text":"With Spark 27396 its possible to pass a RDD of Columnarbatch to operators. We implemented this API with Arrow columnar format.","title":"Apache Arrow formatted intermediate data among Spark operator"},{"location":"NativeSQLEngine/User-Guide/#apache-arrow-based-native-readers-for-parquet-and-other-formats","text":"A native parquet reader was developed to speed up the data loading. it's based on Apache Arrow Dataset. For details please check Arrow Data Source","title":"Apache Arrow based Native Readers for Parquet and other formats"},{"location":"NativeSQLEngine/User-Guide/#apache-arrow-computegandiva-based-operators","text":"We implemented common operators based on Apache Arrow Compute and Gandiva. The SQL expression was compiled to one expression tree with protobuf and passed to native kernels. The native kernels will then evaluate the these expressions based on the input columnar batch.","title":"Apache Arrow Compute/Gandiva based operators"},{"location":"NativeSQLEngine/User-Guide/#native-columnar-shuffle-operator-with-efficient-compression-support","text":"We implemented columnar shuffle to improve the shuffle performance. With the columnar layout we could do very efficient data compression for different data format.","title":"Native Columnar Shuffle Operator with efficient compression support"},{"location":"NativeSQLEngine/User-Guide/#build-the-plugin","text":"","title":"Build the Plugin"},{"location":"NativeSQLEngine/User-Guide/#building-by-conda","text":"If you already have a working Hadoop Spark Cluster, we provide a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. Once finished OAP-Installation-Guide , you can find built spark-columnar-core-<version>-jar-with-dependencies.jar under $HOME/miniconda2/envs/oapenv/oap_jars . Then you can just skip below steps and jump to Getting Started Get Started .","title":"Building by Conda"},{"location":"NativeSQLEngine/User-Guide/#building-by-yourself","text":"If you prefer to build from the source code on your hand, please follow below steps to set up your environment.","title":"Building by yourself"},{"location":"NativeSQLEngine/User-Guide/#prerequisite","text":"There are some requirements before you build the project. Please check the document Prerequisite and make sure you have already installed the software in your system. If you are running a SPARK Cluster, please make sure all the software are installed in every single node.","title":"Prerequisite"},{"location":"NativeSQLEngine/User-Guide/#installation","text":"Please check the document Installation Guide","title":"Installation"},{"location":"NativeSQLEngine/User-Guide/#configuration-testing","text":"Please check the document Configuration Guide","title":"Configuration &amp; Testing"},{"location":"NativeSQLEngine/User-Guide/#get-started","text":"To enable OAP NativeSQL Engine, the previous built jar spark-columnar-core-<version>-jar-with-dependencies.jar should be added to Spark configuration. We also recommend to use spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar . We will demonstrate an example by using both jar files. SPARK related options are: spark.driver.extraClassPath : Set to load jar file to driver. spark.executor.extraClassPath : Set to load jar file to executor. jars : Set to copy jar file to the executors when using yarn cluster mode. spark.executorEnv.ARROW_LIBHDFS3_DIR : Optional if you are using a custom libhdfs3.so. spark.executorEnv.LD_LIBRARY_PATH : Optional if you are using a custom libhdfs3.so. For Spark Standalone Mode, please set the above value as relative path to the jar file. For Spark Yarn Cluster Mode, please set the above value as absolute path to the jar file. Example to run Spark Shell with ArrowDataSource jar file ${SPARK_HOME}/bin/spark-shell \\ --verbose \\ --master yarn \\ --driver-memory 10G \\ --conf spark.driver.extraClassPath=$PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar:$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar \\ --conf spark.executor.extraClassPath=$PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar:$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar \\ --conf spark.driver.cores=1 \\ --conf spark.executor.instances=12 \\ --conf spark.executor.cores=6 \\ --conf spark.executor.memory=20G \\ --conf spark.memory.offHeap.size=80G \\ --conf spark.task.cpus=1 \\ --conf spark.locality.wait=0s \\ --conf spark.sql.shuffle.partitions=72 \\ --conf spark.executorEnv.ARROW_LIBHDFS3_DIR=\"$PATH_TO_LIBHDFS3_DIR/\" \\ --conf spark.executorEnv.LD_LIBRARY_PATH=\"$PATH_TO_LIBHDFS3_DEPENDENCIES_DIR\" --jars $PATH_TO_JAR/spark-arrow-datasource-standard-<version>-jar-with-dependencies.jar,$PATH_TO_JAR/spark-columnar-core-<version>-jar-with-dependencies.jar Here is one example to verify if native sql engine works, make sure you have TPC-H dataset. We could do a simple projection on one parquet table. For detailed testing scripts, please refer to Solution Guide . val orders = spark.read.format(\"arrow\").load(\"hdfs:////user/root/date_tpch_10/orders\") orders.createOrReplaceTempView(\"orders\") spark.sql(\"select * from orders where o_orderdate > date '1998-07-26'\").show(20000, false) The result should show up on Spark console and you can check the DAG diagram with some Columnar Processing stage.","title":"Get started"},{"location":"NativeSQLEngine/User-Guide/#performance-data","text":"For initial microbenchmark performance, we add 10 fields up with spark, data size is 200G data","title":"Performance data"},{"location":"NativeSQLEngine/User-Guide/#coding-style","text":"For Java code, we used google-java-format For Scala code, we used Spark Scala Format , please use scalafmt or run ./scalafmt for scala codes format For Cpp codes, we used Clang-Format, check on this link google-vim-codefmt for details.","title":"Coding Style"},{"location":"NativeSQLEngine/User-Guide/#contact","text":"chendi.xue@intel.com binwei.yang@intel.com","title":"Contact"},{"location":"OAPMLlib/","text":"OAP MLlib Overview OAP MLlib is an optimized package to accelerate machine learning algorithms in Apache Spark MLlib . It is compatible with Spark MLlib and leverages open source Intel\u00ae oneAPI Data Analytics Library (oneDAL) to provide highly optimized algorithms and get most out of CPU and GPU capabilities. It also take advantage of open source Intel\u00ae oneAPI Collective Communications Library (oneCCL) to provide efficient communication patterns in multi-node multi-GPU clusters. Compatibility OAP MLlib tried to maintain the same API interfaces and produce same results that are identical with Spark MLlib. However due to the nature of float point operations, there may be some small deviation from the original result, we will try our best to make sure the error is within acceptable range. For those algorithms that are not accelerated by OAP MLlib, the original Spark MLlib one will be used. Getting Started Java/Scala Users Preferred Use a pre-built OAP MLlib JAR to get started. You can firstly download OAP package from OAP-JARs-Tarball and extract this Tarball to get oap-mllib-x.x.x-with-spark-x.x.x.jar under oap-1.0.0-bin-spark-3.0.0/jars . Then you can refer to the following Running section to try out. Python/PySpark Users Preferred Use a pre-built JAR to get started. If you have finished OAP-Installation-Guide , you can find compiled OAP MLlib JAR oap-mllib-x.x.x-with-spark-x.x.x.jar in $HOME/miniconda2/envs/oapenv/oap_jars/ . Then you can refer to the following Running section to try out. Building From Scratch You can also build the package from source code, please refer to Building section. Running Prerequisites CentOS 7.0+, Ubuntu 18.04 LTS+ Java JRE 8.0+ Runtime Apache Spark 3.0.0+ Generally, our common system requirements are the same with Intel\u00ae oneAPI Toolkit, please refer to here for details. Intel\u00ae oneAPI Toolkits components used by the project are already included into JAR package mentioned above. There are no extra installations for cluster nodes. Spark Configuration Users usually run Spark application on YARN with client mode. In that case, you only need to add the following configurations in spark-defaults.conf or in spark-submit command line before running. # absolute path of the jar for uploading spark.files /path/to/oap-mllib-x.x.x-with-spark-x.x.x.jar # absolute path of the jar for driver class path spark.driver.extraClassPath /path/to/oap-mllib-x.x.x-with-spark-x.x.x.jar # relative path to spark.files, just specify jar name in current dir spark.executor.extraClassPath ./oap-mllib-x.x.x-with-spark-x.x.x.jar Sanity Check To use K-means example for sanity check, you need to upload a data file to your HDFS and change related variables in run.sh of kmeans example. Then run the following commands: $ cd oap-mllib/examples/kmeans $ ./build.sh $ ./run.sh Benchmark with HiBench Use Hibench to generate dataset with various profiles, and change related variables in run-XXX.sh script when applicable. Then run the following commands: $ cd oap-mllib/examples/kmeans-hibench $ ./build.sh $ ./run-hibench-oap-mllib.sh PySpark Support As PySpark-based applications call their Scala couterparts, they shall be supported out-of-box. An example can be found in the Examples section. Building Prerequisites We use Apache Maven to manage and build source code. The following tools and libraries are also needed to build OAP MLlib: JDK 8.0+ Apache Maven 3.6.2+ GNU GCC 4.8.5+ Intel\u00ae oneAPI Toolkits 2021.1.1 Components: Data Analytics Library (oneDAL) Threading Building Blocks (oneTBB) Open Source Intel\u00ae oneAPI Collective Communications Library (oneCCL) Intel\u00ae oneAPI Toolkits and its components can be downloaded and install from here . Installation process for oneAPI using Package Managers (YUM (DNF), APT, and ZYPPER) is also available. Generally you only need to install oneAPI Base Toolkit for Linux with all or selected components mentioned above. Instead of using oneCCL included in Intel\u00ae oneAPI Toolkits, we prefer to build from open source oneCCL to resolve some bugs. More details about oneAPI can be found here . You can also refer to this script and comments in it to install correct oneAPI version and manually setup the environments. Scala and Java dependency descriptions are already included in Maven POM file. Build Building oneCCL To clone and build from open source oneCCL, run the following commands: $ git clone https://github.com/oneapi-src/oneCCL $ cd oneCCL $ git checkout beta08 $ mkdir build && cd build $ cmake .. $ make -j install The generated files will be placed in /your/oneCCL_source_code/build/_install Building OAP MLlib To clone and checkout source code, run the following commands: $ git clone https://github.com/oap-project/oap-mllib.git Optional to checkout specific release branch: $ cd oap-mllib && git checkout ${version} We rely on environment variables to find required toolchains and libraries. Please make sure the following environment variables are set for building: Environment Description JAVA_HOME Path to JDK home directory DAALROOT Path to oneDAL home directory TBB_ROOT Path to oneTBB home directory CCL_ROOT Path to oneCCL home directory We suggest you to source setvars.sh script into current shell to setup building environments as following: $ source /opt/intel/inteloneapi/setvars.sh $ source /your/oneCCL_source_code/build/_install/env/setvars.sh Be noticed we are using our own built oneCCL instead, we should source oneCCL's setvars.sh to overwrite oneAPI one. If you prefer to buid your own open source oneDAL , oneTBB versions rather than use the ones included in oneAPI TookKits, you can refer to the related build instructions and manually source setvars.sh accordingly. To build, run the following commands: $ cd oap-mllib/mllib-dal $ ./build.sh The built JAR package will be placed in target directory with the name oap-mllib-x.x.x-with-spark-x.x.x.jar . Examples Example Description kmeans K-means example for Scala kmeans-pyspark K-means example for PySpark kmeans-hibench Use HiBench-generated input dataset to benchmark K-means performance List of Accelerated Algorithms K-Means (CPU, Experimental)","title":"OAP MLlib"},{"location":"OAPMLlib/#oap-mllib","text":"","title":"OAP MLlib"},{"location":"OAPMLlib/#overview","text":"OAP MLlib is an optimized package to accelerate machine learning algorithms in Apache Spark MLlib . It is compatible with Spark MLlib and leverages open source Intel\u00ae oneAPI Data Analytics Library (oneDAL) to provide highly optimized algorithms and get most out of CPU and GPU capabilities. It also take advantage of open source Intel\u00ae oneAPI Collective Communications Library (oneCCL) to provide efficient communication patterns in multi-node multi-GPU clusters.","title":"Overview"},{"location":"OAPMLlib/#compatibility","text":"OAP MLlib tried to maintain the same API interfaces and produce same results that are identical with Spark MLlib. However due to the nature of float point operations, there may be some small deviation from the original result, we will try our best to make sure the error is within acceptable range. For those algorithms that are not accelerated by OAP MLlib, the original Spark MLlib one will be used.","title":"Compatibility"},{"location":"OAPMLlib/#getting-started","text":"","title":"Getting Started"},{"location":"OAPMLlib/#javascala-users-preferred","text":"Use a pre-built OAP MLlib JAR to get started. You can firstly download OAP package from OAP-JARs-Tarball and extract this Tarball to get oap-mllib-x.x.x-with-spark-x.x.x.jar under oap-1.0.0-bin-spark-3.0.0/jars . Then you can refer to the following Running section to try out.","title":"Java/Scala Users Preferred"},{"location":"OAPMLlib/#pythonpyspark-users-preferred","text":"Use a pre-built JAR to get started. If you have finished OAP-Installation-Guide , you can find compiled OAP MLlib JAR oap-mllib-x.x.x-with-spark-x.x.x.jar in $HOME/miniconda2/envs/oapenv/oap_jars/ . Then you can refer to the following Running section to try out.","title":"Python/PySpark Users Preferred"},{"location":"OAPMLlib/#building-from-scratch","text":"You can also build the package from source code, please refer to Building section.","title":"Building From Scratch"},{"location":"OAPMLlib/#running","text":"","title":"Running"},{"location":"OAPMLlib/#prerequisites","text":"CentOS 7.0+, Ubuntu 18.04 LTS+ Java JRE 8.0+ Runtime Apache Spark 3.0.0+ Generally, our common system requirements are the same with Intel\u00ae oneAPI Toolkit, please refer to here for details. Intel\u00ae oneAPI Toolkits components used by the project are already included into JAR package mentioned above. There are no extra installations for cluster nodes.","title":"Prerequisites"},{"location":"OAPMLlib/#spark-configuration","text":"Users usually run Spark application on YARN with client mode. In that case, you only need to add the following configurations in spark-defaults.conf or in spark-submit command line before running. # absolute path of the jar for uploading spark.files /path/to/oap-mllib-x.x.x-with-spark-x.x.x.jar # absolute path of the jar for driver class path spark.driver.extraClassPath /path/to/oap-mllib-x.x.x-with-spark-x.x.x.jar # relative path to spark.files, just specify jar name in current dir spark.executor.extraClassPath ./oap-mllib-x.x.x-with-spark-x.x.x.jar","title":"Spark Configuration"},{"location":"OAPMLlib/#sanity-check","text":"To use K-means example for sanity check, you need to upload a data file to your HDFS and change related variables in run.sh of kmeans example. Then run the following commands: $ cd oap-mllib/examples/kmeans $ ./build.sh $ ./run.sh","title":"Sanity Check"},{"location":"OAPMLlib/#benchmark-with-hibench","text":"Use Hibench to generate dataset with various profiles, and change related variables in run-XXX.sh script when applicable. Then run the following commands: $ cd oap-mllib/examples/kmeans-hibench $ ./build.sh $ ./run-hibench-oap-mllib.sh","title":"Benchmark with HiBench"},{"location":"OAPMLlib/#pyspark-support","text":"As PySpark-based applications call their Scala couterparts, they shall be supported out-of-box. An example can be found in the Examples section.","title":"PySpark Support"},{"location":"OAPMLlib/#building","text":"","title":"Building"},{"location":"OAPMLlib/#prerequisites_1","text":"We use Apache Maven to manage and build source code. The following tools and libraries are also needed to build OAP MLlib: JDK 8.0+ Apache Maven 3.6.2+ GNU GCC 4.8.5+ Intel\u00ae oneAPI Toolkits 2021.1.1 Components: Data Analytics Library (oneDAL) Threading Building Blocks (oneTBB) Open Source Intel\u00ae oneAPI Collective Communications Library (oneCCL) Intel\u00ae oneAPI Toolkits and its components can be downloaded and install from here . Installation process for oneAPI using Package Managers (YUM (DNF), APT, and ZYPPER) is also available. Generally you only need to install oneAPI Base Toolkit for Linux with all or selected components mentioned above. Instead of using oneCCL included in Intel\u00ae oneAPI Toolkits, we prefer to build from open source oneCCL to resolve some bugs. More details about oneAPI can be found here . You can also refer to this script and comments in it to install correct oneAPI version and manually setup the environments. Scala and Java dependency descriptions are already included in Maven POM file.","title":"Prerequisites"},{"location":"OAPMLlib/#build","text":"","title":"Build"},{"location":"OAPMLlib/#building-oneccl","text":"To clone and build from open source oneCCL, run the following commands: $ git clone https://github.com/oneapi-src/oneCCL $ cd oneCCL $ git checkout beta08 $ mkdir build && cd build $ cmake .. $ make -j install The generated files will be placed in /your/oneCCL_source_code/build/_install","title":"Building oneCCL"},{"location":"OAPMLlib/#building-oap-mllib","text":"To clone and checkout source code, run the following commands: $ git clone https://github.com/oap-project/oap-mllib.git Optional to checkout specific release branch: $ cd oap-mllib && git checkout ${version} We rely on environment variables to find required toolchains and libraries. Please make sure the following environment variables are set for building: Environment Description JAVA_HOME Path to JDK home directory DAALROOT Path to oneDAL home directory TBB_ROOT Path to oneTBB home directory CCL_ROOT Path to oneCCL home directory We suggest you to source setvars.sh script into current shell to setup building environments as following: $ source /opt/intel/inteloneapi/setvars.sh $ source /your/oneCCL_source_code/build/_install/env/setvars.sh Be noticed we are using our own built oneCCL instead, we should source oneCCL's setvars.sh to overwrite oneAPI one. If you prefer to buid your own open source oneDAL , oneTBB versions rather than use the ones included in oneAPI TookKits, you can refer to the related build instructions and manually source setvars.sh accordingly. To build, run the following commands: $ cd oap-mllib/mllib-dal $ ./build.sh The built JAR package will be placed in target directory with the name oap-mllib-x.x.x-with-spark-x.x.x.jar .","title":"Building OAP MLlib"},{"location":"OAPMLlib/#examples","text":"Example Description kmeans K-means example for Scala kmeans-pyspark K-means example for PySpark kmeans-hibench Use HiBench-generated input dataset to benchmark K-means performance","title":"Examples"},{"location":"OAPMLlib/#list-of-accelerated-algorithms","text":"K-Means (CPU, Experimental)","title":"List of Accelerated Algorithms"},{"location":"OAPMLlib/OAP-Developer-Guide/","text":"OAP Developer Guide This document contains the instructions & scripts on installing necessary dependencies and building OAP. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Arrow Data Source Native SQL Engine Building OAP Prerequisites for Building OAP is built with Apache Maven and Oracle Java 8, and mainly required tools to install on your cluster are listed below. Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. We provide scripts below to help automatically install dependencies above except RDMA , need change to root account, run: # git clone -b <tag-version> # cd oap-tools # sh dev/install-compile-time-dependencies.sh Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven Building To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building Specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache","title":"OAP Developer Guide"},{"location":"OAPMLlib/OAP-Developer-Guide/#oap-developer-guide","text":"This document contains the instructions & scripts on installing necessary dependencies and building OAP. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Arrow Data Source Native SQL Engine","title":"OAP Developer Guide"},{"location":"OAPMLlib/OAP-Developer-Guide/#building-oap","text":"","title":"Building OAP"},{"location":"OAPMLlib/OAP-Developer-Guide/#prerequisites-for-building","text":"OAP is built with Apache Maven and Oracle Java 8, and mainly required tools to install on your cluster are listed below. Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. We provide scripts below to help automatically install dependencies above except RDMA , need change to root account, run: # git clone -b <tag-version> # cd oap-tools # sh dev/install-compile-time-dependencies.sh Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven","title":"Prerequisites for Building"},{"location":"OAPMLlib/OAP-Developer-Guide/#building","text":"To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building Specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache","title":"Building"},{"location":"OAPMLlib/OAP-Installation-Guide/","text":"OAP Installation Guide This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine. Contents Prerequisites Installing OAP Configuration Prerequisites OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly. Installing OAP Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Create a conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.0.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Extra Steps for Shuffle Remote PMem Extension If you use one of OAP features -- PMmem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details. Configuration Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar And then you can follow the corresponding feature documents for more details to use them.","title":"OAP Installation Guide"},{"location":"OAPMLlib/OAP-Installation-Guide/#oap-installation-guide","text":"This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine.","title":"OAP Installation Guide"},{"location":"OAPMLlib/OAP-Installation-Guide/#contents","text":"Prerequisites Installing OAP Configuration","title":"Contents"},{"location":"OAPMLlib/OAP-Installation-Guide/#prerequisites","text":"OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly.","title":"Prerequisites"},{"location":"OAPMLlib/OAP-Installation-Guide/#installing-oap","text":"Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Create a conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.0.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars","title":"Installing OAP"},{"location":"OAPMLlib/OAP-Installation-Guide/#extra-steps-for-shuffle-remote-pmem-extension","text":"If you use one of OAP features -- PMmem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details.","title":"Extra Steps for Shuffle Remote PMem Extension"},{"location":"OAPMLlib/OAP-Installation-Guide/#configuration","text":"Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar And then you can follow the corresponding feature documents for more details to use them.","title":"Configuration"},{"location":"OAPMLlib/User-Guide/","text":"OAP MLlib Overview OAP MLlib is an optimized package to accelerate machine learning algorithms in Apache Spark MLlib . It is compatible with Spark MLlib and leverages open source Intel\u00ae oneAPI Data Analytics Library (oneDAL) to provide highly optimized algorithms and get most out of CPU and GPU capabilities. It also take advantage of open source Intel\u00ae oneAPI Collective Communications Library (oneCCL) to provide efficient communication patterns in multi-node multi-GPU clusters. Compatibility OAP MLlib tried to maintain the same API interfaces and produce same results that are identical with Spark MLlib. However due to the nature of float point operations, there may be some small deviation from the original result, we will try our best to make sure the error is within acceptable range. For those algorithms that are not accelerated by OAP MLlib, the original Spark MLlib one will be used. Getting Started Java/Scala Users Preferred Use a pre-built OAP MLlib JAR to get started. You can firstly download OAP package from OAP-JARs-Tarball and extract this Tarball to get oap-mllib-x.x.x-with-spark-x.x.x.jar under oap-1.0.0-bin-spark-3.0.0/jars . Then you can refer to the following Running section to try out. Python/PySpark Users Preferred Use a pre-built JAR to get started. If you have finished OAP-Installation-Guide , you can find compiled OAP MLlib JAR oap-mllib-x.x.x-with-spark-x.x.x.jar in $HOME/miniconda2/envs/oapenv/oap_jars/ . Then you can refer to the following Running section to try out. Building From Scratch You can also build the package from source code, please refer to Building section. Running Prerequisites CentOS 7.0+, Ubuntu 18.04 LTS+ Java JRE 8.0+ Runtime Apache Spark 3.0.0+ Generally, our common system requirements are the same with Intel\u00ae oneAPI Toolkit, please refer to here for details. Intel\u00ae oneAPI Toolkits components used by the project are already included into JAR package mentioned above. There are no extra installations for cluster nodes. Spark Configuration Users usually run Spark application on YARN with client mode. In that case, you only need to add the following configurations in spark-defaults.conf or in spark-submit command line before running. # absolute path of the jar for uploading spark.files /path/to/oap-mllib-x.x.x-with-spark-x.x.x.jar # absolute path of the jar for driver class path spark.driver.extraClassPath /path/to/oap-mllib-x.x.x-with-spark-x.x.x.jar # relative path to spark.files, just specify jar name in current dir spark.executor.extraClassPath ./oap-mllib-x.x.x-with-spark-x.x.x.jar Sanity Check To use K-means example for sanity check, you need to upload a data file to your HDFS and change related variables in run.sh of kmeans example. Then run the following commands: $ cd oap-mllib/examples/kmeans $ ./build.sh $ ./run.sh Benchmark with HiBench Use Hibench to generate dataset with various profiles, and change related variables in run-XXX.sh script when applicable. Then run the following commands: $ cd oap-mllib/examples/kmeans-hibench $ ./build.sh $ ./run-hibench-oap-mllib.sh PySpark Support As PySpark-based applications call their Scala couterparts, they shall be supported out-of-box. An example can be found in the Examples section. Building Prerequisites We use Apache Maven to manage and build source code. The following tools and libraries are also needed to build OAP MLlib: JDK 8.0+ Apache Maven 3.6.2+ GNU GCC 4.8.5+ Intel\u00ae oneAPI Toolkits 2021.1.1 Components: Data Analytics Library (oneDAL) Threading Building Blocks (oneTBB) Open Source Intel\u00ae oneAPI Collective Communications Library (oneCCL) Intel\u00ae oneAPI Toolkits and its components can be downloaded and install from here . Installation process for oneAPI using Package Managers (YUM (DNF), APT, and ZYPPER) is also available. Generally you only need to install oneAPI Base Toolkit for Linux with all or selected components mentioned above. Instead of using oneCCL included in Intel\u00ae oneAPI Toolkits, we prefer to build from open source oneCCL to resolve some bugs. More details about oneAPI can be found here . You can also refer to this script and comments in it to install correct oneAPI version and manually setup the environments. Scala and Java dependency descriptions are already included in Maven POM file. Build Building oneCCL To clone and build from open source oneCCL, run the following commands: $ git clone https://github.com/oneapi-src/oneCCL $ cd oneCCL $ git checkout beta08 $ mkdir build && cd build $ cmake .. $ make -j install The generated files will be placed in /your/oneCCL_source_code/build/_install Building OAP MLlib To clone and checkout source code, run the following commands: $ git clone https://github.com/oap-project/oap-mllib.git Optional to checkout specific release branch: $ cd oap-mllib && git checkout ${version} We rely on environment variables to find required toolchains and libraries. Please make sure the following environment variables are set for building: Environment Description JAVA_HOME Path to JDK home directory DAALROOT Path to oneDAL home directory TBB_ROOT Path to oneTBB home directory CCL_ROOT Path to oneCCL home directory We suggest you to source setvars.sh script into current shell to setup building environments as following: $ source /opt/intel/inteloneapi/setvars.sh $ source /your/oneCCL_source_code/build/_install/env/setvars.sh Be noticed we are using our own built oneCCL instead, we should source oneCCL's setvars.sh to overwrite oneAPI one. If you prefer to buid your own open source oneDAL , oneTBB versions rather than use the ones included in oneAPI TookKits, you can refer to the related build instructions and manually source setvars.sh accordingly. To build, run the following commands: $ cd oap-mllib/mllib-dal $ ./build.sh The built JAR package will be placed in target directory with the name oap-mllib-x.x.x-with-spark-x.x.x.jar . Examples Example Description kmeans K-means example for Scala kmeans-pyspark K-means example for PySpark kmeans-hibench Use HiBench-generated input dataset to benchmark K-means performance List of Accelerated Algorithms K-Means (CPU, Experimental)","title":"User Guide"},{"location":"OAPMLlib/User-Guide/#oap-mllib","text":"","title":"OAP MLlib"},{"location":"OAPMLlib/User-Guide/#overview","text":"OAP MLlib is an optimized package to accelerate machine learning algorithms in Apache Spark MLlib . It is compatible with Spark MLlib and leverages open source Intel\u00ae oneAPI Data Analytics Library (oneDAL) to provide highly optimized algorithms and get most out of CPU and GPU capabilities. It also take advantage of open source Intel\u00ae oneAPI Collective Communications Library (oneCCL) to provide efficient communication patterns in multi-node multi-GPU clusters.","title":"Overview"},{"location":"OAPMLlib/User-Guide/#compatibility","text":"OAP MLlib tried to maintain the same API interfaces and produce same results that are identical with Spark MLlib. However due to the nature of float point operations, there may be some small deviation from the original result, we will try our best to make sure the error is within acceptable range. For those algorithms that are not accelerated by OAP MLlib, the original Spark MLlib one will be used.","title":"Compatibility"},{"location":"OAPMLlib/User-Guide/#getting-started","text":"","title":"Getting Started"},{"location":"OAPMLlib/User-Guide/#javascala-users-preferred","text":"Use a pre-built OAP MLlib JAR to get started. You can firstly download OAP package from OAP-JARs-Tarball and extract this Tarball to get oap-mllib-x.x.x-with-spark-x.x.x.jar under oap-1.0.0-bin-spark-3.0.0/jars . Then you can refer to the following Running section to try out.","title":"Java/Scala Users Preferred"},{"location":"OAPMLlib/User-Guide/#pythonpyspark-users-preferred","text":"Use a pre-built JAR to get started. If you have finished OAP-Installation-Guide , you can find compiled OAP MLlib JAR oap-mllib-x.x.x-with-spark-x.x.x.jar in $HOME/miniconda2/envs/oapenv/oap_jars/ . Then you can refer to the following Running section to try out.","title":"Python/PySpark Users Preferred"},{"location":"OAPMLlib/User-Guide/#building-from-scratch","text":"You can also build the package from source code, please refer to Building section.","title":"Building From Scratch"},{"location":"OAPMLlib/User-Guide/#running","text":"","title":"Running"},{"location":"OAPMLlib/User-Guide/#prerequisites","text":"CentOS 7.0+, Ubuntu 18.04 LTS+ Java JRE 8.0+ Runtime Apache Spark 3.0.0+ Generally, our common system requirements are the same with Intel\u00ae oneAPI Toolkit, please refer to here for details. Intel\u00ae oneAPI Toolkits components used by the project are already included into JAR package mentioned above. There are no extra installations for cluster nodes.","title":"Prerequisites"},{"location":"OAPMLlib/User-Guide/#spark-configuration","text":"Users usually run Spark application on YARN with client mode. In that case, you only need to add the following configurations in spark-defaults.conf or in spark-submit command line before running. # absolute path of the jar for uploading spark.files /path/to/oap-mllib-x.x.x-with-spark-x.x.x.jar # absolute path of the jar for driver class path spark.driver.extraClassPath /path/to/oap-mllib-x.x.x-with-spark-x.x.x.jar # relative path to spark.files, just specify jar name in current dir spark.executor.extraClassPath ./oap-mllib-x.x.x-with-spark-x.x.x.jar","title":"Spark Configuration"},{"location":"OAPMLlib/User-Guide/#sanity-check","text":"To use K-means example for sanity check, you need to upload a data file to your HDFS and change related variables in run.sh of kmeans example. Then run the following commands: $ cd oap-mllib/examples/kmeans $ ./build.sh $ ./run.sh","title":"Sanity Check"},{"location":"OAPMLlib/User-Guide/#benchmark-with-hibench","text":"Use Hibench to generate dataset with various profiles, and change related variables in run-XXX.sh script when applicable. Then run the following commands: $ cd oap-mllib/examples/kmeans-hibench $ ./build.sh $ ./run-hibench-oap-mllib.sh","title":"Benchmark with HiBench"},{"location":"OAPMLlib/User-Guide/#pyspark-support","text":"As PySpark-based applications call their Scala couterparts, they shall be supported out-of-box. An example can be found in the Examples section.","title":"PySpark Support"},{"location":"OAPMLlib/User-Guide/#building","text":"","title":"Building"},{"location":"OAPMLlib/User-Guide/#prerequisites_1","text":"We use Apache Maven to manage and build source code. The following tools and libraries are also needed to build OAP MLlib: JDK 8.0+ Apache Maven 3.6.2+ GNU GCC 4.8.5+ Intel\u00ae oneAPI Toolkits 2021.1.1 Components: Data Analytics Library (oneDAL) Threading Building Blocks (oneTBB) Open Source Intel\u00ae oneAPI Collective Communications Library (oneCCL) Intel\u00ae oneAPI Toolkits and its components can be downloaded and install from here . Installation process for oneAPI using Package Managers (YUM (DNF), APT, and ZYPPER) is also available. Generally you only need to install oneAPI Base Toolkit for Linux with all or selected components mentioned above. Instead of using oneCCL included in Intel\u00ae oneAPI Toolkits, we prefer to build from open source oneCCL to resolve some bugs. More details about oneAPI can be found here . You can also refer to this script and comments in it to install correct oneAPI version and manually setup the environments. Scala and Java dependency descriptions are already included in Maven POM file.","title":"Prerequisites"},{"location":"OAPMLlib/User-Guide/#build","text":"","title":"Build"},{"location":"OAPMLlib/User-Guide/#building-oneccl","text":"To clone and build from open source oneCCL, run the following commands: $ git clone https://github.com/oneapi-src/oneCCL $ cd oneCCL $ git checkout beta08 $ mkdir build && cd build $ cmake .. $ make -j install The generated files will be placed in /your/oneCCL_source_code/build/_install","title":"Building oneCCL"},{"location":"OAPMLlib/User-Guide/#building-oap-mllib","text":"To clone and checkout source code, run the following commands: $ git clone https://github.com/oap-project/oap-mllib.git Optional to checkout specific release branch: $ cd oap-mllib && git checkout ${version} We rely on environment variables to find required toolchains and libraries. Please make sure the following environment variables are set for building: Environment Description JAVA_HOME Path to JDK home directory DAALROOT Path to oneDAL home directory TBB_ROOT Path to oneTBB home directory CCL_ROOT Path to oneCCL home directory We suggest you to source setvars.sh script into current shell to setup building environments as following: $ source /opt/intel/inteloneapi/setvars.sh $ source /your/oneCCL_source_code/build/_install/env/setvars.sh Be noticed we are using our own built oneCCL instead, we should source oneCCL's setvars.sh to overwrite oneAPI one. If you prefer to buid your own open source oneDAL , oneTBB versions rather than use the ones included in oneAPI TookKits, you can refer to the related build instructions and manually source setvars.sh accordingly. To build, run the following commands: $ cd oap-mllib/mllib-dal $ ./build.sh The built JAR package will be placed in target directory with the name oap-mllib-x.x.x-with-spark-x.x.x.jar .","title":"Building OAP MLlib"},{"location":"OAPMLlib/User-Guide/#examples","text":"Example Description kmeans K-means example for Scala kmeans-pyspark K-means example for PySpark kmeans-hibench Use HiBench-generated input dataset to benchmark K-means performance","title":"Examples"},{"location":"OAPMLlib/User-Guide/#list-of-accelerated-algorithms","text":"K-Means (CPU, Experimental)","title":"List of Accelerated Algorithms"},{"location":"PMemCommon/","text":"PMem Common PMem common package includes native libraries and JNI interface for Intel Optane PMem. Prerequisites Below libraries need to be installed in the machine Memkind Vmemcache Building mvn clean package -Ppersistent-memory,vmemcache","title":"PMem Common"},{"location":"PMemCommon/#pmem-common","text":"PMem common package includes native libraries and JNI interface for Intel Optane PMem.","title":"PMem Common"},{"location":"PMemCommon/#prerequisites","text":"Below libraries need to be installed in the machine Memkind Vmemcache","title":"Prerequisites"},{"location":"PMemCommon/#building","text":"mvn clean package -Ppersistent-memory,vmemcache","title":"Building"},{"location":"PMemCommon/OAP-Developer-Guide/","text":"OAP Developer Guide This document contains the instructions & scripts on installing necessary dependencies and building OAP. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Arrow Data Source Native SQL Engine Building OAP Prerequisites for Building OAP is built with Apache Maven and Oracle Java 8, and mainly required tools to install on your cluster are listed below. Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. We provide scripts below to help automatically install dependencies above except RDMA , need change to root account, run: # git clone -b <tag-version> # cd oap-tools # sh dev/install-compile-time-dependencies.sh Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven Building To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building Specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache","title":"OAP Developer Guide"},{"location":"PMemCommon/OAP-Developer-Guide/#oap-developer-guide","text":"This document contains the instructions & scripts on installing necessary dependencies and building OAP. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Arrow Data Source Native SQL Engine","title":"OAP Developer Guide"},{"location":"PMemCommon/OAP-Developer-Guide/#building-oap","text":"","title":"Building OAP"},{"location":"PMemCommon/OAP-Developer-Guide/#prerequisites-for-building","text":"OAP is built with Apache Maven and Oracle Java 8, and mainly required tools to install on your cluster are listed below. Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. We provide scripts below to help automatically install dependencies above except RDMA , need change to root account, run: # git clone -b <tag-version> # cd oap-tools # sh dev/install-compile-time-dependencies.sh Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven","title":"Prerequisites for Building"},{"location":"PMemCommon/OAP-Developer-Guide/#building","text":"To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building Specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache","title":"Building"},{"location":"PMemCommon/OAP-Installation-Guide/","text":"OAP Installation Guide This document introduces how to install OAP and its dependencies on your cluster nodes by Conda* . Follow steps below on every node*** of your cluster to set right environment for each machine. Contents Prerequisites Installing OAP Configuration Prerequisites OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly. Installing OAP Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Create a conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.0.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Extra Steps for Shuffle Remote PMem Extension If you use one of OAP features -- PMmem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details. Configuration Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar And then you can follow the corresponding feature documents for more details to use them.","title":"OAP Installation Guide"},{"location":"PMemCommon/OAP-Installation-Guide/#oap-installation-guide","text":"This document introduces how to install OAP and its dependencies on your cluster nodes by Conda* . Follow steps below on every node*** of your cluster to set right environment for each machine.","title":"OAP Installation Guide"},{"location":"PMemCommon/OAP-Installation-Guide/#contents","text":"Prerequisites Installing OAP Configuration","title":"Contents"},{"location":"PMemCommon/OAP-Installation-Guide/#prerequisites","text":"OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly.","title":"Prerequisites"},{"location":"PMemCommon/OAP-Installation-Guide/#installing-oap","text":"Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Create a conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.0.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars","title":"Installing OAP"},{"location":"PMemCommon/OAP-Installation-Guide/#extra-steps-for-shuffle-remote-pmem-extension","text":"If you use one of OAP features -- PMmem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details.","title":"Extra Steps for Shuffle Remote PMem Extension"},{"location":"PMemCommon/OAP-Installation-Guide/#configuration","text":"Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar And then you can follow the corresponding feature documents for more details to use them.","title":"Configuration"},{"location":"PMemCommon/User-Guide/","text":"PMem Common PMem common package includes native libraries and JNI interface for Intel Optane PMem. Prerequisites Below libraries need to be installed in the machine Memkind Vmemcache Building mvn clean package -Ppersistent-memory,vmemcache","title":"User Guide"},{"location":"PMemCommon/User-Guide/#pmem-common","text":"PMem common package includes native libraries and JNI interface for Intel Optane PMem.","title":"PMem Common"},{"location":"PMemCommon/User-Guide/#prerequisites","text":"Below libraries need to be installed in the machine Memkind Vmemcache","title":"Prerequisites"},{"location":"PMemCommon/User-Guide/#building","text":"mvn clean package -Ppersistent-memory,vmemcache","title":"Building"},{"location":"PMemShuffle/","text":"PMem Shuffle for Apache Spark Guide 1. PMem Shuffle introduction 2. Recommended HW environment 3. Install and configure PMem 4. Configure and Validate RDMA 5. Install dependencies for PMem Shuffle 6. Install PMem Shuffle for Spark 7. PMem Shuffle for Spark Testing Reference PMem Shuffle for Spark (previously Spark-PMoF) depends on multiple native libraries like libfabrics, libcuckoo, PMDK. This enabling guide covers the installing process for the time being, but it might change as the install commands and related dependency packages for the 3rd party libraries might vary depending on the OS version and distribution you are using. Yarn, HDFS, Spark installation and configuration is out of the scope of this document. 1. PMem Shuffle introduction Intel Optane DC persistent memory is the next-generation storage at memory speed. It closes the performance gap between DRAM memory technology and traditional NAND SSDs. Remote Persistent Memory extends PMem usage to new scenario, lots of new usage cases & value proposition can be developed. Spark shuffle is a high cost operation as it issues a great number of small random disk IO, serialization, network data transmission, and thus contributes a lot to job latency and could be the bottleneck for workloads performance. PMem Shuffle for spark (previously Spark PMoF) https://github.com/Intel-bigdata/Spark-PMoF ) is a Persistent Memory over Fabrics (PMoF) plugin for Spark shuffle, which leverages the RDMA network and remote persistent memory (for read) to provide extremely high performance and low latency shuffle solutions for Spark to address performance issues for shuffle intensive workloads. PMem Shuffle brings follow benefits: Leverage high performance persistent memory as shuffle media as well as spill media, increased shuffle performance and reduced memory footprint Using PMDK libs to avoid inefficient context switches and memory copies with zero-copy remote access to persistent memory. Leveraging RDMA for network offloading The Figure 1 shows the high level architecture of PMem Shuffle, it shows how data flows between Spark and shuffle devices in PMem Shuffle for spark shuffle and Vanilla Spark. In this guide, we will introduce how to deploy and use PMem Shuffle for Spark. Figure 1: PMem Shuffle for Spark 2. Recommended HW environment 2.1. System Configuration 2.1.1 HW and SW Configuration A 4x or 3x Node cluster is recommended for a proof of concept tests, depending your system configurations, if using 3 nodes cluster, the Name node and Spark Master node can be co-located with one of the Hadoop data nodes. Hardware: - Intel\u00ae Xeon\u2122 processor Gold 6240 CPU @ 2.60GHz, 384GB Memory (12x 32GB 2666 MT/s) or 192GB Memory (12x 16GB 2666MT/s) - An RDMA capable NIC, 40Gb+ is preferred. e.g., 1x Intel X722 NIC or Mellanox ConnectX-4 40Gb NIC - RDMA cables: - Mellanox MCP1600-C003 100GbE 3m 28AWG - Shuffle Devices\uff1a - 1x 1TB HDD for shuffle (baseline) - 4x 128GB Persistent Memory for shuffle - 4x 1T NVMe for HDFS Switch : - Arista 7060 CX2 (7060CX2-32S-F) 100Gb switches was used Please refer to section 4.2 for configurations Software: Hadoop 2.7 Spark 3.0.0 Fedora 29 with ww08.2019 BKC 2.2. Recommended RDMA NIC PMem Shuffle is using HPNL ( https://cloud.google.com/solutions/big-data/ ) for network communication, which leverages libfabric for efficient network communication, so a RDMA capable NIC is recommended. Libfabric supports RoCE, iWrap, IB protocol, so various RNICs with different protocol can be used. 2.3 Recommended PMEM configuration It is recommended to install 4+ PMem DIMMs on the SUT, but you can adjust the numbers accordingly. In this enabling guide, 4x 128GB PMEMM was installed on the SUT as an exmaple. 2.4 Recommended PMEM BKC (optional) This development guide was based on ww08.2019 BKC (best known configuration). Please contact your HW vendor for latest BKC. Please refer to backup if you do not have BKC access. BKC installation/enabling or FW installation is out of the scope of this guide. 3. Install and configure PMEM (example) 1) Please install ipmctl and ndctl according to your OS version 2) Run ipmctl show -dimm to check whether dimms can be recognized 3) Run ipmctl create -goal PersistentMemoryType=AppDirect to create AD mode 4) Run ndctl list -R , you will see region0 and region1 . 5) Assume you have 4x PMEM installed on 1 node. a. Run ndctl create-namespace -m devdax -r region0 -s 120g b. Run ndctl create-namespace -m devdax -r region0 -s 120g c. Run ndctl create-namespace -m devdax -r region1 -s 120g d. Run ndctl create-namespace -m devdax -r region1 -s 120g This will create four namespaces, namely /dev/dax0.0, /dev/dax0.1, /dev/dax1.0, /dev/dax1.1 in that node, and it will be used as PMem Shuffle media. You can change your configuration (namespaces numbers, size) accordingly. 4. Configure and Validate RDMA Notes This part is vendor specific, it might NOT apply to your environment, please check your switch, NIC manuals accordingly. 4.1 Configure and test iWARP RDMA 4.1.1 Download rdma-core and install dependencies The rdma-core provides the necessary userspace libraries to test rdma connectivity with tests such as rping. Refer to latest rdma-core documentation for updated installation guidelines (https://github.com/linux-rdma/rdma-core.git). You might refer to HW specific instructions or guide to enable your RDMA NICs. Take Mellanox as an example, perform below steps to enable it: git clone <https://github.com/linux-rdma/rdma-core.git> dnf install cmake gcc libnl3-devel libudev-devel pkgconfig valgrind-devel ninja-build python3-devel python3-Cython python3-docutils pandoc //change to yum on centos bash build.sh #on centos 7 yum install cmake gcc libnl3-devel libudev-devel make pkgconfig valgrind-devel yum install epel-release yum install cmake3 ninja-build pandoc 4.1.2 Switch Configuration (optional) This part is HW specific, please check your switch manual accordingly. Connect the console port to PC. Username is admin. No password. Enter global configuration mode. Below example is based on Arista 7060 CX2 100Gb Switch, it is to configure the 100Gb port to work at 40Gb to match the NIC speed. It is NOT required if your NIC and calbes are match. Config Switch Speed to 40Gb/s switch# enable switch# config switch(config)# show interface status Configure corresponding port to 40 Gb/s to match the NIC speed switch(config)# interface Et(num_of_port)/1 switch(config)# speed forced 40gfull RoCE might have performance issues, so PFC configuration is strongly suggested. You will need to check the RDMA NIC driver manual and switch manual to configure PFC. Below is the example for ConnectX-4 and Arista 7060-CX2 switches. Below is to set the two connection ports in the same vlan and configure it in trunk mode. Configure interface as trunk mode and add to vlan switch(config)# vlan 1 switch(config-vlan-1)# switch(config)# interface ethernet 12-16 switch(config-if-Et12-16)# switchport trunk allowed vlan 1 switch (config-if-et1) # **priority-flow-control on** switch (config-if-et1) # **priority-flow-control priority 3 no-drop** 4.1.3 Download and install drivers A. Example: Mellanox Enabling RoCE V2 RDMA (Optional) There are lots of packages need to be installed for dependency, please refer to your RDMA NIC's manualls to install it correctly. yum install atk gcc-gfortran tcsh gtk2 tcl tk please install NIC drivers accordingly. # Download MLNX_OFED_LINUX-4.7-3.2.9.0-* from https://community.mellanox.com/s/article/howto-install-mlnx-ofed-driver # e.g., wget http://www.mellanox.com/downloads/ofed/MLNX_OFED-<version>/MLNX_OFED_LINUX-<version>-<distribution>-<arch>.tgz . tar zxf MLNX_OFED_LINUX-4.7-3.2.9.0-* cd MLNX_OFED_LINUX-4.7-3.2.9.0- ./mlnxofedinstall --add-kernel-support. # The process might interpret and promote you to install dependencies. Install dependencies and try again # This process will take some time. tar zxf MLNX_OFED_LINUX-4.7-3.2.9.0-* cd MLNX_OFED_LINUX-4.7-3.2.9.0- ./mlnxofedinstall --add-kernel-support. # The process might interpret and promote you to install dependencies. Install dependencies and try again # This process will take some time. ** Restart the driver: /etc/init.d/openibd restart Might need to unload the modules if it is in use. Make sure the that the field link_layer is \u201cEthernet\u201d. Then you can use following command to get the device name. B. Enable PFC (Priority Flow Control) to guarantee stable performance (optional) Then you can use following command to get the device name If you\u2019re using Mellanox NIC, PFC is a must to guarantee stable performance. Fetch RDMA info with rdma command: rdma link 0/1: i40iw0/1: state DOWN physical_state NOP 1/1: i40iw1/1: state ACTIVE physical_state NOP 2/1: mlx5_0/1: state DOWN physical_state DISABLED netdev ens803f0 3/1: mlx5_1/1: state ACTIVE physical_state LINK_UP netdev ens803f1 lspci | grep Mellanox 86:00.0 Ethernet controller: Mellanox Technologies MT27700 Family [ConnectX-4] 86:00.1 Ethernet controller: Mellanox Technologies MT27700 Family [ConnectX-4] Set PFC: /etc/init.d/openibd restart mlnx_qos -i ens803f1 --pfc 0,0,0,1,0,0,0,0 modprobe 8021q vconfig add ens803f1 100 ifconfig ens803f1.100 $ip1/$mask up //change to your own IP ifconfig ens803f1 $ip2/$mask up //Change to your own IP for i in {0..7}; do vconfig set_egress_map ens803f1.100 $i 3 ; done tc_wrap.py -i ens803f1 -u 3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3 Modify the IP address part based on your environment and execute the script. 4.1.4 Check RDMA module Make sure the following modules are loaded: modprobe ib_core i40iw iw_cm rdma_cm rdma_ucm ib_cm ib_uverbs ``` #### 4.1.5 Validate RDMA functionalities Check that you see your RDMA interfaces listed on each server when you run the following command: **ibv_devices** Check with rping for RDMA connectivity between target interface and client interface. 1) Assign IPs to the RDMA interfaces on Target and Client. 2) On Target run:rping -sdVa &lt;Target IP&gt; 3) On Client run: rping -cdVa &lt;Target IP&gt; Example: On the server side: ```bash rping -sda $ip1 created cm_id 0x17766d0 rdma_bind_addr successful rdma_listen accepting client connection request cq_thread started. recv completion Received rkey 97a4f addr 17ce190 len 64 from peer cma_event type RDMA_CM_EVENT_ESTABLISHED cma_id 0x7fe9ec000c90 (child) ESTABLISHED Received rkey 96b40 addr 17ce1e0 len 64 from peer server received sink adv rdma write from lkey 143c0 laddr 1771190 len 64 rdma write completion rping -sda $ip2 created cm_id 0x17766d0 rdma_bind_addr successful rdma_listen \u2026 accepting client connection request cq_thread started. recv completion Received rkey 97a4f addr 17ce190 len 64 from peer cma_event type RDMA_CM_EVENT_ESTABLISHED cma_id 0x7fe9ec000c90 (child) ESTABLISHED \u2026 Received rkey 96b40 addr 17ce1e0 len 64 from peer server received sink adv rdma write from lkey 143c0 laddr 1771190 len 64 rdma write completion \u2026 ``` On Client run: rping -cdVa &lt;Target IP&gt; ```bash # Client side use .100 ip 172.168.0.209 for an example rping -c -a 172.168.0.209 -v -C 4 ping data: rdma-ping-0: ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqr ping data: rdma-ping-1: BCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrs ping data: rdma-ping-2: CDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrst ping data: rdma-ping-3: DEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstu Please refer to your NIC manuual for detail instructions on how to validate RDMA works. 5. Install dependencies for PMem Shuffle We have provided a Conda package which will automatically install dependencies needed for PMem Shuffle, refer to OAP-Installation-Guide for more information. If you have finished OAP-Installation-Guide , you can find compiled OAP jars in $HOME/miniconda2/envs/oapenv/oap_jars/ , and skip this session and jump to 6.Install PMem Shuffle for Spark 5.1 Install HPNL ( https://github.com/Intel-bigdata/HPNL ) HPNL is a fast, CPU-Efficient network library designed for modern network technology. HPNL depends on Libfabric, which is protocol independent, it supports TCP/IP, RoCE, IB, iWRAP etc. Please make sure the Libfabric is installed in your setup. Based on this issue , please make sure NOT to install Libfabric 1.9.0. You might need to install automake/libtool first to resolve dependency issues. git clone https://github.com/ofiwg/libfabric.git cd libfabric git checkout v1.6.0 ./autogen.sh ./configure --disable-sockets --enable-verbs --disable-mlx make -j && sudo make install 5.1.1 Build and install HPNL Assume Project_root_path is HPNL folder\u2019s path, HPNL here. sudo apt-get install cmake libboost-dev libboost-system-dev #Fedora dnf install cmake boost-devel boost-system git clone https://github.com/Intel-bigdata/HPNL.git cd HPNL git checkout origin/spark-pmof-test --track git submodule update --init --recursive mkdir build; cd build cmake -DWITH_VERBS=ON .. make -j && make install cd ${project_root_path}/java/hpnl mvn install 5.2 install basic C library dependencies yum install -y autoconf asciidoctor kmod-devel.x86\\_64 libudev-devel libuuid-devel json-c-devel jemalloc-devel yum groupinstall -y \"Development Tools\" 5.3 install ndctl This can be installed with your package managmenet tool as well. git clone https://github.com/pmem/ndctl.git cd ndctl git checkout v63 ./autogen.sh ./configure CFLAGS='-g -O2' --prefix=/usr --sysconfdir=/etc --libdir=/usr/lib64 make -j make check make install 5.4 install PMDK yum install -y pandoc git clone https://github.com/pmem/pmdk.git cd pmdk git checkout tags/1.8 make -j && make install export PKG_CONFIG_PATH=/usr/local/lib64/pkgconfig/:$PKG_CONFIG_PATH echo \u201cexport PKG_CONFIG_PATH=/usr/local/lib64/pkgconfig/:$PKG_CONFIG_PATH\u201d > /etc/profile.d/pmdk.sh 5.5 Install RPMem extension git clone https://github.com/efficient/libcuckoo cd libcuckoo mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr/local -DBUILD_EXAMPLES=1 -DBUILD_TESTS=1 .. make all && make install git clone -b <tag-version> https://github.com/intel-bigdata/OAP.git cd OAP/oap-shuffle/RPMem-shuffle mvn install -DskipTests 6. Install PMem Shuffle for Spark 6.1 Configure RPMem extension for spark shuffle in Spark PMem Shuffle for spark shuffle is designed as a plugin to Spark. Currently the plugin supports Spark 3.0.0 and works well on various Network fabrics, including Socket, RDMA and Omni-Path. There are several configurations files needs to be modified in order to run PMem Shuffle. Prerequisite Use below command to remove original initialization of one PMem, this is a MUST step, or RPMemShuffle won\u2019t be able to open PMem devices. pmempool rm ${device_name} #example: pmempool rm /dev/dax0.0 If you install OAP Conda package , you can use below command to remove original initialization of one PMem. ```shell script export LD_LIBRARY_PATH=$HOME/miniconda2/envs/oapenv/lib/:$LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/bin/pmempool rm ${device_name} **Refer to the Reference section for detail descrption of each parameter.** #### Enable RPMemShuffle ```bash spark.shuffle.manager org.apache.spark.shuffle.pmof.PmofShuffleManager spark.driver.extraClassPath /$path/oap-shuffle/RPMem-shuffle/core/target/oap-rpmem-shuffle-java-<version>-jar-with-dependencies.jar spark.executor.extraClassPath /$path/oap-shuffle/RPMem-shuffle/core/target/oap-rpmem-shuffle-java-<version>-jar-with-dependencies.jar Switch On/Off PMem and RDMA spark.shuffle.pmof.enable_rdma true spark.shuffle.pmof.enable_pmem true Add PMem information to spark config Explanation: spark.shuffle.pmof.pmem_capacity: the capacity of one PMem device, this value will be used when register PMem device to RDMA. spark.shuffle.pmof.pmem_list: a list of all local PMem device, make sure your per physical node executor number won\u2019t exceed PMem device number, or one PMem device maybe opened by two spark executor processes and this will leads to a PMem open failure. spark.shuffle.pmof.dev_core_set: a mapping of which core range will be task set to which PMem device, this is a performance optimal configuration for better PMem numa accessing. spark.io.compression.codec: use \u201csnappy\u201d to do shuffle data and spilling data compression, this is a MUST when enabled PMem due to a default LZ4 ByteBuffer incompatible issue. spark.shuffle.pmof.pmem_capacity ${total_size_of_one_device} spark.shuffle.pmof.pmem_list ${device_name},${device_name},\u2026 spark.shuffle.pmof.dev_core_set ${device_name}:${core_range};\u2026 #example: /dev/dax0.0:0-17,36-53;/dev/dax0.2:0-17,36-53 spark.io.compression.codec snappy Memory configuration suggestion Suitable for any release before OAP 0.8. In OAP 0.8 and later release, the memory footprint of each core is reduced dramatically and the formula below is not applicable any more. Spark.executor.memory must be greater than shuffle_block_size * numPartitions * numCores * 2 (for both shuffle and external sort), for example, default HiBench Terasort numPartition is 200, and we configured 10 cores each executor, then this executor must has memory capacity greater than 2MB(spark.shuffle.pmof.shuffle_block_size) * 200 * 10 * 2 = 8G. Recommendation configuration as below, but it needs to be adjusted accordingly based on your system configurations. Yarn.executor.num 4 // same as PMem namespaces number Yarn.executor.cores 18 // total core number divide executor number spark.executor.memory 15g // 2MB * numPartition(200) * 18 * 2 spark.yarn.executor.memoryOverhead 5g // 30% of spark.executor.memory spark.shuffle.pmof.shuffle_block_size 2096128 // 2MB \u2013 1024 Bytes spark.shuffle.pmof.spill_throttle 2096128 // 2MB \u2013 1024 Bytes, spill_throttle is used to // set throttle by when spill buffer data to // Persistent Memory, must set spill_throttle // equal to shuffle_block_size spark.driver.memory 10g spark.yarn.driver.memoryOverhead 5g Configuration of RDMA enabled case spark.shuffle.pmof.node : spark nodes and RDMA ip mapping list spark.driver.rhost / spark.driver.rport : Specify spark driver RDMA IP and port spark.shuffle.pmof.server_buffer_nums 64 spark.shuffle.pmof.client_buffer_nums 64 spark.shuffle.pmof.map_serializer_buffer_size 262144 spark.shuffle.pmof.reduce_serializer_buffer_size 262144 spark.shuffle.pmof.chunk_size 262144 spark.shuffle.pmof.server_pool_size 3 spark.shuffle.pmof.client_pool_size 3 spark.shuffle.pmof.node $HOST1-$IP1,$HOST2-$IP2//Host-IP pairs, $hostname-$ip spark.driver.rhost $IP //change to your host IP spark.driver.rport 61000 7. PMem Shuffle for Spark Testing Pmem shuffle extension have been tested and validated with Terasort and Decision support workloads. 7.1 Decision support workloads The Decision support workloads is a decision support benchmark that models several general applicable aspects of a decision support system, including queries and data maintenance. 7.1.1 Download spark-sql-perf The link is https://github.com/databricks/spark-sql-perf and follow README to use sbt build the artifact. 7.1.2 Download the kit As per instruction from spark-sql-perf README, tpcds-kit is required and please download it from https://github.com/databricks/tpcds-kit , follow README to setup the benchmark. 7.1.3 Prepare data As an example, generate parquet format data to HDFS with 1TB data scale. The data stored path, data format and data scale are configurable. Please check script below as a sample. import com.databricks.spark.sql.perf.tpcds.TPCDSTables import org.apache.spark.sql._ // Set: val rootDir: String = \"hdfs://${ip}:9000/tpcds_1T\" // root directory of location to create data in. val databaseName: String = \"tpcds_1T\" // name of database to create. val scaleFactor: String = \"1024\" // scaleFactor defines the size of the dataset to generate (in GB). val format: String = \"parquet\" // valid spark format like parquet \"parquet\". val sqlContext = new SQLContext(sc) // Run: val tables = new TPCDSTables(sqlContext, dsdgenDir = \"/mnt/spark-pmof/tool/tpcds-kit/tools\", // location of dsdgen scaleFactor = scaleFactor, useDoubleForDecimal = false, // true to replace DecimalType with DoubleType useStringForDate = false) // true to replace DateType with StringType tables.genData( location = rootDir, format = format, overwrite = true, // overwrite the data that is already there partitionTables = true, // create the partitioned fact tables clusterByPartitionColumns = true, // shuffle to get partitions coalesced into single files. filterOutNullPartitionValues = false, // true to filter out the partition with NULL key value tableFilter = \"\", // \"\" means generate all tables numPartitions = 400) // how many dsdgen partitions to run - number of input tasks. // Create the specified database sql(s\"create database $databaseName\") // Create metastore tables in a specified database for your data. // Once tables are created, the current database will be switched to the specified database. tables.createExternalTables(rootDir, \"parquet\", databaseName, overwrite = true, discoverPartitions = true) 7.1.4 Run the benchmark Launch DECISION SUPPORT WORKLOADS queries on generated data, check benchmark.scala below as a sample, it runs query64. import com.databricks.spark.sql.perf.tpcds.TPCDS import org.apache.spark.sql._ val sqlContext = new SQLContext(sc) val tpcds = new TPCDS (sqlContext = sqlContext) // Set: val databaseName = \"tpcds_1T\" // name of database with TPCDS data. val resultLocation = \"tpcds_1T_result\" // place to write results val iterations = 1 // how many iterations of queries to run. val query_filter = Seq(\"q64-v2.4\") val randomizeQueries = false def queries = { val filtered_queries = query_filter match { case Seq() => tpcds.tpcds2_4Queries case _=> tpcds.tpcds2_4Queries.filter(q => query_filter.contains(q.name)) } filtered_queries } val timeout = 24*60*60 // timeout, in seconds. // Run: sql(s\"use $databaseName\") val experiment = tpcds.runExperiment( queries, iterations = iterations, resultLocation = resultLocation, forkThread = true) experiment.waitForFinish(timeout) 7.1.5 Check the result Check the result under tpcds_1T_result folder. It can be an option to check the result at spark history server. (Need to start history server by \\$SPARK_HOME/sbin/start-history-server.sh ) 7.2 TeraSort TeraSort is a benchmark that measures the amount of time to sort one terabyte of randomly distributed data on a given computer system. 7.2.1 Download HiBench This guide uses HiBench for Terasort tests, https://github.com/Intel-bigdata/HiBench . HiBench is a big data benchmark suite and contains a set of Hadoop, Spark and streaming workloads including TeraSort. 7.2.2 Build HiBench as per instructions from build-bench . 7.2.3 Configuration Modify \\$HiBench-HOME/conf/spark.conf to specify the spark home and other spark configurations. It will overwrite the configuration of \\$SPARK-HOME/conf/spark-defaults.conf at run time. 7.2.4 Launch the benchmark Need to prepare the data with \\$HiBench-HOME/bin/workloads/micro/terasort/prepare/prepare.sh Kick off the evaluation by \\$HiBench-HOME/bin/workloads/micro/terasort/spark/run.sh Change directory to \\$HiBench-HOME/bin/workloads/micro/terasort/spark and launch the run.sh . You can add some PMEM cleaning work to make sure it starts from empty shuffle device every test iteration. Take run.sh below as a sample. # ***Change below command accordingly *** ssh ${node} pmempool rm /dev/dax0.0 current_dir=`dirname \"$0\"` current_dir=`cd \"$current_dir\"; pwd` root_dir=${current_dir}/../../../../.. workload_config=${root_dir}/conf/workloads/micro/terasort.conf . \"${root_dir}/bin/functions/load_bench_config.sh\" enter_bench ScalaSparkTerasort ${workload_config} ${current_dir} show_bannar start rmr_hdfs $OUTPUT_HDFS || true SIZE=`dir_size $INPUT_HDFS` START_TIME=`timestamp` run_spark_job com.intel.hibench.sparkbench.micro.ScalaTeraSort $INPUT_HDFS $OUTPUT_HDFS END_TIME=`timestamp` gen_report ${START_TIME} ${END_TIME} ${SIZE} show_bannar finish leave_bench 7.2.5 Check the result Check the result at spark history server to see the execution time and other spark metrics like spark shuffle spill status. (Need to start history server by \\$SPARK_HOME/sbin/start-history-server.sh ) Reference RPMemShuffle Spark configuration Before running Spark workload, add following contents in spark-defaults.conf . spark.executor.instances 4 // same as total PMem namespace numbers of your cluster spark.executor.cores 18 // total core number divide executor number spark.executor.memory 70g // 4~5G * spark.executor.cores spark.executor.memoryOverhead 15g // 30% of spark.executor.memory spark.shuffle.pmof.shuffle_block_size 2096128 // 2MB \u2013 1024 Bytes spark.shuffle.pmof.spill_throttle 2096128 // 2MB \u2013 1024 Bytes spark.driver.memory 10g spark.yarn.driver.memoryOverhead 5g spark.shuffle.compress true spark.io.compression.codec snappy spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-rpmem-shuffle-java-<version>-jar-with-dependencies.jar spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-rpmem-shuffle-java-<version>-jar-with-dependencies.jar spark.shuffle.manager org.apache.spark.shuffle.pmof.PmofShuffleManager spark.shuffle.pmof.enable_rdma true spark.shuffle.pmof.enable_pmem true spark.shuffle.pmof.pmem_capacity 126833655808 // size should be same as pmem size spark.shuffle.pmof.pmem_list /dev/dax0.0,/dev/dax0.1,/dev/dax1.0,/dev/dax1.1 spark.shuffle.pmof.dev_core_set dax0.0:0-71,dax0.1:0-71,dax1.0:0-71,dax1.1:0-71 spark.shuffle.pmof.server_buffer_nums 64 spark.shuffle.pmof.client_buffer_nums 64 spark.shuffle.pmof.map_serializer_buffer_size 262144 spark.shuffle.pmof.reduce_serializer_buffer_size 262144 spark.shuffle.pmof.chunk_size 262144 spark.shuffle.pmof.server_pool_size 3 spark.shuffle.pmof.client_pool_size 3 spark.shuffle.pmof.node $host1-$IP1,$host2-$IP2//HOST-IP Pair, seperate with \",\" spark.driver.rhost $IP //change to your host spark.driver.rport 61000 Reference guides (without BKC access) If you do not have BKC access, please following below official guide: (1): General PMEMM support: PMEMM support https://www.intel.com/content/www/us/en/support/products/190349/memory-and-storage/data-center-persistent-memory/intel-optane-dc-persistent-memory.html (2) PMEMM population rule: Module DIMM Population for Intel\u00ae Optane\u2122 DC Persistent Memory https://www.intel.com/content/www/us/en/support/articles/000032932/memory-and-storage/data-center-persistent-memory.html?productId=190349&localeCode=us_en (3) OS support requirement: Operating System OS for Intel\u00ae Optane\u2122 DC Persistent Memory https://www.intel.com/content/www/us/en/support/articles/000032860/memory-and-storage/data-center-persistent-memory.html?productId=190349&localeCode=us_en (4): Quick Start Guide: Provision Intel\u00ae Optane\u2122 DC Persistent Memory https://software.intel.com/en-us/articles/quick-start-guide-configure-intel-optane-dc-persistent-memory-on-linux","title":"PMem Shuffle for Apache Spark Guide"},{"location":"PMemShuffle/#pmem-shuffle-for-apache-spark-guide","text":"1. PMem Shuffle introduction 2. Recommended HW environment 3. Install and configure PMem 4. Configure and Validate RDMA 5. Install dependencies for PMem Shuffle 6. Install PMem Shuffle for Spark 7. PMem Shuffle for Spark Testing Reference PMem Shuffle for Spark (previously Spark-PMoF) depends on multiple native libraries like libfabrics, libcuckoo, PMDK. This enabling guide covers the installing process for the time being, but it might change as the install commands and related dependency packages for the 3rd party libraries might vary depending on the OS version and distribution you are using. Yarn, HDFS, Spark installation and configuration is out of the scope of this document.","title":"PMem Shuffle for Apache Spark Guide"},{"location":"PMemShuffle/#1-pmem-shuffle-introduction","text":"Intel Optane DC persistent memory is the next-generation storage at memory speed. It closes the performance gap between DRAM memory technology and traditional NAND SSDs. Remote Persistent Memory extends PMem usage to new scenario, lots of new usage cases & value proposition can be developed. Spark shuffle is a high cost operation as it issues a great number of small random disk IO, serialization, network data transmission, and thus contributes a lot to job latency and could be the bottleneck for workloads performance. PMem Shuffle for spark (previously Spark PMoF) https://github.com/Intel-bigdata/Spark-PMoF ) is a Persistent Memory over Fabrics (PMoF) plugin for Spark shuffle, which leverages the RDMA network and remote persistent memory (for read) to provide extremely high performance and low latency shuffle solutions for Spark to address performance issues for shuffle intensive workloads. PMem Shuffle brings follow benefits: Leverage high performance persistent memory as shuffle media as well as spill media, increased shuffle performance and reduced memory footprint Using PMDK libs to avoid inefficient context switches and memory copies with zero-copy remote access to persistent memory. Leveraging RDMA for network offloading The Figure 1 shows the high level architecture of PMem Shuffle, it shows how data flows between Spark and shuffle devices in PMem Shuffle for spark shuffle and Vanilla Spark. In this guide, we will introduce how to deploy and use PMem Shuffle for Spark. Figure 1: PMem Shuffle for Spark","title":"1. PMem Shuffle introduction"},{"location":"PMemShuffle/#2-recommended-hw-environment","text":"","title":"2. Recommended HW environment"},{"location":"PMemShuffle/#21-system-configuration","text":"","title":"2.1. System Configuration"},{"location":"PMemShuffle/#211-hw-and-sw-configuration","text":"A 4x or 3x Node cluster is recommended for a proof of concept tests, depending your system configurations, if using 3 nodes cluster, the Name node and Spark Master node can be co-located with one of the Hadoop data nodes. Hardware: - Intel\u00ae Xeon\u2122 processor Gold 6240 CPU @ 2.60GHz, 384GB Memory (12x 32GB 2666 MT/s) or 192GB Memory (12x 16GB 2666MT/s) - An RDMA capable NIC, 40Gb+ is preferred. e.g., 1x Intel X722 NIC or Mellanox ConnectX-4 40Gb NIC - RDMA cables: - Mellanox MCP1600-C003 100GbE 3m 28AWG - Shuffle Devices\uff1a - 1x 1TB HDD for shuffle (baseline) - 4x 128GB Persistent Memory for shuffle - 4x 1T NVMe for HDFS Switch : - Arista 7060 CX2 (7060CX2-32S-F) 100Gb switches was used Please refer to section 4.2 for configurations Software: Hadoop 2.7 Spark 3.0.0 Fedora 29 with ww08.2019 BKC","title":"2.1.1 HW and SW Configuration"},{"location":"PMemShuffle/#22-recommended-rdma-nic","text":"PMem Shuffle is using HPNL ( https://cloud.google.com/solutions/big-data/ ) for network communication, which leverages libfabric for efficient network communication, so a RDMA capable NIC is recommended. Libfabric supports RoCE, iWrap, IB protocol, so various RNICs with different protocol can be used.","title":"2.2. Recommended RDMA NIC"},{"location":"PMemShuffle/#23-recommended-pmem-configuration","text":"It is recommended to install 4+ PMem DIMMs on the SUT, but you can adjust the numbers accordingly. In this enabling guide, 4x 128GB PMEMM was installed on the SUT as an exmaple.","title":"2.3 Recommended PMEM configuration"},{"location":"PMemShuffle/#24-recommended-pmem-bkc-optional","text":"This development guide was based on ww08.2019 BKC (best known configuration). Please contact your HW vendor for latest BKC. Please refer to backup if you do not have BKC access. BKC installation/enabling or FW installation is out of the scope of this guide.","title":"2.4 Recommended PMEM BKC (optional)"},{"location":"PMemShuffle/#3-install-and-configure-pmem-example","text":"1) Please install ipmctl and ndctl according to your OS version 2) Run ipmctl show -dimm to check whether dimms can be recognized 3) Run ipmctl create -goal PersistentMemoryType=AppDirect to create AD mode 4) Run ndctl list -R , you will see region0 and region1 . 5) Assume you have 4x PMEM installed on 1 node. a. Run ndctl create-namespace -m devdax -r region0 -s 120g b. Run ndctl create-namespace -m devdax -r region0 -s 120g c. Run ndctl create-namespace -m devdax -r region1 -s 120g d. Run ndctl create-namespace -m devdax -r region1 -s 120g This will create four namespaces, namely /dev/dax0.0, /dev/dax0.1, /dev/dax1.0, /dev/dax1.1 in that node, and it will be used as PMem Shuffle media. You can change your configuration (namespaces numbers, size) accordingly.","title":"3. Install and configure PMEM (example)"},{"location":"PMemShuffle/#4-configure-and-validate-rdma","text":"Notes This part is vendor specific, it might NOT apply to your environment, please check your switch, NIC manuals accordingly.","title":"4. Configure and Validate RDMA"},{"location":"PMemShuffle/#41-configure-and-test-iwarp-rdma","text":"","title":"4.1 Configure and test iWARP RDMA"},{"location":"PMemShuffle/#411-download-rdma-core-and-install-dependencies","text":"The rdma-core provides the necessary userspace libraries to test rdma connectivity with tests such as rping. Refer to latest rdma-core documentation for updated installation guidelines (https://github.com/linux-rdma/rdma-core.git). You might refer to HW specific instructions or guide to enable your RDMA NICs. Take Mellanox as an example, perform below steps to enable it: git clone <https://github.com/linux-rdma/rdma-core.git> dnf install cmake gcc libnl3-devel libudev-devel pkgconfig valgrind-devel ninja-build python3-devel python3-Cython python3-docutils pandoc //change to yum on centos bash build.sh #on centos 7 yum install cmake gcc libnl3-devel libudev-devel make pkgconfig valgrind-devel yum install epel-release yum install cmake3 ninja-build pandoc","title":"4.1.1 Download rdma-core and install dependencies"},{"location":"PMemShuffle/#412-switch-configuration-optional","text":"This part is HW specific, please check your switch manual accordingly. Connect the console port to PC. Username is admin. No password. Enter global configuration mode. Below example is based on Arista 7060 CX2 100Gb Switch, it is to configure the 100Gb port to work at 40Gb to match the NIC speed. It is NOT required if your NIC and calbes are match. Config Switch Speed to 40Gb/s switch# enable switch# config switch(config)# show interface status Configure corresponding port to 40 Gb/s to match the NIC speed switch(config)# interface Et(num_of_port)/1 switch(config)# speed forced 40gfull RoCE might have performance issues, so PFC configuration is strongly suggested. You will need to check the RDMA NIC driver manual and switch manual to configure PFC. Below is the example for ConnectX-4 and Arista 7060-CX2 switches. Below is to set the two connection ports in the same vlan and configure it in trunk mode. Configure interface as trunk mode and add to vlan switch(config)# vlan 1 switch(config-vlan-1)# switch(config)# interface ethernet 12-16 switch(config-if-Et12-16)# switchport trunk allowed vlan 1 switch (config-if-et1) # **priority-flow-control on** switch (config-if-et1) # **priority-flow-control priority 3 no-drop**","title":"4.1.2 Switch Configuration (optional)"},{"location":"PMemShuffle/#413-download-and-install-drivers","text":"","title":"4.1.3 Download and install drivers"},{"location":"PMemShuffle/#a-example-mellanox-enabling-roce-v2-rdma-optional","text":"There are lots of packages need to be installed for dependency, please refer to your RDMA NIC's manualls to install it correctly. yum install atk gcc-gfortran tcsh gtk2 tcl tk please install NIC drivers accordingly. # Download MLNX_OFED_LINUX-4.7-3.2.9.0-* from https://community.mellanox.com/s/article/howto-install-mlnx-ofed-driver # e.g., wget http://www.mellanox.com/downloads/ofed/MLNX_OFED-<version>/MLNX_OFED_LINUX-<version>-<distribution>-<arch>.tgz . tar zxf MLNX_OFED_LINUX-4.7-3.2.9.0-* cd MLNX_OFED_LINUX-4.7-3.2.9.0- ./mlnxofedinstall --add-kernel-support. # The process might interpret and promote you to install dependencies. Install dependencies and try again # This process will take some time. tar zxf MLNX_OFED_LINUX-4.7-3.2.9.0-* cd MLNX_OFED_LINUX-4.7-3.2.9.0- ./mlnxofedinstall --add-kernel-support. # The process might interpret and promote you to install dependencies. Install dependencies and try again # This process will take some time. ** Restart the driver: /etc/init.d/openibd restart Might need to unload the modules if it is in use. Make sure the that the field link_layer is \u201cEthernet\u201d. Then you can use following command to get the device name.","title":"A. Example: Mellanox Enabling RoCE V2 RDMA (Optional)"},{"location":"PMemShuffle/#b-enable-pfc-priority-flow-control-to-guarantee-stable-performance-optional","text":"Then you can use following command to get the device name If you\u2019re using Mellanox NIC, PFC is a must to guarantee stable performance. Fetch RDMA info with rdma command: rdma link 0/1: i40iw0/1: state DOWN physical_state NOP 1/1: i40iw1/1: state ACTIVE physical_state NOP 2/1: mlx5_0/1: state DOWN physical_state DISABLED netdev ens803f0 3/1: mlx5_1/1: state ACTIVE physical_state LINK_UP netdev ens803f1 lspci | grep Mellanox 86:00.0 Ethernet controller: Mellanox Technologies MT27700 Family [ConnectX-4] 86:00.1 Ethernet controller: Mellanox Technologies MT27700 Family [ConnectX-4] Set PFC: /etc/init.d/openibd restart mlnx_qos -i ens803f1 --pfc 0,0,0,1,0,0,0,0 modprobe 8021q vconfig add ens803f1 100 ifconfig ens803f1.100 $ip1/$mask up //change to your own IP ifconfig ens803f1 $ip2/$mask up //Change to your own IP for i in {0..7}; do vconfig set_egress_map ens803f1.100 $i 3 ; done tc_wrap.py -i ens803f1 -u 3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3 Modify the IP address part based on your environment and execute the script.","title":"B. Enable PFC (Priority Flow Control) to guarantee stable performance (optional)"},{"location":"PMemShuffle/#414-check-rdma-module","text":"Make sure the following modules are loaded: modprobe ib_core i40iw iw_cm rdma_cm rdma_ucm ib_cm ib_uverbs ``` #### 4.1.5 Validate RDMA functionalities Check that you see your RDMA interfaces listed on each server when you run the following command: **ibv_devices** Check with rping for RDMA connectivity between target interface and client interface. 1) Assign IPs to the RDMA interfaces on Target and Client. 2) On Target run:rping -sdVa &lt;Target IP&gt; 3) On Client run: rping -cdVa &lt;Target IP&gt; Example: On the server side: ```bash rping -sda $ip1 created cm_id 0x17766d0 rdma_bind_addr successful rdma_listen accepting client connection request cq_thread started. recv completion Received rkey 97a4f addr 17ce190 len 64 from peer cma_event type RDMA_CM_EVENT_ESTABLISHED cma_id 0x7fe9ec000c90 (child) ESTABLISHED Received rkey 96b40 addr 17ce1e0 len 64 from peer server received sink adv rdma write from lkey 143c0 laddr 1771190 len 64 rdma write completion rping -sda $ip2 created cm_id 0x17766d0 rdma_bind_addr successful rdma_listen \u2026 accepting client connection request cq_thread started. recv completion Received rkey 97a4f addr 17ce190 len 64 from peer cma_event type RDMA_CM_EVENT_ESTABLISHED cma_id 0x7fe9ec000c90 (child) ESTABLISHED \u2026 Received rkey 96b40 addr 17ce1e0 len 64 from peer server received sink adv rdma write from lkey 143c0 laddr 1771190 len 64 rdma write completion \u2026 ``` On Client run: rping -cdVa &lt;Target IP&gt; ```bash # Client side use .100 ip 172.168.0.209 for an example rping -c -a 172.168.0.209 -v -C 4 ping data: rdma-ping-0: ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqr ping data: rdma-ping-1: BCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrs ping data: rdma-ping-2: CDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrst ping data: rdma-ping-3: DEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstu Please refer to your NIC manuual for detail instructions on how to validate RDMA works.","title":"4.1.4 Check RDMA module"},{"location":"PMemShuffle/#5-install-dependencies-for-pmem-shuffle","text":"We have provided a Conda package which will automatically install dependencies needed for PMem Shuffle, refer to OAP-Installation-Guide for more information. If you have finished OAP-Installation-Guide , you can find compiled OAP jars in $HOME/miniconda2/envs/oapenv/oap_jars/ , and skip this session and jump to 6.Install PMem Shuffle for Spark","title":"5. Install dependencies for PMem Shuffle"},{"location":"PMemShuffle/#51-install-hpnl-httpsgithubcomintel-bigdatahpnl","text":"HPNL is a fast, CPU-Efficient network library designed for modern network technology. HPNL depends on Libfabric, which is protocol independent, it supports TCP/IP, RoCE, IB, iWRAP etc. Please make sure the Libfabric is installed in your setup. Based on this issue , please make sure NOT to install Libfabric 1.9.0. You might need to install automake/libtool first to resolve dependency issues. git clone https://github.com/ofiwg/libfabric.git cd libfabric git checkout v1.6.0 ./autogen.sh ./configure --disable-sockets --enable-verbs --disable-mlx make -j && sudo make install","title":"5.1 Install HPNL (https://github.com/Intel-bigdata/HPNL)"},{"location":"PMemShuffle/#511-build-and-install-hpnl","text":"Assume Project_root_path is HPNL folder\u2019s path, HPNL here. sudo apt-get install cmake libboost-dev libboost-system-dev #Fedora dnf install cmake boost-devel boost-system git clone https://github.com/Intel-bigdata/HPNL.git cd HPNL git checkout origin/spark-pmof-test --track git submodule update --init --recursive mkdir build; cd build cmake -DWITH_VERBS=ON .. make -j && make install cd ${project_root_path}/java/hpnl mvn install","title":"5.1.1 Build and install HPNL"},{"location":"PMemShuffle/#52-install-basic-c-library-dependencies","text":"yum install -y autoconf asciidoctor kmod-devel.x86\\_64 libudev-devel libuuid-devel json-c-devel jemalloc-devel yum groupinstall -y \"Development Tools\"","title":"5.2 install basic C library dependencies"},{"location":"PMemShuffle/#53-install-ndctl","text":"This can be installed with your package managmenet tool as well. git clone https://github.com/pmem/ndctl.git cd ndctl git checkout v63 ./autogen.sh ./configure CFLAGS='-g -O2' --prefix=/usr --sysconfdir=/etc --libdir=/usr/lib64 make -j make check make install","title":"5.3 install ndctl"},{"location":"PMemShuffle/#54-install-pmdk","text":"yum install -y pandoc git clone https://github.com/pmem/pmdk.git cd pmdk git checkout tags/1.8 make -j && make install export PKG_CONFIG_PATH=/usr/local/lib64/pkgconfig/:$PKG_CONFIG_PATH echo \u201cexport PKG_CONFIG_PATH=/usr/local/lib64/pkgconfig/:$PKG_CONFIG_PATH\u201d > /etc/profile.d/pmdk.sh","title":"5.4 install PMDK"},{"location":"PMemShuffle/#55-install-rpmem-extension","text":"git clone https://github.com/efficient/libcuckoo cd libcuckoo mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr/local -DBUILD_EXAMPLES=1 -DBUILD_TESTS=1 .. make all && make install git clone -b <tag-version> https://github.com/intel-bigdata/OAP.git cd OAP/oap-shuffle/RPMem-shuffle mvn install -DskipTests","title":"5.5 Install RPMem extension"},{"location":"PMemShuffle/#6-install-pmem-shuffle-for-spark","text":"","title":"6. Install PMem Shuffle for Spark"},{"location":"PMemShuffle/#61-configure-rpmem-extension-for-spark-shuffle-in-spark","text":"PMem Shuffle for spark shuffle is designed as a plugin to Spark. Currently the plugin supports Spark 3.0.0 and works well on various Network fabrics, including Socket, RDMA and Omni-Path. There are several configurations files needs to be modified in order to run PMem Shuffle.","title":"6.1 Configure RPMem extension for spark shuffle in Spark"},{"location":"PMemShuffle/#prerequisite","text":"Use below command to remove original initialization of one PMem, this is a MUST step, or RPMemShuffle won\u2019t be able to open PMem devices. pmempool rm ${device_name} #example: pmempool rm /dev/dax0.0 If you install OAP Conda package , you can use below command to remove original initialization of one PMem. ```shell script export LD_LIBRARY_PATH=$HOME/miniconda2/envs/oapenv/lib/:$LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/bin/pmempool rm ${device_name} **Refer to the Reference section for detail descrption of each parameter.** #### Enable RPMemShuffle ```bash spark.shuffle.manager org.apache.spark.shuffle.pmof.PmofShuffleManager spark.driver.extraClassPath /$path/oap-shuffle/RPMem-shuffle/core/target/oap-rpmem-shuffle-java-<version>-jar-with-dependencies.jar spark.executor.extraClassPath /$path/oap-shuffle/RPMem-shuffle/core/target/oap-rpmem-shuffle-java-<version>-jar-with-dependencies.jar","title":"Prerequisite"},{"location":"PMemShuffle/#switch-onoff-pmem-and-rdma","text":"spark.shuffle.pmof.enable_rdma true spark.shuffle.pmof.enable_pmem true","title":"Switch On/Off PMem and RDMA"},{"location":"PMemShuffle/#add-pmem-information-to-spark-config","text":"Explanation: spark.shuffle.pmof.pmem_capacity: the capacity of one PMem device, this value will be used when register PMem device to RDMA. spark.shuffle.pmof.pmem_list: a list of all local PMem device, make sure your per physical node executor number won\u2019t exceed PMem device number, or one PMem device maybe opened by two spark executor processes and this will leads to a PMem open failure. spark.shuffle.pmof.dev_core_set: a mapping of which core range will be task set to which PMem device, this is a performance optimal configuration for better PMem numa accessing. spark.io.compression.codec: use \u201csnappy\u201d to do shuffle data and spilling data compression, this is a MUST when enabled PMem due to a default LZ4 ByteBuffer incompatible issue. spark.shuffle.pmof.pmem_capacity ${total_size_of_one_device} spark.shuffle.pmof.pmem_list ${device_name},${device_name},\u2026 spark.shuffle.pmof.dev_core_set ${device_name}:${core_range};\u2026 #example: /dev/dax0.0:0-17,36-53;/dev/dax0.2:0-17,36-53 spark.io.compression.codec snappy","title":"Add PMem information to spark config"},{"location":"PMemShuffle/#memory-configuration-suggestion","text":"Suitable for any release before OAP 0.8. In OAP 0.8 and later release, the memory footprint of each core is reduced dramatically and the formula below is not applicable any more. Spark.executor.memory must be greater than shuffle_block_size * numPartitions * numCores * 2 (for both shuffle and external sort), for example, default HiBench Terasort numPartition is 200, and we configured 10 cores each executor, then this executor must has memory capacity greater than 2MB(spark.shuffle.pmof.shuffle_block_size) * 200 * 10 * 2 = 8G. Recommendation configuration as below, but it needs to be adjusted accordingly based on your system configurations. Yarn.executor.num 4 // same as PMem namespaces number Yarn.executor.cores 18 // total core number divide executor number spark.executor.memory 15g // 2MB * numPartition(200) * 18 * 2 spark.yarn.executor.memoryOverhead 5g // 30% of spark.executor.memory spark.shuffle.pmof.shuffle_block_size 2096128 // 2MB \u2013 1024 Bytes spark.shuffle.pmof.spill_throttle 2096128 // 2MB \u2013 1024 Bytes, spill_throttle is used to // set throttle by when spill buffer data to // Persistent Memory, must set spill_throttle // equal to shuffle_block_size spark.driver.memory 10g spark.yarn.driver.memoryOverhead 5g Configuration of RDMA enabled case spark.shuffle.pmof.node : spark nodes and RDMA ip mapping list spark.driver.rhost / spark.driver.rport : Specify spark driver RDMA IP and port spark.shuffle.pmof.server_buffer_nums 64 spark.shuffle.pmof.client_buffer_nums 64 spark.shuffle.pmof.map_serializer_buffer_size 262144 spark.shuffle.pmof.reduce_serializer_buffer_size 262144 spark.shuffle.pmof.chunk_size 262144 spark.shuffle.pmof.server_pool_size 3 spark.shuffle.pmof.client_pool_size 3 spark.shuffle.pmof.node $HOST1-$IP1,$HOST2-$IP2//Host-IP pairs, $hostname-$ip spark.driver.rhost $IP //change to your host IP spark.driver.rport 61000","title":"Memory configuration suggestion"},{"location":"PMemShuffle/#7-pmem-shuffle-for-spark-testing","text":"Pmem shuffle extension have been tested and validated with Terasort and Decision support workloads.","title":"7. PMem Shuffle for Spark Testing"},{"location":"PMemShuffle/#71-decision-support-workloads","text":"The Decision support workloads is a decision support benchmark that models several general applicable aspects of a decision support system, including queries and data maintenance.","title":"7.1 Decision support workloads"},{"location":"PMemShuffle/#711-download-spark-sql-perf","text":"The link is https://github.com/databricks/spark-sql-perf and follow README to use sbt build the artifact.","title":"7.1.1 Download spark-sql-perf"},{"location":"PMemShuffle/#712-download-the-kit","text":"As per instruction from spark-sql-perf README, tpcds-kit is required and please download it from https://github.com/databricks/tpcds-kit , follow README to setup the benchmark.","title":"7.1.2 Download the kit"},{"location":"PMemShuffle/#713-prepare-data","text":"As an example, generate parquet format data to HDFS with 1TB data scale. The data stored path, data format and data scale are configurable. Please check script below as a sample. import com.databricks.spark.sql.perf.tpcds.TPCDSTables import org.apache.spark.sql._ // Set: val rootDir: String = \"hdfs://${ip}:9000/tpcds_1T\" // root directory of location to create data in. val databaseName: String = \"tpcds_1T\" // name of database to create. val scaleFactor: String = \"1024\" // scaleFactor defines the size of the dataset to generate (in GB). val format: String = \"parquet\" // valid spark format like parquet \"parquet\". val sqlContext = new SQLContext(sc) // Run: val tables = new TPCDSTables(sqlContext, dsdgenDir = \"/mnt/spark-pmof/tool/tpcds-kit/tools\", // location of dsdgen scaleFactor = scaleFactor, useDoubleForDecimal = false, // true to replace DecimalType with DoubleType useStringForDate = false) // true to replace DateType with StringType tables.genData( location = rootDir, format = format, overwrite = true, // overwrite the data that is already there partitionTables = true, // create the partitioned fact tables clusterByPartitionColumns = true, // shuffle to get partitions coalesced into single files. filterOutNullPartitionValues = false, // true to filter out the partition with NULL key value tableFilter = \"\", // \"\" means generate all tables numPartitions = 400) // how many dsdgen partitions to run - number of input tasks. // Create the specified database sql(s\"create database $databaseName\") // Create metastore tables in a specified database for your data. // Once tables are created, the current database will be switched to the specified database. tables.createExternalTables(rootDir, \"parquet\", databaseName, overwrite = true, discoverPartitions = true)","title":"7.1.3 Prepare data"},{"location":"PMemShuffle/#714-run-the-benchmark","text":"Launch DECISION SUPPORT WORKLOADS queries on generated data, check benchmark.scala below as a sample, it runs query64. import com.databricks.spark.sql.perf.tpcds.TPCDS import org.apache.spark.sql._ val sqlContext = new SQLContext(sc) val tpcds = new TPCDS (sqlContext = sqlContext) // Set: val databaseName = \"tpcds_1T\" // name of database with TPCDS data. val resultLocation = \"tpcds_1T_result\" // place to write results val iterations = 1 // how many iterations of queries to run. val query_filter = Seq(\"q64-v2.4\") val randomizeQueries = false def queries = { val filtered_queries = query_filter match { case Seq() => tpcds.tpcds2_4Queries case _=> tpcds.tpcds2_4Queries.filter(q => query_filter.contains(q.name)) } filtered_queries } val timeout = 24*60*60 // timeout, in seconds. // Run: sql(s\"use $databaseName\") val experiment = tpcds.runExperiment( queries, iterations = iterations, resultLocation = resultLocation, forkThread = true) experiment.waitForFinish(timeout)","title":"7.1.4 Run the benchmark"},{"location":"PMemShuffle/#715-check-the-result","text":"Check the result under tpcds_1T_result folder. It can be an option to check the result at spark history server. (Need to start history server by \\$SPARK_HOME/sbin/start-history-server.sh )","title":"7.1.5 Check the result"},{"location":"PMemShuffle/#72-terasort","text":"TeraSort is a benchmark that measures the amount of time to sort one terabyte of randomly distributed data on a given computer system.","title":"7.2 TeraSort"},{"location":"PMemShuffle/#721-download-hibench","text":"This guide uses HiBench for Terasort tests, https://github.com/Intel-bigdata/HiBench . HiBench is a big data benchmark suite and contains a set of Hadoop, Spark and streaming workloads including TeraSort.","title":"7.2.1 Download HiBench"},{"location":"PMemShuffle/#722-build-hibench-as-per-instructions-from-build-bench","text":"","title":"7.2.2 Build HiBench as per instructions from build-bench."},{"location":"PMemShuffle/#723-configuration","text":"Modify \\$HiBench-HOME/conf/spark.conf to specify the spark home and other spark configurations. It will overwrite the configuration of \\$SPARK-HOME/conf/spark-defaults.conf at run time.","title":"7.2.3 Configuration"},{"location":"PMemShuffle/#724-launch-the-benchmark","text":"Need to prepare the data with \\$HiBench-HOME/bin/workloads/micro/terasort/prepare/prepare.sh Kick off the evaluation by \\$HiBench-HOME/bin/workloads/micro/terasort/spark/run.sh Change directory to \\$HiBench-HOME/bin/workloads/micro/terasort/spark and launch the run.sh . You can add some PMEM cleaning work to make sure it starts from empty shuffle device every test iteration. Take run.sh below as a sample. # ***Change below command accordingly *** ssh ${node} pmempool rm /dev/dax0.0 current_dir=`dirname \"$0\"` current_dir=`cd \"$current_dir\"; pwd` root_dir=${current_dir}/../../../../.. workload_config=${root_dir}/conf/workloads/micro/terasort.conf . \"${root_dir}/bin/functions/load_bench_config.sh\" enter_bench ScalaSparkTerasort ${workload_config} ${current_dir} show_bannar start rmr_hdfs $OUTPUT_HDFS || true SIZE=`dir_size $INPUT_HDFS` START_TIME=`timestamp` run_spark_job com.intel.hibench.sparkbench.micro.ScalaTeraSort $INPUT_HDFS $OUTPUT_HDFS END_TIME=`timestamp` gen_report ${START_TIME} ${END_TIME} ${SIZE} show_bannar finish leave_bench","title":"7.2.4 Launch the benchmark"},{"location":"PMemShuffle/#725-check-the-result","text":"Check the result at spark history server to see the execution time and other spark metrics like spark shuffle spill status. (Need to start history server by \\$SPARK_HOME/sbin/start-history-server.sh )","title":"7.2.5 Check the result"},{"location":"PMemShuffle/#reference","text":"","title":"Reference"},{"location":"PMemShuffle/#rpmemshuffle-spark-configuration","text":"Before running Spark workload, add following contents in spark-defaults.conf . spark.executor.instances 4 // same as total PMem namespace numbers of your cluster spark.executor.cores 18 // total core number divide executor number spark.executor.memory 70g // 4~5G * spark.executor.cores spark.executor.memoryOverhead 15g // 30% of spark.executor.memory spark.shuffle.pmof.shuffle_block_size 2096128 // 2MB \u2013 1024 Bytes spark.shuffle.pmof.spill_throttle 2096128 // 2MB \u2013 1024 Bytes spark.driver.memory 10g spark.yarn.driver.memoryOverhead 5g spark.shuffle.compress true spark.io.compression.codec snappy spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-rpmem-shuffle-java-<version>-jar-with-dependencies.jar spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-rpmem-shuffle-java-<version>-jar-with-dependencies.jar spark.shuffle.manager org.apache.spark.shuffle.pmof.PmofShuffleManager spark.shuffle.pmof.enable_rdma true spark.shuffle.pmof.enable_pmem true spark.shuffle.pmof.pmem_capacity 126833655808 // size should be same as pmem size spark.shuffle.pmof.pmem_list /dev/dax0.0,/dev/dax0.1,/dev/dax1.0,/dev/dax1.1 spark.shuffle.pmof.dev_core_set dax0.0:0-71,dax0.1:0-71,dax1.0:0-71,dax1.1:0-71 spark.shuffle.pmof.server_buffer_nums 64 spark.shuffle.pmof.client_buffer_nums 64 spark.shuffle.pmof.map_serializer_buffer_size 262144 spark.shuffle.pmof.reduce_serializer_buffer_size 262144 spark.shuffle.pmof.chunk_size 262144 spark.shuffle.pmof.server_pool_size 3 spark.shuffle.pmof.client_pool_size 3 spark.shuffle.pmof.node $host1-$IP1,$host2-$IP2//HOST-IP Pair, seperate with \",\" spark.driver.rhost $IP //change to your host spark.driver.rport 61000","title":"RPMemShuffle Spark configuration"},{"location":"PMemShuffle/#reference-guides-without-bkc-access","text":"If you do not have BKC access, please following below official guide: (1): General PMEMM support: PMEMM support https://www.intel.com/content/www/us/en/support/products/190349/memory-and-storage/data-center-persistent-memory/intel-optane-dc-persistent-memory.html (2) PMEMM population rule: Module DIMM Population for Intel\u00ae Optane\u2122 DC Persistent Memory https://www.intel.com/content/www/us/en/support/articles/000032932/memory-and-storage/data-center-persistent-memory.html?productId=190349&localeCode=us_en (3) OS support requirement: Operating System OS for Intel\u00ae Optane\u2122 DC Persistent Memory https://www.intel.com/content/www/us/en/support/articles/000032860/memory-and-storage/data-center-persistent-memory.html?productId=190349&localeCode=us_en (4): Quick Start Guide: Provision Intel\u00ae Optane\u2122 DC Persistent Memory https://software.intel.com/en-us/articles/quick-start-guide-configure-intel-optane-dc-persistent-memory-on-linux","title":"Reference guides (without BKC access)"},{"location":"PMemShuffle/OAP-Developer-Guide/","text":"OAP Developer Guide This document contains the instructions & scripts on installing necessary dependencies and building OAP. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Arrow Data Source Native SQL Engine Building OAP Prerequisites for Building OAP is built with Apache Maven and Oracle Java 8, and mainly required tools to install on your cluster are listed below. Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. We provide scripts below to help automatically install dependencies above except RDMA , need change to root account, run: # git clone -b <tag-version> # cd oap-tools # sh dev/install-compile-time-dependencies.sh Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven Building To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building Specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache","title":"OAP Developer Guide"},{"location":"PMemShuffle/OAP-Developer-Guide/#oap-developer-guide","text":"This document contains the instructions & scripts on installing necessary dependencies and building OAP. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Arrow Data Source Native SQL Engine","title":"OAP Developer Guide"},{"location":"PMemShuffle/OAP-Developer-Guide/#building-oap","text":"","title":"Building OAP"},{"location":"PMemShuffle/OAP-Developer-Guide/#prerequisites-for-building","text":"OAP is built with Apache Maven and Oracle Java 8, and mainly required tools to install on your cluster are listed below. Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. We provide scripts below to help automatically install dependencies above except RDMA , need change to root account, run: # git clone -b <tag-version> # cd oap-tools # sh dev/install-compile-time-dependencies.sh Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven","title":"Prerequisites for Building"},{"location":"PMemShuffle/OAP-Developer-Guide/#building","text":"To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building Specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache","title":"Building"},{"location":"PMemShuffle/OAP-Installation-Guide/","text":"OAP Installation Guide This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine. Contents Prerequisites Installing OAP Configuration Prerequisites OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly. Installing OAP Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Create a conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.0.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Extra Steps for Shuffle Remote PMem Extension If you use one of OAP features -- PMmem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details. Configuration Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar And then you can follow the corresponding feature documents for more details to use them.","title":"OAP Installation Guide"},{"location":"PMemShuffle/OAP-Installation-Guide/#oap-installation-guide","text":"This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine.","title":"OAP Installation Guide"},{"location":"PMemShuffle/OAP-Installation-Guide/#contents","text":"Prerequisites Installing OAP Configuration","title":"Contents"},{"location":"PMemShuffle/OAP-Installation-Guide/#prerequisites","text":"OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly.","title":"Prerequisites"},{"location":"PMemShuffle/OAP-Installation-Guide/#installing-oap","text":"Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Create a conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.0.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars","title":"Installing OAP"},{"location":"PMemShuffle/OAP-Installation-Guide/#extra-steps-for-shuffle-remote-pmem-extension","text":"If you use one of OAP features -- PMmem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details.","title":"Extra Steps for Shuffle Remote PMem Extension"},{"location":"PMemShuffle/OAP-Installation-Guide/#configuration","text":"Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar And then you can follow the corresponding feature documents for more details to use them.","title":"Configuration"},{"location":"PMemShuffle/User-Guide/","text":"PMem Shuffle for Apache Spark Guide 1. PMem Shuffle introduction 2. Recommended HW environment 3. Install and configure PMem 4. Configure and Validate RDMA 5. Install dependencies for PMem Shuffle 6. Install PMem Shuffle for Spark 7. PMem Shuffle for Spark Testing Reference PMem Shuffle for Spark (previously Spark-PMoF) depends on multiple native libraries like libfabrics, libcuckoo, PMDK. This enabling guide covers the installing process for the time being, but it might change as the install commands and related dependency packages for the 3rd party libraries might vary depending on the OS version and distribution you are using. Yarn, HDFS, Spark installation and configuration is out of the scope of this document. 1. PMem Shuffle introduction Intel Optane DC persistent memory is the next-generation storage at memory speed. It closes the performance gap between DRAM memory technology and traditional NAND SSDs. Remote Persistent Memory extends PMem usage to new scenario, lots of new usage cases & value proposition can be developed. Spark shuffle is a high cost operation as it issues a great number of small random disk IO, serialization, network data transmission, and thus contributes a lot to job latency and could be the bottleneck for workloads performance. PMem Shuffle for spark (previously Spark PMoF) https://github.com/Intel-bigdata/Spark-PMoF ) is a Persistent Memory over Fabrics (PMoF) plugin for Spark shuffle, which leverages the RDMA network and remote persistent memory (for read) to provide extremely high performance and low latency shuffle solutions for Spark to address performance issues for shuffle intensive workloads. PMem Shuffle brings follow benefits: Leverage high performance persistent memory as shuffle media as well as spill media, increased shuffle performance and reduced memory footprint Using PMDK libs to avoid inefficient context switches and memory copies with zero-copy remote access to persistent memory. Leveraging RDMA for network offloading The Figure 1 shows the high level architecture of PMem Shuffle, it shows how data flows between Spark and shuffle devices in PMem Shuffle for spark shuffle and Vanilla Spark. In this guide, we will introduce how to deploy and use PMem Shuffle for Spark. Figure 1: PMem Shuffle for Spark 2. Recommended HW environment 2.1. System Configuration 2.1.1 HW and SW Configuration A 4x or 3x Node cluster is recommended for a proof of concept tests, depending your system configurations, if using 3 nodes cluster, the Name node and Spark Master node can be co-located with one of the Hadoop data nodes. Hardware: - Intel\u00ae Xeon\u2122 processor Gold 6240 CPU @ 2.60GHz, 384GB Memory (12x 32GB 2666 MT/s) or 192GB Memory (12x 16GB 2666MT/s) - An RDMA capable NIC, 40Gb+ is preferred. e.g., 1x Intel X722 NIC or Mellanox ConnectX-4 40Gb NIC - RDMA cables: - Mellanox MCP1600-C003 100GbE 3m 28AWG - Shuffle Devices\uff1a - 1x 1TB HDD for shuffle (baseline) - 4x 128GB Persistent Memory for shuffle - 4x 1T NVMe for HDFS Switch : - Arista 7060 CX2 (7060CX2-32S-F) 100Gb switches was used Please refer to section 4.2 for configurations Software: Hadoop 2.7 Spark 3.0.0 Fedora 29 with ww08.2019 BKC 2.2. Recommended RDMA NIC PMem Shuffle is using HPNL ( https://cloud.google.com/solutions/big-data/ ) for network communication, which leverages libfabric for efficient network communication, so a RDMA capable NIC is recommended. Libfabric supports RoCE, iWrap, IB protocol, so various RNICs with different protocol can be used. 2.3 Recommended PMEM configuration It is recommended to install 4+ PMem DIMMs on the SUT, but you can adjust the numbers accordingly. In this enabling guide, 4x 128GB PMEMM was installed on the SUT as an exmaple. 2.4 Recommended PMEM BKC (optional) This development guide was based on ww08.2019 BKC (best known configuration). Please contact your HW vendor for latest BKC. Please refer to backup if you do not have BKC access. BKC installation/enabling or FW installation is out of the scope of this guide. 3. Install and configure PMEM (example) 1) Please install ipmctl and ndctl according to your OS version 2) Run ipmctl show -dimm to check whether dimms can be recognized 3) Run ipmctl create -goal PersistentMemoryType=AppDirect to create AD mode 4) Run ndctl list -R , you will see region0 and region1 . 5) Assume you have 4x PMEM installed on 1 node. a. Run ndctl create-namespace -m devdax -r region0 -s 120g b. Run ndctl create-namespace -m devdax -r region0 -s 120g c. Run ndctl create-namespace -m devdax -r region1 -s 120g d. Run ndctl create-namespace -m devdax -r region1 -s 120g This will create four namespaces, namely /dev/dax0.0, /dev/dax0.1, /dev/dax1.0, /dev/dax1.1 in that node, and it will be used as PMem Shuffle media. You can change your configuration (namespaces numbers, size) accordingly. 4. Configure and Validate RDMA Notes This part is vendor specific, it might NOT apply to your environment, please check your switch, NIC manuals accordingly. 4.1 Configure and test iWARP RDMA 4.1.1 Download rdma-core and install dependencies The rdma-core provides the necessary userspace libraries to test rdma connectivity with tests such as rping. Refer to latest rdma-core documentation for updated installation guidelines (https://github.com/linux-rdma/rdma-core.git). You might refer to HW specific instructions or guide to enable your RDMA NICs. Take Mellanox as an example, perform below steps to enable it: git clone <https://github.com/linux-rdma/rdma-core.git> dnf install cmake gcc libnl3-devel libudev-devel pkgconfig valgrind-devel ninja-build python3-devel python3-Cython python3-docutils pandoc //change to yum on centos bash build.sh #on centos 7 yum install cmake gcc libnl3-devel libudev-devel make pkgconfig valgrind-devel yum install epel-release yum install cmake3 ninja-build pandoc 4.1.2 Switch Configuration (optional) This part is HW specific, please check your switch manual accordingly. Connect the console port to PC. Username is admin. No password. Enter global configuration mode. Below example is based on Arista 7060 CX2 100Gb Switch, it is to configure the 100Gb port to work at 40Gb to match the NIC speed. It is NOT required if your NIC and calbes are match. Config Switch Speed to 40Gb/s switch# enable switch# config switch(config)# show interface status Configure corresponding port to 40 Gb/s to match the NIC speed switch(config)# interface Et(num_of_port)/1 switch(config)# speed forced 40gfull RoCE might have performance issues, so PFC configuration is strongly suggested. You will need to check the RDMA NIC driver manual and switch manual to configure PFC. Below is the example for ConnectX-4 and Arista 7060-CX2 switches. Below is to set the two connection ports in the same vlan and configure it in trunk mode. Configure interface as trunk mode and add to vlan switch(config)# vlan 1 switch(config-vlan-1)# switch(config)# interface ethernet 12-16 switch(config-if-Et12-16)# switchport trunk allowed vlan 1 switch (config-if-et1) # **priority-flow-control on** switch (config-if-et1) # **priority-flow-control priority 3 no-drop** 4.1.3 Download and install drivers A. Example: Mellanox Enabling RoCE V2 RDMA (Optional) There are lots of packages need to be installed for dependency, please refer to your RDMA NIC's manualls to install it correctly. yum install atk gcc-gfortran tcsh gtk2 tcl tk please install NIC drivers accordingly. # Download MLNX_OFED_LINUX-4.7-3.2.9.0-* from https://community.mellanox.com/s/article/howto-install-mlnx-ofed-driver # e.g., wget http://www.mellanox.com/downloads/ofed/MLNX_OFED-<version>/MLNX_OFED_LINUX-<version>-<distribution>-<arch>.tgz . tar zxf MLNX_OFED_LINUX-4.7-3.2.9.0-* cd MLNX_OFED_LINUX-4.7-3.2.9.0- ./mlnxofedinstall --add-kernel-support. # The process might interpret and promote you to install dependencies. Install dependencies and try again # This process will take some time. tar zxf MLNX_OFED_LINUX-4.7-3.2.9.0-* cd MLNX_OFED_LINUX-4.7-3.2.9.0- ./mlnxofedinstall --add-kernel-support. # The process might interpret and promote you to install dependencies. Install dependencies and try again # This process will take some time. ** Restart the driver: /etc/init.d/openibd restart Might need to unload the modules if it is in use. Make sure the that the field link_layer is \u201cEthernet\u201d. Then you can use following command to get the device name. B. Enable PFC (Priority Flow Control) to guarantee stable performance (optional) Then you can use following command to get the device name If you\u2019re using Mellanox NIC, PFC is a must to guarantee stable performance. Fetch RDMA info with rdma command: rdma link 0/1: i40iw0/1: state DOWN physical_state NOP 1/1: i40iw1/1: state ACTIVE physical_state NOP 2/1: mlx5_0/1: state DOWN physical_state DISABLED netdev ens803f0 3/1: mlx5_1/1: state ACTIVE physical_state LINK_UP netdev ens803f1 lspci | grep Mellanox 86:00.0 Ethernet controller: Mellanox Technologies MT27700 Family [ConnectX-4] 86:00.1 Ethernet controller: Mellanox Technologies MT27700 Family [ConnectX-4] Set PFC: /etc/init.d/openibd restart mlnx_qos -i ens803f1 --pfc 0,0,0,1,0,0,0,0 modprobe 8021q vconfig add ens803f1 100 ifconfig ens803f1.100 $ip1/$mask up //change to your own IP ifconfig ens803f1 $ip2/$mask up //Change to your own IP for i in {0..7}; do vconfig set_egress_map ens803f1.100 $i 3 ; done tc_wrap.py -i ens803f1 -u 3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3 Modify the IP address part based on your environment and execute the script. 4.1.4 Check RDMA module Make sure the following modules are loaded: modprobe ib_core i40iw iw_cm rdma_cm rdma_ucm ib_cm ib_uverbs ``` #### 4.1.5 Validate RDMA functionalities Check that you see your RDMA interfaces listed on each server when you run the following command: **ibv_devices** Check with rping for RDMA connectivity between target interface and client interface. 1) Assign IPs to the RDMA interfaces on Target and Client. 2) On Target run:rping -sdVa &lt;Target IP&gt; 3) On Client run: rping -cdVa &lt;Target IP&gt; Example: On the server side: ```bash rping -sda $ip1 created cm_id 0x17766d0 rdma_bind_addr successful rdma_listen accepting client connection request cq_thread started. recv completion Received rkey 97a4f addr 17ce190 len 64 from peer cma_event type RDMA_CM_EVENT_ESTABLISHED cma_id 0x7fe9ec000c90 (child) ESTABLISHED Received rkey 96b40 addr 17ce1e0 len 64 from peer server received sink adv rdma write from lkey 143c0 laddr 1771190 len 64 rdma write completion rping -sda $ip2 created cm_id 0x17766d0 rdma_bind_addr successful rdma_listen \u2026 accepting client connection request cq_thread started. recv completion Received rkey 97a4f addr 17ce190 len 64 from peer cma_event type RDMA_CM_EVENT_ESTABLISHED cma_id 0x7fe9ec000c90 (child) ESTABLISHED \u2026 Received rkey 96b40 addr 17ce1e0 len 64 from peer server received sink adv rdma write from lkey 143c0 laddr 1771190 len 64 rdma write completion \u2026 ``` On Client run: rping -cdVa &lt;Target IP&gt; ```bash # Client side use .100 ip 172.168.0.209 for an example rping -c -a 172.168.0.209 -v -C 4 ping data: rdma-ping-0: ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqr ping data: rdma-ping-1: BCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrs ping data: rdma-ping-2: CDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrst ping data: rdma-ping-3: DEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstu Please refer to your NIC manuual for detail instructions on how to validate RDMA works. 5. Install dependencies for PMem Shuffle We have provided a Conda package which will automatically install dependencies needed for PMem Shuffle, refer to OAP-Installation-Guide for more information. If you have finished OAP-Installation-Guide , you can find compiled OAP jars in $HOME/miniconda2/envs/oapenv/oap_jars/ , and skip this session and jump to 6.Install PMem Shuffle for Spark 5.1 Install HPNL ( https://github.com/Intel-bigdata/HPNL ) HPNL is a fast, CPU-Efficient network library designed for modern network technology. HPNL depends on Libfabric, which is protocol independent, it supports TCP/IP, RoCE, IB, iWRAP etc. Please make sure the Libfabric is installed in your setup. Based on this issue , please make sure NOT to install Libfabric 1.9.0. You might need to install automake/libtool first to resolve dependency issues. git clone https://github.com/ofiwg/libfabric.git cd libfabric git checkout v1.6.0 ./autogen.sh ./configure --disable-sockets --enable-verbs --disable-mlx make -j && sudo make install 5.1.1 Build and install HPNL Assume Project_root_path is HPNL folder\u2019s path, HPNL here. sudo apt-get install cmake libboost-dev libboost-system-dev #Fedora dnf install cmake boost-devel boost-system git clone https://github.com/Intel-bigdata/HPNL.git cd HPNL git checkout origin/spark-pmof-test --track git submodule update --init --recursive mkdir build; cd build cmake -DWITH_VERBS=ON .. make -j && make install cd ${project_root_path}/java/hpnl mvn install 5.2 install basic C library dependencies yum install -y autoconf asciidoctor kmod-devel.x86\\_64 libudev-devel libuuid-devel json-c-devel jemalloc-devel yum groupinstall -y \"Development Tools\" 5.3 install ndctl This can be installed with your package managmenet tool as well. git clone https://github.com/pmem/ndctl.git cd ndctl git checkout v63 ./autogen.sh ./configure CFLAGS='-g -O2' --prefix=/usr --sysconfdir=/etc --libdir=/usr/lib64 make -j make check make install 5.4 install PMDK yum install -y pandoc git clone https://github.com/pmem/pmdk.git cd pmdk git checkout tags/1.8 make -j && make install export PKG_CONFIG_PATH=/usr/local/lib64/pkgconfig/:$PKG_CONFIG_PATH echo \u201cexport PKG_CONFIG_PATH=/usr/local/lib64/pkgconfig/:$PKG_CONFIG_PATH\u201d > /etc/profile.d/pmdk.sh 5.5 Install RPMem extension git clone https://github.com/efficient/libcuckoo cd libcuckoo mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr/local -DBUILD_EXAMPLES=1 -DBUILD_TESTS=1 .. make all && make install git clone -b <tag-version> https://github.com/intel-bigdata/OAP.git cd OAP/oap-shuffle/RPMem-shuffle mvn install -DskipTests 6. Install PMem Shuffle for Spark 6.1 Configure RPMem extension for spark shuffle in Spark PMem Shuffle for spark shuffle is designed as a plugin to Spark. Currently the plugin supports Spark 3.0.0 and works well on various Network fabrics, including Socket, RDMA and Omni-Path. There are several configurations files needs to be modified in order to run PMem Shuffle. Prerequisite Use below command to remove original initialization of one PMem, this is a MUST step, or RPMemShuffle won\u2019t be able to open PMem devices. pmempool rm ${device_name} #example: pmempool rm /dev/dax0.0 If you install OAP Conda package , you can use below command to remove original initialization of one PMem. ```shell script export LD_LIBRARY_PATH=$HOME/miniconda2/envs/oapenv/lib/:$LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/bin/pmempool rm ${device_name} **Refer to the Reference section for detail descrption of each parameter.** #### Enable RPMemShuffle ```bash spark.shuffle.manager org.apache.spark.shuffle.pmof.PmofShuffleManager spark.driver.extraClassPath /$path/oap-shuffle/RPMem-shuffle/core/target/oap-rpmem-shuffle-java-<version>-jar-with-dependencies.jar spark.executor.extraClassPath /$path/oap-shuffle/RPMem-shuffle/core/target/oap-rpmem-shuffle-java-<version>-jar-with-dependencies.jar Switch On/Off PMem and RDMA spark.shuffle.pmof.enable_rdma true spark.shuffle.pmof.enable_pmem true Add PMem information to spark config Explanation: spark.shuffle.pmof.pmem_capacity: the capacity of one PMem device, this value will be used when register PMem device to RDMA. spark.shuffle.pmof.pmem_list: a list of all local PMem device, make sure your per physical node executor number won\u2019t exceed PMem device number, or one PMem device maybe opened by two spark executor processes and this will leads to a PMem open failure. spark.shuffle.pmof.dev_core_set: a mapping of which core range will be task set to which PMem device, this is a performance optimal configuration for better PMem numa accessing. spark.io.compression.codec: use \u201csnappy\u201d to do shuffle data and spilling data compression, this is a MUST when enabled PMem due to a default LZ4 ByteBuffer incompatible issue. spark.shuffle.pmof.pmem_capacity ${total_size_of_one_device} spark.shuffle.pmof.pmem_list ${device_name},${device_name},\u2026 spark.shuffle.pmof.dev_core_set ${device_name}:${core_range};\u2026 #example: /dev/dax0.0:0-17,36-53;/dev/dax0.2:0-17,36-53 spark.io.compression.codec snappy Memory configuration suggestion Suitable for any release before OAP 0.8. In OAP 0.8 and later release, the memory footprint of each core is reduced dramatically and the formula below is not applicable any more. Spark.executor.memory must be greater than shuffle_block_size * numPartitions * numCores * 2 (for both shuffle and external sort), for example, default HiBench Terasort numPartition is 200, and we configured 10 cores each executor, then this executor must has memory capacity greater than 2MB(spark.shuffle.pmof.shuffle_block_size) * 200 * 10 * 2 = 8G. Recommendation configuration as below, but it needs to be adjusted accordingly based on your system configurations. Yarn.executor.num 4 // same as PMem namespaces number Yarn.executor.cores 18 // total core number divide executor number spark.executor.memory 15g // 2MB * numPartition(200) * 18 * 2 spark.yarn.executor.memoryOverhead 5g // 30% of spark.executor.memory spark.shuffle.pmof.shuffle_block_size 2096128 // 2MB \u2013 1024 Bytes spark.shuffle.pmof.spill_throttle 2096128 // 2MB \u2013 1024 Bytes, spill_throttle is used to // set throttle by when spill buffer data to // Persistent Memory, must set spill_throttle // equal to shuffle_block_size spark.driver.memory 10g spark.yarn.driver.memoryOverhead 5g Configuration of RDMA enabled case spark.shuffle.pmof.node : spark nodes and RDMA ip mapping list spark.driver.rhost / spark.driver.rport : Specify spark driver RDMA IP and port spark.shuffle.pmof.server_buffer_nums 64 spark.shuffle.pmof.client_buffer_nums 64 spark.shuffle.pmof.map_serializer_buffer_size 262144 spark.shuffle.pmof.reduce_serializer_buffer_size 262144 spark.shuffle.pmof.chunk_size 262144 spark.shuffle.pmof.server_pool_size 3 spark.shuffle.pmof.client_pool_size 3 spark.shuffle.pmof.node $HOST1-$IP1,$HOST2-$IP2//Host-IP pairs, $hostname-$ip spark.driver.rhost $IP //change to your host IP spark.driver.rport 61000 7. PMem Shuffle for Spark Testing Pmem shuffle extension have been tested and validated with Terasort and Decision support workloads. 7.1 Decision support workloads The Decision support workloads is a decision support benchmark that models several general applicable aspects of a decision support system, including queries and data maintenance. 7.1.1 Download spark-sql-perf The link is https://github.com/databricks/spark-sql-perf and follow README to use sbt build the artifact. 7.1.2 Download the kit As per instruction from spark-sql-perf README, tpcds-kit is required and please download it from https://github.com/databricks/tpcds-kit , follow README to setup the benchmark. 7.1.3 Prepare data As an example, generate parquet format data to HDFS with 1TB data scale. The data stored path, data format and data scale are configurable. Please check script below as a sample. import com.databricks.spark.sql.perf.tpcds.TPCDSTables import org.apache.spark.sql._ // Set: val rootDir: String = \"hdfs://${ip}:9000/tpcds_1T\" // root directory of location to create data in. val databaseName: String = \"tpcds_1T\" // name of database to create. val scaleFactor: String = \"1024\" // scaleFactor defines the size of the dataset to generate (in GB). val format: String = \"parquet\" // valid spark format like parquet \"parquet\". val sqlContext = new SQLContext(sc) // Run: val tables = new TPCDSTables(sqlContext, dsdgenDir = \"/mnt/spark-pmof/tool/tpcds-kit/tools\", // location of dsdgen scaleFactor = scaleFactor, useDoubleForDecimal = false, // true to replace DecimalType with DoubleType useStringForDate = false) // true to replace DateType with StringType tables.genData( location = rootDir, format = format, overwrite = true, // overwrite the data that is already there partitionTables = true, // create the partitioned fact tables clusterByPartitionColumns = true, // shuffle to get partitions coalesced into single files. filterOutNullPartitionValues = false, // true to filter out the partition with NULL key value tableFilter = \"\", // \"\" means generate all tables numPartitions = 400) // how many dsdgen partitions to run - number of input tasks. // Create the specified database sql(s\"create database $databaseName\") // Create metastore tables in a specified database for your data. // Once tables are created, the current database will be switched to the specified database. tables.createExternalTables(rootDir, \"parquet\", databaseName, overwrite = true, discoverPartitions = true) 7.1.4 Run the benchmark Launch DECISION SUPPORT WORKLOADS queries on generated data, check benchmark.scala below as a sample, it runs query64. import com.databricks.spark.sql.perf.tpcds.TPCDS import org.apache.spark.sql._ val sqlContext = new SQLContext(sc) val tpcds = new TPCDS (sqlContext = sqlContext) // Set: val databaseName = \"tpcds_1T\" // name of database with TPCDS data. val resultLocation = \"tpcds_1T_result\" // place to write results val iterations = 1 // how many iterations of queries to run. val query_filter = Seq(\"q64-v2.4\") val randomizeQueries = false def queries = { val filtered_queries = query_filter match { case Seq() => tpcds.tpcds2_4Queries case _=> tpcds.tpcds2_4Queries.filter(q => query_filter.contains(q.name)) } filtered_queries } val timeout = 24*60*60 // timeout, in seconds. // Run: sql(s\"use $databaseName\") val experiment = tpcds.runExperiment( queries, iterations = iterations, resultLocation = resultLocation, forkThread = true) experiment.waitForFinish(timeout) 7.1.5 Check the result Check the result under tpcds_1T_result folder. It can be an option to check the result at spark history server. (Need to start history server by \\$SPARK_HOME/sbin/start-history-server.sh ) 7.2 TeraSort TeraSort is a benchmark that measures the amount of time to sort one terabyte of randomly distributed data on a given computer system. 7.2.1 Download HiBench This guide uses HiBench for Terasort tests, https://github.com/Intel-bigdata/HiBench . HiBench is a big data benchmark suite and contains a set of Hadoop, Spark and streaming workloads including TeraSort. 7.2.2 Build HiBench as per instructions from build-bench . 7.2.3 Configuration Modify \\$HiBench-HOME/conf/spark.conf to specify the spark home and other spark configurations. It will overwrite the configuration of \\$SPARK-HOME/conf/spark-defaults.conf at run time. 7.2.4 Launch the benchmark Need to prepare the data with \\$HiBench-HOME/bin/workloads/micro/terasort/prepare/prepare.sh Kick off the evaluation by \\$HiBench-HOME/bin/workloads/micro/terasort/spark/run.sh Change directory to \\$HiBench-HOME/bin/workloads/micro/terasort/spark and launch the run.sh . You can add some PMEM cleaning work to make sure it starts from empty shuffle device every test iteration. Take run.sh below as a sample. # ***Change below command accordingly *** ssh ${node} pmempool rm /dev/dax0.0 current_dir=`dirname \"$0\"` current_dir=`cd \"$current_dir\"; pwd` root_dir=${current_dir}/../../../../.. workload_config=${root_dir}/conf/workloads/micro/terasort.conf . \"${root_dir}/bin/functions/load_bench_config.sh\" enter_bench ScalaSparkTerasort ${workload_config} ${current_dir} show_bannar start rmr_hdfs $OUTPUT_HDFS || true SIZE=`dir_size $INPUT_HDFS` START_TIME=`timestamp` run_spark_job com.intel.hibench.sparkbench.micro.ScalaTeraSort $INPUT_HDFS $OUTPUT_HDFS END_TIME=`timestamp` gen_report ${START_TIME} ${END_TIME} ${SIZE} show_bannar finish leave_bench 7.2.5 Check the result Check the result at spark history server to see the execution time and other spark metrics like spark shuffle spill status. (Need to start history server by \\$SPARK_HOME/sbin/start-history-server.sh ) Reference RPMemShuffle Spark configuration Before running Spark workload, add following contents in spark-defaults.conf . spark.executor.instances 4 // same as total PMem namespace numbers of your cluster spark.executor.cores 18 // total core number divide executor number spark.executor.memory 70g // 4~5G * spark.executor.cores spark.executor.memoryOverhead 15g // 30% of spark.executor.memory spark.shuffle.pmof.shuffle_block_size 2096128 // 2MB \u2013 1024 Bytes spark.shuffle.pmof.spill_throttle 2096128 // 2MB \u2013 1024 Bytes spark.driver.memory 10g spark.yarn.driver.memoryOverhead 5g spark.shuffle.compress true spark.io.compression.codec snappy spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-rpmem-shuffle-java-<version>-jar-with-dependencies.jar spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-rpmem-shuffle-java-<version>-jar-with-dependencies.jar spark.shuffle.manager org.apache.spark.shuffle.pmof.PmofShuffleManager spark.shuffle.pmof.enable_rdma true spark.shuffle.pmof.enable_pmem true spark.shuffle.pmof.pmem_capacity 126833655808 // size should be same as pmem size spark.shuffle.pmof.pmem_list /dev/dax0.0,/dev/dax0.1,/dev/dax1.0,/dev/dax1.1 spark.shuffle.pmof.dev_core_set dax0.0:0-71,dax0.1:0-71,dax1.0:0-71,dax1.1:0-71 spark.shuffle.pmof.server_buffer_nums 64 spark.shuffle.pmof.client_buffer_nums 64 spark.shuffle.pmof.map_serializer_buffer_size 262144 spark.shuffle.pmof.reduce_serializer_buffer_size 262144 spark.shuffle.pmof.chunk_size 262144 spark.shuffle.pmof.server_pool_size 3 spark.shuffle.pmof.client_pool_size 3 spark.shuffle.pmof.node $host1-$IP1,$host2-$IP2//HOST-IP Pair, seperate with \",\" spark.driver.rhost $IP //change to your host spark.driver.rport 61000 Reference guides (without BKC access) If you do not have BKC access, please following below official guide: (1): General PMEMM support: PMEMM support https://www.intel.com/content/www/us/en/support/products/190349/memory-and-storage/data-center-persistent-memory/intel-optane-dc-persistent-memory.html (2) PMEMM population rule: Module DIMM Population for Intel\u00ae Optane\u2122 DC Persistent Memory https://www.intel.com/content/www/us/en/support/articles/000032932/memory-and-storage/data-center-persistent-memory.html?productId=190349&localeCode=us_en (3) OS support requirement: Operating System OS for Intel\u00ae Optane\u2122 DC Persistent Memory https://www.intel.com/content/www/us/en/support/articles/000032860/memory-and-storage/data-center-persistent-memory.html?productId=190349&localeCode=us_en (4): Quick Start Guide: Provision Intel\u00ae Optane\u2122 DC Persistent Memory https://software.intel.com/en-us/articles/quick-start-guide-configure-intel-optane-dc-persistent-memory-on-linux","title":"User Guide"},{"location":"PMemShuffle/User-Guide/#pmem-shuffle-for-apache-spark-guide","text":"1. PMem Shuffle introduction 2. Recommended HW environment 3. Install and configure PMem 4. Configure and Validate RDMA 5. Install dependencies for PMem Shuffle 6. Install PMem Shuffle for Spark 7. PMem Shuffle for Spark Testing Reference PMem Shuffle for Spark (previously Spark-PMoF) depends on multiple native libraries like libfabrics, libcuckoo, PMDK. This enabling guide covers the installing process for the time being, but it might change as the install commands and related dependency packages for the 3rd party libraries might vary depending on the OS version and distribution you are using. Yarn, HDFS, Spark installation and configuration is out of the scope of this document.","title":"PMem Shuffle for Apache Spark Guide"},{"location":"PMemShuffle/User-Guide/#1-pmem-shuffle-introduction","text":"Intel Optane DC persistent memory is the next-generation storage at memory speed. It closes the performance gap between DRAM memory technology and traditional NAND SSDs. Remote Persistent Memory extends PMem usage to new scenario, lots of new usage cases & value proposition can be developed. Spark shuffle is a high cost operation as it issues a great number of small random disk IO, serialization, network data transmission, and thus contributes a lot to job latency and could be the bottleneck for workloads performance. PMem Shuffle for spark (previously Spark PMoF) https://github.com/Intel-bigdata/Spark-PMoF ) is a Persistent Memory over Fabrics (PMoF) plugin for Spark shuffle, which leverages the RDMA network and remote persistent memory (for read) to provide extremely high performance and low latency shuffle solutions for Spark to address performance issues for shuffle intensive workloads. PMem Shuffle brings follow benefits: Leverage high performance persistent memory as shuffle media as well as spill media, increased shuffle performance and reduced memory footprint Using PMDK libs to avoid inefficient context switches and memory copies with zero-copy remote access to persistent memory. Leveraging RDMA for network offloading The Figure 1 shows the high level architecture of PMem Shuffle, it shows how data flows between Spark and shuffle devices in PMem Shuffle for spark shuffle and Vanilla Spark. In this guide, we will introduce how to deploy and use PMem Shuffle for Spark. Figure 1: PMem Shuffle for Spark","title":"1. PMem Shuffle introduction"},{"location":"PMemShuffle/User-Guide/#2-recommended-hw-environment","text":"","title":"2. Recommended HW environment"},{"location":"PMemShuffle/User-Guide/#21-system-configuration","text":"","title":"2.1. System Configuration"},{"location":"PMemShuffle/User-Guide/#211-hw-and-sw-configuration","text":"A 4x or 3x Node cluster is recommended for a proof of concept tests, depending your system configurations, if using 3 nodes cluster, the Name node and Spark Master node can be co-located with one of the Hadoop data nodes. Hardware: - Intel\u00ae Xeon\u2122 processor Gold 6240 CPU @ 2.60GHz, 384GB Memory (12x 32GB 2666 MT/s) or 192GB Memory (12x 16GB 2666MT/s) - An RDMA capable NIC, 40Gb+ is preferred. e.g., 1x Intel X722 NIC or Mellanox ConnectX-4 40Gb NIC - RDMA cables: - Mellanox MCP1600-C003 100GbE 3m 28AWG - Shuffle Devices\uff1a - 1x 1TB HDD for shuffle (baseline) - 4x 128GB Persistent Memory for shuffle - 4x 1T NVMe for HDFS Switch : - Arista 7060 CX2 (7060CX2-32S-F) 100Gb switches was used Please refer to section 4.2 for configurations Software: Hadoop 2.7 Spark 3.0.0 Fedora 29 with ww08.2019 BKC","title":"2.1.1 HW and SW Configuration"},{"location":"PMemShuffle/User-Guide/#22-recommended-rdma-nic","text":"PMem Shuffle is using HPNL ( https://cloud.google.com/solutions/big-data/ ) for network communication, which leverages libfabric for efficient network communication, so a RDMA capable NIC is recommended. Libfabric supports RoCE, iWrap, IB protocol, so various RNICs with different protocol can be used.","title":"2.2. Recommended RDMA NIC"},{"location":"PMemShuffle/User-Guide/#23-recommended-pmem-configuration","text":"It is recommended to install 4+ PMem DIMMs on the SUT, but you can adjust the numbers accordingly. In this enabling guide, 4x 128GB PMEMM was installed on the SUT as an exmaple.","title":"2.3 Recommended PMEM configuration"},{"location":"PMemShuffle/User-Guide/#24-recommended-pmem-bkc-optional","text":"This development guide was based on ww08.2019 BKC (best known configuration). Please contact your HW vendor for latest BKC. Please refer to backup if you do not have BKC access. BKC installation/enabling or FW installation is out of the scope of this guide.","title":"2.4 Recommended PMEM BKC (optional)"},{"location":"PMemShuffle/User-Guide/#3-install-and-configure-pmem-example","text":"1) Please install ipmctl and ndctl according to your OS version 2) Run ipmctl show -dimm to check whether dimms can be recognized 3) Run ipmctl create -goal PersistentMemoryType=AppDirect to create AD mode 4) Run ndctl list -R , you will see region0 and region1 . 5) Assume you have 4x PMEM installed on 1 node. a. Run ndctl create-namespace -m devdax -r region0 -s 120g b. Run ndctl create-namespace -m devdax -r region0 -s 120g c. Run ndctl create-namespace -m devdax -r region1 -s 120g d. Run ndctl create-namespace -m devdax -r region1 -s 120g This will create four namespaces, namely /dev/dax0.0, /dev/dax0.1, /dev/dax1.0, /dev/dax1.1 in that node, and it will be used as PMem Shuffle media. You can change your configuration (namespaces numbers, size) accordingly.","title":"3. Install and configure PMEM (example)"},{"location":"PMemShuffle/User-Guide/#4-configure-and-validate-rdma","text":"Notes This part is vendor specific, it might NOT apply to your environment, please check your switch, NIC manuals accordingly.","title":"4. Configure and Validate RDMA"},{"location":"PMemShuffle/User-Guide/#41-configure-and-test-iwarp-rdma","text":"","title":"4.1 Configure and test iWARP RDMA"},{"location":"PMemShuffle/User-Guide/#411-download-rdma-core-and-install-dependencies","text":"The rdma-core provides the necessary userspace libraries to test rdma connectivity with tests such as rping. Refer to latest rdma-core documentation for updated installation guidelines (https://github.com/linux-rdma/rdma-core.git). You might refer to HW specific instructions or guide to enable your RDMA NICs. Take Mellanox as an example, perform below steps to enable it: git clone <https://github.com/linux-rdma/rdma-core.git> dnf install cmake gcc libnl3-devel libudev-devel pkgconfig valgrind-devel ninja-build python3-devel python3-Cython python3-docutils pandoc //change to yum on centos bash build.sh #on centos 7 yum install cmake gcc libnl3-devel libudev-devel make pkgconfig valgrind-devel yum install epel-release yum install cmake3 ninja-build pandoc","title":"4.1.1 Download rdma-core and install dependencies"},{"location":"PMemShuffle/User-Guide/#412-switch-configuration-optional","text":"This part is HW specific, please check your switch manual accordingly. Connect the console port to PC. Username is admin. No password. Enter global configuration mode. Below example is based on Arista 7060 CX2 100Gb Switch, it is to configure the 100Gb port to work at 40Gb to match the NIC speed. It is NOT required if your NIC and calbes are match. Config Switch Speed to 40Gb/s switch# enable switch# config switch(config)# show interface status Configure corresponding port to 40 Gb/s to match the NIC speed switch(config)# interface Et(num_of_port)/1 switch(config)# speed forced 40gfull RoCE might have performance issues, so PFC configuration is strongly suggested. You will need to check the RDMA NIC driver manual and switch manual to configure PFC. Below is the example for ConnectX-4 and Arista 7060-CX2 switches. Below is to set the two connection ports in the same vlan and configure it in trunk mode. Configure interface as trunk mode and add to vlan switch(config)# vlan 1 switch(config-vlan-1)# switch(config)# interface ethernet 12-16 switch(config-if-Et12-16)# switchport trunk allowed vlan 1 switch (config-if-et1) # **priority-flow-control on** switch (config-if-et1) # **priority-flow-control priority 3 no-drop**","title":"4.1.2 Switch Configuration (optional)"},{"location":"PMemShuffle/User-Guide/#413-download-and-install-drivers","text":"","title":"4.1.3 Download and install drivers"},{"location":"PMemShuffle/User-Guide/#a-example-mellanox-enabling-roce-v2-rdma-optional","text":"There are lots of packages need to be installed for dependency, please refer to your RDMA NIC's manualls to install it correctly. yum install atk gcc-gfortran tcsh gtk2 tcl tk please install NIC drivers accordingly. # Download MLNX_OFED_LINUX-4.7-3.2.9.0-* from https://community.mellanox.com/s/article/howto-install-mlnx-ofed-driver # e.g., wget http://www.mellanox.com/downloads/ofed/MLNX_OFED-<version>/MLNX_OFED_LINUX-<version>-<distribution>-<arch>.tgz . tar zxf MLNX_OFED_LINUX-4.7-3.2.9.0-* cd MLNX_OFED_LINUX-4.7-3.2.9.0- ./mlnxofedinstall --add-kernel-support. # The process might interpret and promote you to install dependencies. Install dependencies and try again # This process will take some time. tar zxf MLNX_OFED_LINUX-4.7-3.2.9.0-* cd MLNX_OFED_LINUX-4.7-3.2.9.0- ./mlnxofedinstall --add-kernel-support. # The process might interpret and promote you to install dependencies. Install dependencies and try again # This process will take some time. ** Restart the driver: /etc/init.d/openibd restart Might need to unload the modules if it is in use. Make sure the that the field link_layer is \u201cEthernet\u201d. Then you can use following command to get the device name.","title":"A. Example: Mellanox Enabling RoCE V2 RDMA (Optional)"},{"location":"PMemShuffle/User-Guide/#b-enable-pfc-priority-flow-control-to-guarantee-stable-performance-optional","text":"Then you can use following command to get the device name If you\u2019re using Mellanox NIC, PFC is a must to guarantee stable performance. Fetch RDMA info with rdma command: rdma link 0/1: i40iw0/1: state DOWN physical_state NOP 1/1: i40iw1/1: state ACTIVE physical_state NOP 2/1: mlx5_0/1: state DOWN physical_state DISABLED netdev ens803f0 3/1: mlx5_1/1: state ACTIVE physical_state LINK_UP netdev ens803f1 lspci | grep Mellanox 86:00.0 Ethernet controller: Mellanox Technologies MT27700 Family [ConnectX-4] 86:00.1 Ethernet controller: Mellanox Technologies MT27700 Family [ConnectX-4] Set PFC: /etc/init.d/openibd restart mlnx_qos -i ens803f1 --pfc 0,0,0,1,0,0,0,0 modprobe 8021q vconfig add ens803f1 100 ifconfig ens803f1.100 $ip1/$mask up //change to your own IP ifconfig ens803f1 $ip2/$mask up //Change to your own IP for i in {0..7}; do vconfig set_egress_map ens803f1.100 $i 3 ; done tc_wrap.py -i ens803f1 -u 3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3 Modify the IP address part based on your environment and execute the script.","title":"B. Enable PFC (Priority Flow Control) to guarantee stable performance (optional)"},{"location":"PMemShuffle/User-Guide/#414-check-rdma-module","text":"Make sure the following modules are loaded: modprobe ib_core i40iw iw_cm rdma_cm rdma_ucm ib_cm ib_uverbs ``` #### 4.1.5 Validate RDMA functionalities Check that you see your RDMA interfaces listed on each server when you run the following command: **ibv_devices** Check with rping for RDMA connectivity between target interface and client interface. 1) Assign IPs to the RDMA interfaces on Target and Client. 2) On Target run:rping -sdVa &lt;Target IP&gt; 3) On Client run: rping -cdVa &lt;Target IP&gt; Example: On the server side: ```bash rping -sda $ip1 created cm_id 0x17766d0 rdma_bind_addr successful rdma_listen accepting client connection request cq_thread started. recv completion Received rkey 97a4f addr 17ce190 len 64 from peer cma_event type RDMA_CM_EVENT_ESTABLISHED cma_id 0x7fe9ec000c90 (child) ESTABLISHED Received rkey 96b40 addr 17ce1e0 len 64 from peer server received sink adv rdma write from lkey 143c0 laddr 1771190 len 64 rdma write completion rping -sda $ip2 created cm_id 0x17766d0 rdma_bind_addr successful rdma_listen \u2026 accepting client connection request cq_thread started. recv completion Received rkey 97a4f addr 17ce190 len 64 from peer cma_event type RDMA_CM_EVENT_ESTABLISHED cma_id 0x7fe9ec000c90 (child) ESTABLISHED \u2026 Received rkey 96b40 addr 17ce1e0 len 64 from peer server received sink adv rdma write from lkey 143c0 laddr 1771190 len 64 rdma write completion \u2026 ``` On Client run: rping -cdVa &lt;Target IP&gt; ```bash # Client side use .100 ip 172.168.0.209 for an example rping -c -a 172.168.0.209 -v -C 4 ping data: rdma-ping-0: ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqr ping data: rdma-ping-1: BCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrs ping data: rdma-ping-2: CDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrst ping data: rdma-ping-3: DEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstu Please refer to your NIC manuual for detail instructions on how to validate RDMA works.","title":"4.1.4 Check RDMA module"},{"location":"PMemShuffle/User-Guide/#5-install-dependencies-for-pmem-shuffle","text":"We have provided a Conda package which will automatically install dependencies needed for PMem Shuffle, refer to OAP-Installation-Guide for more information. If you have finished OAP-Installation-Guide , you can find compiled OAP jars in $HOME/miniconda2/envs/oapenv/oap_jars/ , and skip this session and jump to 6.Install PMem Shuffle for Spark","title":"5. Install dependencies for PMem Shuffle"},{"location":"PMemShuffle/User-Guide/#51-install-hpnl-httpsgithubcomintel-bigdatahpnl","text":"HPNL is a fast, CPU-Efficient network library designed for modern network technology. HPNL depends on Libfabric, which is protocol independent, it supports TCP/IP, RoCE, IB, iWRAP etc. Please make sure the Libfabric is installed in your setup. Based on this issue , please make sure NOT to install Libfabric 1.9.0. You might need to install automake/libtool first to resolve dependency issues. git clone https://github.com/ofiwg/libfabric.git cd libfabric git checkout v1.6.0 ./autogen.sh ./configure --disable-sockets --enable-verbs --disable-mlx make -j && sudo make install","title":"5.1 Install HPNL (https://github.com/Intel-bigdata/HPNL)"},{"location":"PMemShuffle/User-Guide/#511-build-and-install-hpnl","text":"Assume Project_root_path is HPNL folder\u2019s path, HPNL here. sudo apt-get install cmake libboost-dev libboost-system-dev #Fedora dnf install cmake boost-devel boost-system git clone https://github.com/Intel-bigdata/HPNL.git cd HPNL git checkout origin/spark-pmof-test --track git submodule update --init --recursive mkdir build; cd build cmake -DWITH_VERBS=ON .. make -j && make install cd ${project_root_path}/java/hpnl mvn install","title":"5.1.1 Build and install HPNL"},{"location":"PMemShuffle/User-Guide/#52-install-basic-c-library-dependencies","text":"yum install -y autoconf asciidoctor kmod-devel.x86\\_64 libudev-devel libuuid-devel json-c-devel jemalloc-devel yum groupinstall -y \"Development Tools\"","title":"5.2 install basic C library dependencies"},{"location":"PMemShuffle/User-Guide/#53-install-ndctl","text":"This can be installed with your package managmenet tool as well. git clone https://github.com/pmem/ndctl.git cd ndctl git checkout v63 ./autogen.sh ./configure CFLAGS='-g -O2' --prefix=/usr --sysconfdir=/etc --libdir=/usr/lib64 make -j make check make install","title":"5.3 install ndctl"},{"location":"PMemShuffle/User-Guide/#54-install-pmdk","text":"yum install -y pandoc git clone https://github.com/pmem/pmdk.git cd pmdk git checkout tags/1.8 make -j && make install export PKG_CONFIG_PATH=/usr/local/lib64/pkgconfig/:$PKG_CONFIG_PATH echo \u201cexport PKG_CONFIG_PATH=/usr/local/lib64/pkgconfig/:$PKG_CONFIG_PATH\u201d > /etc/profile.d/pmdk.sh","title":"5.4 install PMDK"},{"location":"PMemShuffle/User-Guide/#55-install-rpmem-extension","text":"git clone https://github.com/efficient/libcuckoo cd libcuckoo mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr/local -DBUILD_EXAMPLES=1 -DBUILD_TESTS=1 .. make all && make install git clone -b <tag-version> https://github.com/intel-bigdata/OAP.git cd OAP/oap-shuffle/RPMem-shuffle mvn install -DskipTests","title":"5.5 Install RPMem extension"},{"location":"PMemShuffle/User-Guide/#6-install-pmem-shuffle-for-spark","text":"","title":"6. Install PMem Shuffle for Spark"},{"location":"PMemShuffle/User-Guide/#61-configure-rpmem-extension-for-spark-shuffle-in-spark","text":"PMem Shuffle for spark shuffle is designed as a plugin to Spark. Currently the plugin supports Spark 3.0.0 and works well on various Network fabrics, including Socket, RDMA and Omni-Path. There are several configurations files needs to be modified in order to run PMem Shuffle.","title":"6.1 Configure RPMem extension for spark shuffle in Spark"},{"location":"PMemShuffle/User-Guide/#prerequisite","text":"Use below command to remove original initialization of one PMem, this is a MUST step, or RPMemShuffle won\u2019t be able to open PMem devices. pmempool rm ${device_name} #example: pmempool rm /dev/dax0.0 If you install OAP Conda package , you can use below command to remove original initialization of one PMem. ```shell script export LD_LIBRARY_PATH=$HOME/miniconda2/envs/oapenv/lib/:$LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/bin/pmempool rm ${device_name} **Refer to the Reference section for detail descrption of each parameter.** #### Enable RPMemShuffle ```bash spark.shuffle.manager org.apache.spark.shuffle.pmof.PmofShuffleManager spark.driver.extraClassPath /$path/oap-shuffle/RPMem-shuffle/core/target/oap-rpmem-shuffle-java-<version>-jar-with-dependencies.jar spark.executor.extraClassPath /$path/oap-shuffle/RPMem-shuffle/core/target/oap-rpmem-shuffle-java-<version>-jar-with-dependencies.jar","title":"Prerequisite"},{"location":"PMemShuffle/User-Guide/#switch-onoff-pmem-and-rdma","text":"spark.shuffle.pmof.enable_rdma true spark.shuffle.pmof.enable_pmem true","title":"Switch On/Off PMem and RDMA"},{"location":"PMemShuffle/User-Guide/#add-pmem-information-to-spark-config","text":"Explanation: spark.shuffle.pmof.pmem_capacity: the capacity of one PMem device, this value will be used when register PMem device to RDMA. spark.shuffle.pmof.pmem_list: a list of all local PMem device, make sure your per physical node executor number won\u2019t exceed PMem device number, or one PMem device maybe opened by two spark executor processes and this will leads to a PMem open failure. spark.shuffle.pmof.dev_core_set: a mapping of which core range will be task set to which PMem device, this is a performance optimal configuration for better PMem numa accessing. spark.io.compression.codec: use \u201csnappy\u201d to do shuffle data and spilling data compression, this is a MUST when enabled PMem due to a default LZ4 ByteBuffer incompatible issue. spark.shuffle.pmof.pmem_capacity ${total_size_of_one_device} spark.shuffle.pmof.pmem_list ${device_name},${device_name},\u2026 spark.shuffle.pmof.dev_core_set ${device_name}:${core_range};\u2026 #example: /dev/dax0.0:0-17,36-53;/dev/dax0.2:0-17,36-53 spark.io.compression.codec snappy","title":"Add PMem information to spark config"},{"location":"PMemShuffle/User-Guide/#memory-configuration-suggestion","text":"Suitable for any release before OAP 0.8. In OAP 0.8 and later release, the memory footprint of each core is reduced dramatically and the formula below is not applicable any more. Spark.executor.memory must be greater than shuffle_block_size * numPartitions * numCores * 2 (for both shuffle and external sort), for example, default HiBench Terasort numPartition is 200, and we configured 10 cores each executor, then this executor must has memory capacity greater than 2MB(spark.shuffle.pmof.shuffle_block_size) * 200 * 10 * 2 = 8G. Recommendation configuration as below, but it needs to be adjusted accordingly based on your system configurations. Yarn.executor.num 4 // same as PMem namespaces number Yarn.executor.cores 18 // total core number divide executor number spark.executor.memory 15g // 2MB * numPartition(200) * 18 * 2 spark.yarn.executor.memoryOverhead 5g // 30% of spark.executor.memory spark.shuffle.pmof.shuffle_block_size 2096128 // 2MB \u2013 1024 Bytes spark.shuffle.pmof.spill_throttle 2096128 // 2MB \u2013 1024 Bytes, spill_throttle is used to // set throttle by when spill buffer data to // Persistent Memory, must set spill_throttle // equal to shuffle_block_size spark.driver.memory 10g spark.yarn.driver.memoryOverhead 5g Configuration of RDMA enabled case spark.shuffle.pmof.node : spark nodes and RDMA ip mapping list spark.driver.rhost / spark.driver.rport : Specify spark driver RDMA IP and port spark.shuffle.pmof.server_buffer_nums 64 spark.shuffle.pmof.client_buffer_nums 64 spark.shuffle.pmof.map_serializer_buffer_size 262144 spark.shuffle.pmof.reduce_serializer_buffer_size 262144 spark.shuffle.pmof.chunk_size 262144 spark.shuffle.pmof.server_pool_size 3 spark.shuffle.pmof.client_pool_size 3 spark.shuffle.pmof.node $HOST1-$IP1,$HOST2-$IP2//Host-IP pairs, $hostname-$ip spark.driver.rhost $IP //change to your host IP spark.driver.rport 61000","title":"Memory configuration suggestion"},{"location":"PMemShuffle/User-Guide/#7-pmem-shuffle-for-spark-testing","text":"Pmem shuffle extension have been tested and validated with Terasort and Decision support workloads.","title":"7. PMem Shuffle for Spark Testing"},{"location":"PMemShuffle/User-Guide/#71-decision-support-workloads","text":"The Decision support workloads is a decision support benchmark that models several general applicable aspects of a decision support system, including queries and data maintenance.","title":"7.1 Decision support workloads"},{"location":"PMemShuffle/User-Guide/#711-download-spark-sql-perf","text":"The link is https://github.com/databricks/spark-sql-perf and follow README to use sbt build the artifact.","title":"7.1.1 Download spark-sql-perf"},{"location":"PMemShuffle/User-Guide/#712-download-the-kit","text":"As per instruction from spark-sql-perf README, tpcds-kit is required and please download it from https://github.com/databricks/tpcds-kit , follow README to setup the benchmark.","title":"7.1.2 Download the kit"},{"location":"PMemShuffle/User-Guide/#713-prepare-data","text":"As an example, generate parquet format data to HDFS with 1TB data scale. The data stored path, data format and data scale are configurable. Please check script below as a sample. import com.databricks.spark.sql.perf.tpcds.TPCDSTables import org.apache.spark.sql._ // Set: val rootDir: String = \"hdfs://${ip}:9000/tpcds_1T\" // root directory of location to create data in. val databaseName: String = \"tpcds_1T\" // name of database to create. val scaleFactor: String = \"1024\" // scaleFactor defines the size of the dataset to generate (in GB). val format: String = \"parquet\" // valid spark format like parquet \"parquet\". val sqlContext = new SQLContext(sc) // Run: val tables = new TPCDSTables(sqlContext, dsdgenDir = \"/mnt/spark-pmof/tool/tpcds-kit/tools\", // location of dsdgen scaleFactor = scaleFactor, useDoubleForDecimal = false, // true to replace DecimalType with DoubleType useStringForDate = false) // true to replace DateType with StringType tables.genData( location = rootDir, format = format, overwrite = true, // overwrite the data that is already there partitionTables = true, // create the partitioned fact tables clusterByPartitionColumns = true, // shuffle to get partitions coalesced into single files. filterOutNullPartitionValues = false, // true to filter out the partition with NULL key value tableFilter = \"\", // \"\" means generate all tables numPartitions = 400) // how many dsdgen partitions to run - number of input tasks. // Create the specified database sql(s\"create database $databaseName\") // Create metastore tables in a specified database for your data. // Once tables are created, the current database will be switched to the specified database. tables.createExternalTables(rootDir, \"parquet\", databaseName, overwrite = true, discoverPartitions = true)","title":"7.1.3 Prepare data"},{"location":"PMemShuffle/User-Guide/#714-run-the-benchmark","text":"Launch DECISION SUPPORT WORKLOADS queries on generated data, check benchmark.scala below as a sample, it runs query64. import com.databricks.spark.sql.perf.tpcds.TPCDS import org.apache.spark.sql._ val sqlContext = new SQLContext(sc) val tpcds = new TPCDS (sqlContext = sqlContext) // Set: val databaseName = \"tpcds_1T\" // name of database with TPCDS data. val resultLocation = \"tpcds_1T_result\" // place to write results val iterations = 1 // how many iterations of queries to run. val query_filter = Seq(\"q64-v2.4\") val randomizeQueries = false def queries = { val filtered_queries = query_filter match { case Seq() => tpcds.tpcds2_4Queries case _=> tpcds.tpcds2_4Queries.filter(q => query_filter.contains(q.name)) } filtered_queries } val timeout = 24*60*60 // timeout, in seconds. // Run: sql(s\"use $databaseName\") val experiment = tpcds.runExperiment( queries, iterations = iterations, resultLocation = resultLocation, forkThread = true) experiment.waitForFinish(timeout)","title":"7.1.4 Run the benchmark"},{"location":"PMemShuffle/User-Guide/#715-check-the-result","text":"Check the result under tpcds_1T_result folder. It can be an option to check the result at spark history server. (Need to start history server by \\$SPARK_HOME/sbin/start-history-server.sh )","title":"7.1.5 Check the result"},{"location":"PMemShuffle/User-Guide/#72-terasort","text":"TeraSort is a benchmark that measures the amount of time to sort one terabyte of randomly distributed data on a given computer system.","title":"7.2 TeraSort"},{"location":"PMemShuffle/User-Guide/#721-download-hibench","text":"This guide uses HiBench for Terasort tests, https://github.com/Intel-bigdata/HiBench . HiBench is a big data benchmark suite and contains a set of Hadoop, Spark and streaming workloads including TeraSort.","title":"7.2.1 Download HiBench"},{"location":"PMemShuffle/User-Guide/#722-build-hibench-as-per-instructions-from-build-bench","text":"","title":"7.2.2 Build HiBench as per instructions from build-bench."},{"location":"PMemShuffle/User-Guide/#723-configuration","text":"Modify \\$HiBench-HOME/conf/spark.conf to specify the spark home and other spark configurations. It will overwrite the configuration of \\$SPARK-HOME/conf/spark-defaults.conf at run time.","title":"7.2.3 Configuration"},{"location":"PMemShuffle/User-Guide/#724-launch-the-benchmark","text":"Need to prepare the data with \\$HiBench-HOME/bin/workloads/micro/terasort/prepare/prepare.sh Kick off the evaluation by \\$HiBench-HOME/bin/workloads/micro/terasort/spark/run.sh Change directory to \\$HiBench-HOME/bin/workloads/micro/terasort/spark and launch the run.sh . You can add some PMEM cleaning work to make sure it starts from empty shuffle device every test iteration. Take run.sh below as a sample. # ***Change below command accordingly *** ssh ${node} pmempool rm /dev/dax0.0 current_dir=`dirname \"$0\"` current_dir=`cd \"$current_dir\"; pwd` root_dir=${current_dir}/../../../../.. workload_config=${root_dir}/conf/workloads/micro/terasort.conf . \"${root_dir}/bin/functions/load_bench_config.sh\" enter_bench ScalaSparkTerasort ${workload_config} ${current_dir} show_bannar start rmr_hdfs $OUTPUT_HDFS || true SIZE=`dir_size $INPUT_HDFS` START_TIME=`timestamp` run_spark_job com.intel.hibench.sparkbench.micro.ScalaTeraSort $INPUT_HDFS $OUTPUT_HDFS END_TIME=`timestamp` gen_report ${START_TIME} ${END_TIME} ${SIZE} show_bannar finish leave_bench","title":"7.2.4 Launch the benchmark"},{"location":"PMemShuffle/User-Guide/#725-check-the-result","text":"Check the result at spark history server to see the execution time and other spark metrics like spark shuffle spill status. (Need to start history server by \\$SPARK_HOME/sbin/start-history-server.sh )","title":"7.2.5 Check the result"},{"location":"PMemShuffle/User-Guide/#reference","text":"","title":"Reference"},{"location":"PMemShuffle/User-Guide/#rpmemshuffle-spark-configuration","text":"Before running Spark workload, add following contents in spark-defaults.conf . spark.executor.instances 4 // same as total PMem namespace numbers of your cluster spark.executor.cores 18 // total core number divide executor number spark.executor.memory 70g // 4~5G * spark.executor.cores spark.executor.memoryOverhead 15g // 30% of spark.executor.memory spark.shuffle.pmof.shuffle_block_size 2096128 // 2MB \u2013 1024 Bytes spark.shuffle.pmof.spill_throttle 2096128 // 2MB \u2013 1024 Bytes spark.driver.memory 10g spark.yarn.driver.memoryOverhead 5g spark.shuffle.compress true spark.io.compression.codec snappy spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-rpmem-shuffle-java-<version>-jar-with-dependencies.jar spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-rpmem-shuffle-java-<version>-jar-with-dependencies.jar spark.shuffle.manager org.apache.spark.shuffle.pmof.PmofShuffleManager spark.shuffle.pmof.enable_rdma true spark.shuffle.pmof.enable_pmem true spark.shuffle.pmof.pmem_capacity 126833655808 // size should be same as pmem size spark.shuffle.pmof.pmem_list /dev/dax0.0,/dev/dax0.1,/dev/dax1.0,/dev/dax1.1 spark.shuffle.pmof.dev_core_set dax0.0:0-71,dax0.1:0-71,dax1.0:0-71,dax1.1:0-71 spark.shuffle.pmof.server_buffer_nums 64 spark.shuffle.pmof.client_buffer_nums 64 spark.shuffle.pmof.map_serializer_buffer_size 262144 spark.shuffle.pmof.reduce_serializer_buffer_size 262144 spark.shuffle.pmof.chunk_size 262144 spark.shuffle.pmof.server_pool_size 3 spark.shuffle.pmof.client_pool_size 3 spark.shuffle.pmof.node $host1-$IP1,$host2-$IP2//HOST-IP Pair, seperate with \",\" spark.driver.rhost $IP //change to your host spark.driver.rport 61000","title":"RPMemShuffle Spark configuration"},{"location":"PMemShuffle/User-Guide/#reference-guides-without-bkc-access","text":"If you do not have BKC access, please following below official guide: (1): General PMEMM support: PMEMM support https://www.intel.com/content/www/us/en/support/products/190349/memory-and-storage/data-center-persistent-memory/intel-optane-dc-persistent-memory.html (2) PMEMM population rule: Module DIMM Population for Intel\u00ae Optane\u2122 DC Persistent Memory https://www.intel.com/content/www/us/en/support/articles/000032932/memory-and-storage/data-center-persistent-memory.html?productId=190349&localeCode=us_en (3) OS support requirement: Operating System OS for Intel\u00ae Optane\u2122 DC Persistent Memory https://www.intel.com/content/www/us/en/support/articles/000032860/memory-and-storage/data-center-persistent-memory.html?productId=190349&localeCode=us_en (4): Quick Start Guide: Provision Intel\u00ae Optane\u2122 DC Persistent Memory https://software.intel.com/en-us/articles/quick-start-guide-configure-intel-optane-dc-persistent-memory-on-linux","title":"Reference guides (without BKC access)"},{"location":"PMemSpill/","text":"PMem Spill Contents Introduction User Guide Introduction PMem Spill supports RDD Cache with Optane PMem. Spark has various storage levels serving for different purposes including memory and disk. PMem storage level is added to support a new tier for storage level besides memory and disk. Using PMem library to access Optane PMem can help to avoid the overhead from disk. Large capacity and high I/O performance of PMem shows better performance than tied DRAM and disk solution under the same cost. User Guide Installation We have provided a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. If you have finished OAP-Installation-Guide , you needn't compile and install Memkind, and you can find compiled OAP jars in $HOME/miniconda2/envs/oapenv/oap_jars . Prerequisites The following are required to configure OAP to use PMem cache in AppDirect mode. - PMem hardware is successfully deployed on each node in cluster. - Directories exposing PMem hardware on each socket. For example, on a two socket system the mounted PMem directories should appear as /mnt/pmem0 and /mnt/pmem1 . Correctly installed PMem must be formatted and mounted on every cluster worker node. // use ipmctl command to show topology and dimm info of PMem ipmctl show -topology ipmctl show -dimm // provision PMem in app direct mode ipmctl create -goal PersistentMemoryType=AppDirect // reboot system to make configuration take affect reboot // check capacity provisioned for app direct mode(AppDirectCapacity) ipmctl show -memoryresources // show the PMem region information ipmctl show -region // create namespace based on the region, multi namespaces can be created on a single region ndctl create-namespace -m fsdax -r region0 ndctl create-namespace -m fsdax -r region1 // show the created namespaces fdisk -l // create and mount file system echo y | mkfs.ext4 /dev/pmem0 echo y | mkfs.ext4 /dev/pmem1 mkdir -p /mnt/pmem0 mkdir -p /mnt/pmem1 mount -o dax /dev/pmem0 /mnt/pmem0 mount -o dax /dev/pmem1 /mnt/pmem1 In this case file systems are generated for 2 numa nodes, which can be checked by \"numactl --hardware\". For a different number of numa nodes, a corresponding number of namespaces should be created to assure correct file system paths mapping to numa nodes. Make sure Memkind library installed on every cluster worker node. Compile Memkind based on your system or directly place our pre-built binary of libmemkind.so.0 for x86 64bit CentOS Linux in the /lib64/ directory of each worker node in cluster. The Memkind library depends on libnuma at the runtime, so it must already exist in the worker node system. Build the latest memkind lib from source: git clone -b v1.10.1 https://github.com/memkind/memkind cd memkind ./autogen.sh ./configure make make install For KMem Dax mode, we need to configure PMem as system ram. Kernel 5.1 or above is required to this mode. daxctl migrate-device-model ndctl create-namespace --mode=devdax --map=mem ndctl list daxctl reconfigure-device dax0.0 --mode=system-ram daxctl reconfigure-device dax1.0 --mode=system-ram daxctl reconfigure-device daxX.Y --mode=system-ram Refer Memkind KMem for details. Compiling To build pmem spill, you can run below commands: cd ${PMEM-SPILL} mvn clean package -DskipTests You will find jar files under oap-common/target and oap-spark/target. Configuration To enable rdd cache on Intel Optane PMem, you need add the following configurations to spark-defaults.conf spark.memory.pmem.initial.path [Your Optane PMem paths seperate with comma] spark.memory.pmem.initial.size [Your Optane PMem size in GB] spark.memory.pmem.usable.ratio [from 0 to 1, 0.85 is recommended] spark.yarn.numa.enabled true spark.yarn.numa.num [Your numa node number] spark.memory.pmem.mode [AppDirect | KMemDax] spark.files file://${PATH_TO_PMEM_SPILL_JAR}/pmem-spill-<version>-with-spark-<version>.jar,file://${PATH_TO_PMEM_COMMON_JAR}/pmem-common-<version>-with-spark-<version>.jar spark.executor.extraClassPath ./pmem-spill-<version>-with-spark-<version>.jar:./pmem-common-<version>-with-spark-<version>.jar spark.driver.extraClassPath file://${PATH_TO_PMEM_SPILL_JAR}/pmem-spill-<version>-with-spark-<version>.jar:file://${PATH_TO_PMEM_COMMON_JAR}/pmem-common-<version>-with-spark-<version>.jar Use Optane PMem to cache data There's a new StorageLevel: PMEM_AND_DISK being added to cache data to Optane PMem, at the places you previously cache/persist data to memory, use PMEM_AND_DISK to substitute the previous StorageLevel, data will be cached to Optane PMem. persist(StorageLevel.PMEM_AND_DISK) Run K-means benchmark You can use Hibench to run K-means workload: After you Build Hibench, then follow Run SparkBench documentation. Here are some tips besides this documentation you need to notice. Follow the documentation to configure these 4 files: HiBench/conf/hadoop.conf HiBench/conf/hibench.conf HiBench/conf/spark.conf HiBench/conf/workloads/ml/kmeans.conf Note that you need add hibench.kmeans.storage.level PMEM_AND_DISK to kmeans.conf , which can enable both PMem and Disk to cache data. If you completed OAP-Installation-Guide , you also need add the following configs to spark.conf spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib Then you can run the following 2 commands to run K-means workloads: bin/workloads/ml/kmeans/prepare/prepare.sh bin/workloads/ml/kmeans/spark/run.sh Then you can find the log as below: patching args= Parsing conf: /home/wh/HiBench/conf/hadoop.conf Parsing conf: /home/wh/HiBench/conf/hibench.conf Parsing conf: /home/wh/HiBench/conf/spark.conf Parsing conf: /home/wh/HiBench/conf/workloads/ml/kmeans.conf probe sleep jar: /opt/Beaver/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar start ScalaSparkKmeans bench hdfs rm -r: /opt/Beaver/hadoop/bin/hadoop --config /opt/Beaver/hadoop/etc/hadoop fs -rm -r -skipTrash hdfs://vsr219:9000/HiBench/Kmeans/Output rm: `hdfs://vsr219:9000/HiBench/Kmeans/Output': No such file or directory hdfs du -s: /opt/Beaver/hadoop/bin/hadoop --config /opt/Beaver/hadoop/etc/hadoop fs -du -s hdfs://vsr219:9000/HiBench/Kmeans/Input Export env: SPARKBENCH_PROPERTIES_FILES=/home/wh/HiBench/report/kmeans/spark/conf/sparkbench/sparkbench.conf Export env: HADOOP_CONF_DIR=/opt/Beaver/hadoop/etc/hadoop Submit Spark job: /opt/Beaver/spark/bin/spark-submit --properties-file /home/wh/HiBench/report/kmeans/spark/conf/sparkbench/spark.conf --class com.intel.hibench.sparkbench.ml.DenseKMeans --master yarn-client --num-executors 2 --executor-cores 45 --executor-memory 100g /home/wh/HiBench/sparkbench/assembly/target/sparkbench-assembly-8.0-SNAPSHOT-dist.jar -k 10 --numIterations 5 --storageLevel PMEM_AND_DISK hdfs://vsr219:9000/HiBench/Kmeans/Input/samples 20/07/03 09:07:49 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-43459116-f4e3-4fe1-bb29-9bca0afa5286 finish ScalaSparkKmeans bench Open the Spark History Web UI and go to the Storage tab page to verify the cache metrics. Limitations For the scenario that data will exceed the block cache capacity. Memkind 1.9.0 and kernel 4.18 is recommended to avoid the unexpected issue. How to contribute Currently, PMem Spill packages includes all Spark changed files. MemoryMode.java MemoryManager.scala StorageMemoryPool.scala UnifiedMemoryManager.scala MemoryStore.scala BlockManager.scala StorageLevel.scala TestMemoryManager Please make sure your code change in above source code will not break current function. The files from this package should avoid depending on other OAP module except PMem-Common.","title":"PMem Spill"},{"location":"PMemSpill/#pmem-spill","text":"","title":"PMem Spill"},{"location":"PMemSpill/#contents","text":"Introduction User Guide","title":"Contents"},{"location":"PMemSpill/#introduction","text":"PMem Spill supports RDD Cache with Optane PMem. Spark has various storage levels serving for different purposes including memory and disk. PMem storage level is added to support a new tier for storage level besides memory and disk. Using PMem library to access Optane PMem can help to avoid the overhead from disk. Large capacity and high I/O performance of PMem shows better performance than tied DRAM and disk solution under the same cost.","title":"Introduction"},{"location":"PMemSpill/#user-guide","text":"","title":"User Guide"},{"location":"PMemSpill/#installation","text":"We have provided a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. If you have finished OAP-Installation-Guide , you needn't compile and install Memkind, and you can find compiled OAP jars in $HOME/miniconda2/envs/oapenv/oap_jars .","title":"Installation"},{"location":"PMemSpill/#prerequisites","text":"The following are required to configure OAP to use PMem cache in AppDirect mode. - PMem hardware is successfully deployed on each node in cluster. - Directories exposing PMem hardware on each socket. For example, on a two socket system the mounted PMem directories should appear as /mnt/pmem0 and /mnt/pmem1 . Correctly installed PMem must be formatted and mounted on every cluster worker node. // use ipmctl command to show topology and dimm info of PMem ipmctl show -topology ipmctl show -dimm // provision PMem in app direct mode ipmctl create -goal PersistentMemoryType=AppDirect // reboot system to make configuration take affect reboot // check capacity provisioned for app direct mode(AppDirectCapacity) ipmctl show -memoryresources // show the PMem region information ipmctl show -region // create namespace based on the region, multi namespaces can be created on a single region ndctl create-namespace -m fsdax -r region0 ndctl create-namespace -m fsdax -r region1 // show the created namespaces fdisk -l // create and mount file system echo y | mkfs.ext4 /dev/pmem0 echo y | mkfs.ext4 /dev/pmem1 mkdir -p /mnt/pmem0 mkdir -p /mnt/pmem1 mount -o dax /dev/pmem0 /mnt/pmem0 mount -o dax /dev/pmem1 /mnt/pmem1 In this case file systems are generated for 2 numa nodes, which can be checked by \"numactl --hardware\". For a different number of numa nodes, a corresponding number of namespaces should be created to assure correct file system paths mapping to numa nodes. Make sure Memkind library installed on every cluster worker node. Compile Memkind based on your system or directly place our pre-built binary of libmemkind.so.0 for x86 64bit CentOS Linux in the /lib64/ directory of each worker node in cluster. The Memkind library depends on libnuma at the runtime, so it must already exist in the worker node system. Build the latest memkind lib from source: git clone -b v1.10.1 https://github.com/memkind/memkind cd memkind ./autogen.sh ./configure make make install For KMem Dax mode, we need to configure PMem as system ram. Kernel 5.1 or above is required to this mode. daxctl migrate-device-model ndctl create-namespace --mode=devdax --map=mem ndctl list daxctl reconfigure-device dax0.0 --mode=system-ram daxctl reconfigure-device dax1.0 --mode=system-ram daxctl reconfigure-device daxX.Y --mode=system-ram Refer Memkind KMem for details.","title":"Prerequisites"},{"location":"PMemSpill/#compiling","text":"To build pmem spill, you can run below commands: cd ${PMEM-SPILL} mvn clean package -DskipTests You will find jar files under oap-common/target and oap-spark/target.","title":"Compiling"},{"location":"PMemSpill/#configuration","text":"To enable rdd cache on Intel Optane PMem, you need add the following configurations to spark-defaults.conf spark.memory.pmem.initial.path [Your Optane PMem paths seperate with comma] spark.memory.pmem.initial.size [Your Optane PMem size in GB] spark.memory.pmem.usable.ratio [from 0 to 1, 0.85 is recommended] spark.yarn.numa.enabled true spark.yarn.numa.num [Your numa node number] spark.memory.pmem.mode [AppDirect | KMemDax] spark.files file://${PATH_TO_PMEM_SPILL_JAR}/pmem-spill-<version>-with-spark-<version>.jar,file://${PATH_TO_PMEM_COMMON_JAR}/pmem-common-<version>-with-spark-<version>.jar spark.executor.extraClassPath ./pmem-spill-<version>-with-spark-<version>.jar:./pmem-common-<version>-with-spark-<version>.jar spark.driver.extraClassPath file://${PATH_TO_PMEM_SPILL_JAR}/pmem-spill-<version>-with-spark-<version>.jar:file://${PATH_TO_PMEM_COMMON_JAR}/pmem-common-<version>-with-spark-<version>.jar","title":"Configuration"},{"location":"PMemSpill/#use-optane-pmem-to-cache-data","text":"There's a new StorageLevel: PMEM_AND_DISK being added to cache data to Optane PMem, at the places you previously cache/persist data to memory, use PMEM_AND_DISK to substitute the previous StorageLevel, data will be cached to Optane PMem. persist(StorageLevel.PMEM_AND_DISK)","title":"Use Optane PMem to cache data"},{"location":"PMemSpill/#run-k-means-benchmark","text":"You can use Hibench to run K-means workload: After you Build Hibench, then follow Run SparkBench documentation. Here are some tips besides this documentation you need to notice. Follow the documentation to configure these 4 files: HiBench/conf/hadoop.conf HiBench/conf/hibench.conf HiBench/conf/spark.conf HiBench/conf/workloads/ml/kmeans.conf Note that you need add hibench.kmeans.storage.level PMEM_AND_DISK to kmeans.conf , which can enable both PMem and Disk to cache data. If you completed OAP-Installation-Guide , you also need add the following configs to spark.conf spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib Then you can run the following 2 commands to run K-means workloads: bin/workloads/ml/kmeans/prepare/prepare.sh bin/workloads/ml/kmeans/spark/run.sh Then you can find the log as below: patching args= Parsing conf: /home/wh/HiBench/conf/hadoop.conf Parsing conf: /home/wh/HiBench/conf/hibench.conf Parsing conf: /home/wh/HiBench/conf/spark.conf Parsing conf: /home/wh/HiBench/conf/workloads/ml/kmeans.conf probe sleep jar: /opt/Beaver/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar start ScalaSparkKmeans bench hdfs rm -r: /opt/Beaver/hadoop/bin/hadoop --config /opt/Beaver/hadoop/etc/hadoop fs -rm -r -skipTrash hdfs://vsr219:9000/HiBench/Kmeans/Output rm: `hdfs://vsr219:9000/HiBench/Kmeans/Output': No such file or directory hdfs du -s: /opt/Beaver/hadoop/bin/hadoop --config /opt/Beaver/hadoop/etc/hadoop fs -du -s hdfs://vsr219:9000/HiBench/Kmeans/Input Export env: SPARKBENCH_PROPERTIES_FILES=/home/wh/HiBench/report/kmeans/spark/conf/sparkbench/sparkbench.conf Export env: HADOOP_CONF_DIR=/opt/Beaver/hadoop/etc/hadoop Submit Spark job: /opt/Beaver/spark/bin/spark-submit --properties-file /home/wh/HiBench/report/kmeans/spark/conf/sparkbench/spark.conf --class com.intel.hibench.sparkbench.ml.DenseKMeans --master yarn-client --num-executors 2 --executor-cores 45 --executor-memory 100g /home/wh/HiBench/sparkbench/assembly/target/sparkbench-assembly-8.0-SNAPSHOT-dist.jar -k 10 --numIterations 5 --storageLevel PMEM_AND_DISK hdfs://vsr219:9000/HiBench/Kmeans/Input/samples 20/07/03 09:07:49 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-43459116-f4e3-4fe1-bb29-9bca0afa5286 finish ScalaSparkKmeans bench Open the Spark History Web UI and go to the Storage tab page to verify the cache metrics.","title":"Run K-means benchmark"},{"location":"PMemSpill/#limitations","text":"For the scenario that data will exceed the block cache capacity. Memkind 1.9.0 and kernel 4.18 is recommended to avoid the unexpected issue.","title":"Limitations"},{"location":"PMemSpill/#how-to-contribute","text":"Currently, PMem Spill packages includes all Spark changed files. MemoryMode.java MemoryManager.scala StorageMemoryPool.scala UnifiedMemoryManager.scala MemoryStore.scala BlockManager.scala StorageLevel.scala TestMemoryManager Please make sure your code change in above source code will not break current function. The files from this package should avoid depending on other OAP module except PMem-Common.","title":"How to contribute"},{"location":"PMemSpill/OAP-Developer-Guide/","text":"OAP Developer Guide This document contains the instructions & scripts on installing necessary dependencies and building OAP. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Arrow Data Source Native SQL Engine Building OAP Prerequisites for Building OAP is built with Apache Maven and Oracle Java 8, and mainly required tools to install on your cluster are listed below. Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. We provide scripts below to help automatically install dependencies above except RDMA , need change to root account, run: # git clone -b <tag-version> # cd oap-tools # sh dev/install-compile-time-dependencies.sh Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven Building To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building Specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache","title":"OAP Developer Guide"},{"location":"PMemSpill/OAP-Developer-Guide/#oap-developer-guide","text":"This document contains the instructions & scripts on installing necessary dependencies and building OAP. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Arrow Data Source Native SQL Engine","title":"OAP Developer Guide"},{"location":"PMemSpill/OAP-Developer-Guide/#building-oap","text":"","title":"Building OAP"},{"location":"PMemSpill/OAP-Developer-Guide/#prerequisites-for-building","text":"OAP is built with Apache Maven and Oracle Java 8, and mainly required tools to install on your cluster are listed below. Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. We provide scripts below to help automatically install dependencies above except RDMA , need change to root account, run: # git clone -b <tag-version> # cd oap-tools # sh dev/install-compile-time-dependencies.sh Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven","title":"Prerequisites for Building"},{"location":"PMemSpill/OAP-Developer-Guide/#building","text":"To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building Specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache","title":"Building"},{"location":"PMemSpill/OAP-Installation-Guide/","text":"OAP Installation Guide This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine. Contents Prerequisites Installing OAP Configuration Prerequisites OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly. Installing OAP Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Create a conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.0.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Extra Steps for Shuffle Remote PMem Extension If you use one of OAP features -- PMmem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details. Configuration Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar And then you can follow the corresponding feature documents for more details to use them.","title":"OAP Installation Guide"},{"location":"PMemSpill/OAP-Installation-Guide/#oap-installation-guide","text":"This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine.","title":"OAP Installation Guide"},{"location":"PMemSpill/OAP-Installation-Guide/#contents","text":"Prerequisites Installing OAP Configuration","title":"Contents"},{"location":"PMemSpill/OAP-Installation-Guide/#prerequisites","text":"OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly.","title":"Prerequisites"},{"location":"PMemSpill/OAP-Installation-Guide/#installing-oap","text":"Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Create a conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.0.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars","title":"Installing OAP"},{"location":"PMemSpill/OAP-Installation-Guide/#extra-steps-for-shuffle-remote-pmem-extension","text":"If you use one of OAP features -- PMmem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details.","title":"Extra Steps for Shuffle Remote PMem Extension"},{"location":"PMemSpill/OAP-Installation-Guide/#configuration","text":"Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar And then you can follow the corresponding feature documents for more details to use them.","title":"Configuration"},{"location":"PMemSpill/User-Guide/","text":"PMem Spill Contents Introduction User Guide Introduction PMem Spill supports RDD Cache with Optane PMem. Spark has various storage levels serving for different purposes including memory and disk. PMem storage level is added to support a new tier for storage level besides memory and disk. Using PMem library to access Optane PMem can help to avoid the overhead from disk. Large capacity and high I/O performance of PMem shows better performance than tied DRAM and disk solution under the same cost. User Guide Installation We have provided a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. If you have finished OAP-Installation-Guide , you needn't compile and install Memkind, and you can find compiled OAP jars in $HOME/miniconda2/envs/oapenv/oap_jars . Prerequisites The following are required to configure OAP to use PMem cache in AppDirect mode. - PMem hardware is successfully deployed on each node in cluster. - Directories exposing PMem hardware on each socket. For example, on a two socket system the mounted PMem directories should appear as /mnt/pmem0 and /mnt/pmem1 . Correctly installed PMem must be formatted and mounted on every cluster worker node. // use ipmctl command to show topology and dimm info of PMem ipmctl show -topology ipmctl show -dimm // provision PMem in app direct mode ipmctl create -goal PersistentMemoryType=AppDirect // reboot system to make configuration take affect reboot // check capacity provisioned for app direct mode(AppDirectCapacity) ipmctl show -memoryresources // show the PMem region information ipmctl show -region // create namespace based on the region, multi namespaces can be created on a single region ndctl create-namespace -m fsdax -r region0 ndctl create-namespace -m fsdax -r region1 // show the created namespaces fdisk -l // create and mount file system echo y | mkfs.ext4 /dev/pmem0 echo y | mkfs.ext4 /dev/pmem1 mkdir -p /mnt/pmem0 mkdir -p /mnt/pmem1 mount -o dax /dev/pmem0 /mnt/pmem0 mount -o dax /dev/pmem1 /mnt/pmem1 In this case file systems are generated for 2 numa nodes, which can be checked by \"numactl --hardware\". For a different number of numa nodes, a corresponding number of namespaces should be created to assure correct file system paths mapping to numa nodes. Make sure Memkind library installed on every cluster worker node. Compile Memkind based on your system or directly place our pre-built binary of libmemkind.so.0 for x86 64bit CentOS Linux in the /lib64/ directory of each worker node in cluster. The Memkind library depends on libnuma at the runtime, so it must already exist in the worker node system. Build the latest memkind lib from source: git clone -b v1.10.1 https://github.com/memkind/memkind cd memkind ./autogen.sh ./configure make make install For KMem Dax mode, we need to configure PMem as system ram. Kernel 5.1 or above is required to this mode. daxctl migrate-device-model ndctl create-namespace --mode=devdax --map=mem ndctl list daxctl reconfigure-device dax0.0 --mode=system-ram daxctl reconfigure-device dax1.0 --mode=system-ram daxctl reconfigure-device daxX.Y --mode=system-ram Refer Memkind KMem for details. Compiling To build pmem spill, you can run below commands: cd ${PMEM-SPILL} mvn clean package -DskipTests You will find jar files under oap-common/target and oap-spark/target. Configuration To enable rdd cache on Intel Optane PMem, you need add the following configurations to spark-defaults.conf spark.memory.pmem.initial.path [Your Optane PMem paths seperate with comma] spark.memory.pmem.initial.size [Your Optane PMem size in GB] spark.memory.pmem.usable.ratio [from 0 to 1, 0.85 is recommended] spark.yarn.numa.enabled true spark.yarn.numa.num [Your numa node number] spark.memory.pmem.mode [AppDirect | KMemDax] spark.files file://${PATH_TO_PMEM_SPILL_JAR}/pmem-spill-<version>-with-spark-<version>.jar,file://${PATH_TO_PMEM_COMMON_JAR}/pmem-common-<version>-with-spark-<version>.jar spark.executor.extraClassPath ./pmem-spill-<version>-with-spark-<version>.jar:./pmem-common-<version>-with-spark-<version>.jar spark.driver.extraClassPath file://${PATH_TO_PMEM_SPILL_JAR}/pmem-spill-<version>-with-spark-<version>.jar:file://${PATH_TO_PMEM_COMMON_JAR}/pmem-common-<version>-with-spark-<version>.jar Use Optane PMem to cache data There's a new StorageLevel: PMEM_AND_DISK being added to cache data to Optane PMem, at the places you previously cache/persist data to memory, use PMEM_AND_DISK to substitute the previous StorageLevel, data will be cached to Optane PMem. persist(StorageLevel.PMEM_AND_DISK) Run K-means benchmark You can use Hibench to run K-means workload: After you Build Hibench, then follow Run SparkBench documentation. Here are some tips besides this documentation you need to notice. Follow the documentation to configure these 4 files: HiBench/conf/hadoop.conf HiBench/conf/hibench.conf HiBench/conf/spark.conf HiBench/conf/workloads/ml/kmeans.conf Note that you need add hibench.kmeans.storage.level PMEM_AND_DISK to kmeans.conf , which can enable both PMem and Disk to cache data. If you completed OAP-Installation-Guide , you also need add the following configs to spark.conf spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib Then you can run the following 2 commands to run K-means workloads: bin/workloads/ml/kmeans/prepare/prepare.sh bin/workloads/ml/kmeans/spark/run.sh Then you can find the log as below: patching args= Parsing conf: /home/wh/HiBench/conf/hadoop.conf Parsing conf: /home/wh/HiBench/conf/hibench.conf Parsing conf: /home/wh/HiBench/conf/spark.conf Parsing conf: /home/wh/HiBench/conf/workloads/ml/kmeans.conf probe sleep jar: /opt/Beaver/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar start ScalaSparkKmeans bench hdfs rm -r: /opt/Beaver/hadoop/bin/hadoop --config /opt/Beaver/hadoop/etc/hadoop fs -rm -r -skipTrash hdfs://vsr219:9000/HiBench/Kmeans/Output rm: `hdfs://vsr219:9000/HiBench/Kmeans/Output': No such file or directory hdfs du -s: /opt/Beaver/hadoop/bin/hadoop --config /opt/Beaver/hadoop/etc/hadoop fs -du -s hdfs://vsr219:9000/HiBench/Kmeans/Input Export env: SPARKBENCH_PROPERTIES_FILES=/home/wh/HiBench/report/kmeans/spark/conf/sparkbench/sparkbench.conf Export env: HADOOP_CONF_DIR=/opt/Beaver/hadoop/etc/hadoop Submit Spark job: /opt/Beaver/spark/bin/spark-submit --properties-file /home/wh/HiBench/report/kmeans/spark/conf/sparkbench/spark.conf --class com.intel.hibench.sparkbench.ml.DenseKMeans --master yarn-client --num-executors 2 --executor-cores 45 --executor-memory 100g /home/wh/HiBench/sparkbench/assembly/target/sparkbench-assembly-8.0-SNAPSHOT-dist.jar -k 10 --numIterations 5 --storageLevel PMEM_AND_DISK hdfs://vsr219:9000/HiBench/Kmeans/Input/samples 20/07/03 09:07:49 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-43459116-f4e3-4fe1-bb29-9bca0afa5286 finish ScalaSparkKmeans bench Open the Spark History Web UI and go to the Storage tab page to verify the cache metrics. Limitations For the scenario that data will exceed the block cache capacity. Memkind 1.9.0 and kernel 4.18 is recommended to avoid the unexpected issue. How to contribute Currently, PMem Spill packages includes all Spark changed files. MemoryMode.java MemoryManager.scala StorageMemoryPool.scala UnifiedMemoryManager.scala MemoryStore.scala BlockManager.scala StorageLevel.scala TestMemoryManager Please make sure your code change in above source code will not break current function. The files from this package should avoid depending on other OAP module except PMem-Common.","title":"User Guide"},{"location":"PMemSpill/User-Guide/#pmem-spill","text":"","title":"PMem Spill"},{"location":"PMemSpill/User-Guide/#contents","text":"Introduction User Guide","title":"Contents"},{"location":"PMemSpill/User-Guide/#introduction","text":"PMem Spill supports RDD Cache with Optane PMem. Spark has various storage levels serving for different purposes including memory and disk. PMem storage level is added to support a new tier for storage level besides memory and disk. Using PMem library to access Optane PMem can help to avoid the overhead from disk. Large capacity and high I/O performance of PMem shows better performance than tied DRAM and disk solution under the same cost.","title":"Introduction"},{"location":"PMemSpill/User-Guide/#user-guide","text":"","title":"User Guide"},{"location":"PMemSpill/User-Guide/#installation","text":"We have provided a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. If you have finished OAP-Installation-Guide , you needn't compile and install Memkind, and you can find compiled OAP jars in $HOME/miniconda2/envs/oapenv/oap_jars .","title":"Installation"},{"location":"PMemSpill/User-Guide/#prerequisites","text":"The following are required to configure OAP to use PMem cache in AppDirect mode. - PMem hardware is successfully deployed on each node in cluster. - Directories exposing PMem hardware on each socket. For example, on a two socket system the mounted PMem directories should appear as /mnt/pmem0 and /mnt/pmem1 . Correctly installed PMem must be formatted and mounted on every cluster worker node. // use ipmctl command to show topology and dimm info of PMem ipmctl show -topology ipmctl show -dimm // provision PMem in app direct mode ipmctl create -goal PersistentMemoryType=AppDirect // reboot system to make configuration take affect reboot // check capacity provisioned for app direct mode(AppDirectCapacity) ipmctl show -memoryresources // show the PMem region information ipmctl show -region // create namespace based on the region, multi namespaces can be created on a single region ndctl create-namespace -m fsdax -r region0 ndctl create-namespace -m fsdax -r region1 // show the created namespaces fdisk -l // create and mount file system echo y | mkfs.ext4 /dev/pmem0 echo y | mkfs.ext4 /dev/pmem1 mkdir -p /mnt/pmem0 mkdir -p /mnt/pmem1 mount -o dax /dev/pmem0 /mnt/pmem0 mount -o dax /dev/pmem1 /mnt/pmem1 In this case file systems are generated for 2 numa nodes, which can be checked by \"numactl --hardware\". For a different number of numa nodes, a corresponding number of namespaces should be created to assure correct file system paths mapping to numa nodes. Make sure Memkind library installed on every cluster worker node. Compile Memkind based on your system or directly place our pre-built binary of libmemkind.so.0 for x86 64bit CentOS Linux in the /lib64/ directory of each worker node in cluster. The Memkind library depends on libnuma at the runtime, so it must already exist in the worker node system. Build the latest memkind lib from source: git clone -b v1.10.1 https://github.com/memkind/memkind cd memkind ./autogen.sh ./configure make make install For KMem Dax mode, we need to configure PMem as system ram. Kernel 5.1 or above is required to this mode. daxctl migrate-device-model ndctl create-namespace --mode=devdax --map=mem ndctl list daxctl reconfigure-device dax0.0 --mode=system-ram daxctl reconfigure-device dax1.0 --mode=system-ram daxctl reconfigure-device daxX.Y --mode=system-ram Refer Memkind KMem for details.","title":"Prerequisites"},{"location":"PMemSpill/User-Guide/#compiling","text":"To build pmem spill, you can run below commands: cd ${PMEM-SPILL} mvn clean package -DskipTests You will find jar files under oap-common/target and oap-spark/target.","title":"Compiling"},{"location":"PMemSpill/User-Guide/#configuration","text":"To enable rdd cache on Intel Optane PMem, you need add the following configurations to spark-defaults.conf spark.memory.pmem.initial.path [Your Optane PMem paths seperate with comma] spark.memory.pmem.initial.size [Your Optane PMem size in GB] spark.memory.pmem.usable.ratio [from 0 to 1, 0.85 is recommended] spark.yarn.numa.enabled true spark.yarn.numa.num [Your numa node number] spark.memory.pmem.mode [AppDirect | KMemDax] spark.files file://${PATH_TO_PMEM_SPILL_JAR}/pmem-spill-<version>-with-spark-<version>.jar,file://${PATH_TO_PMEM_COMMON_JAR}/pmem-common-<version>-with-spark-<version>.jar spark.executor.extraClassPath ./pmem-spill-<version>-with-spark-<version>.jar:./pmem-common-<version>-with-spark-<version>.jar spark.driver.extraClassPath file://${PATH_TO_PMEM_SPILL_JAR}/pmem-spill-<version>-with-spark-<version>.jar:file://${PATH_TO_PMEM_COMMON_JAR}/pmem-common-<version>-with-spark-<version>.jar","title":"Configuration"},{"location":"PMemSpill/User-Guide/#use-optane-pmem-to-cache-data","text":"There's a new StorageLevel: PMEM_AND_DISK being added to cache data to Optane PMem, at the places you previously cache/persist data to memory, use PMEM_AND_DISK to substitute the previous StorageLevel, data will be cached to Optane PMem. persist(StorageLevel.PMEM_AND_DISK)","title":"Use Optane PMem to cache data"},{"location":"PMemSpill/User-Guide/#run-k-means-benchmark","text":"You can use Hibench to run K-means workload: After you Build Hibench, then follow Run SparkBench documentation. Here are some tips besides this documentation you need to notice. Follow the documentation to configure these 4 files: HiBench/conf/hadoop.conf HiBench/conf/hibench.conf HiBench/conf/spark.conf HiBench/conf/workloads/ml/kmeans.conf Note that you need add hibench.kmeans.storage.level PMEM_AND_DISK to kmeans.conf , which can enable both PMem and Disk to cache data. If you completed OAP-Installation-Guide , you also need add the following configs to spark.conf spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib Then you can run the following 2 commands to run K-means workloads: bin/workloads/ml/kmeans/prepare/prepare.sh bin/workloads/ml/kmeans/spark/run.sh Then you can find the log as below: patching args= Parsing conf: /home/wh/HiBench/conf/hadoop.conf Parsing conf: /home/wh/HiBench/conf/hibench.conf Parsing conf: /home/wh/HiBench/conf/spark.conf Parsing conf: /home/wh/HiBench/conf/workloads/ml/kmeans.conf probe sleep jar: /opt/Beaver/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar start ScalaSparkKmeans bench hdfs rm -r: /opt/Beaver/hadoop/bin/hadoop --config /opt/Beaver/hadoop/etc/hadoop fs -rm -r -skipTrash hdfs://vsr219:9000/HiBench/Kmeans/Output rm: `hdfs://vsr219:9000/HiBench/Kmeans/Output': No such file or directory hdfs du -s: /opt/Beaver/hadoop/bin/hadoop --config /opt/Beaver/hadoop/etc/hadoop fs -du -s hdfs://vsr219:9000/HiBench/Kmeans/Input Export env: SPARKBENCH_PROPERTIES_FILES=/home/wh/HiBench/report/kmeans/spark/conf/sparkbench/sparkbench.conf Export env: HADOOP_CONF_DIR=/opt/Beaver/hadoop/etc/hadoop Submit Spark job: /opt/Beaver/spark/bin/spark-submit --properties-file /home/wh/HiBench/report/kmeans/spark/conf/sparkbench/spark.conf --class com.intel.hibench.sparkbench.ml.DenseKMeans --master yarn-client --num-executors 2 --executor-cores 45 --executor-memory 100g /home/wh/HiBench/sparkbench/assembly/target/sparkbench-assembly-8.0-SNAPSHOT-dist.jar -k 10 --numIterations 5 --storageLevel PMEM_AND_DISK hdfs://vsr219:9000/HiBench/Kmeans/Input/samples 20/07/03 09:07:49 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-43459116-f4e3-4fe1-bb29-9bca0afa5286 finish ScalaSparkKmeans bench Open the Spark History Web UI and go to the Storage tab page to verify the cache metrics.","title":"Run K-means benchmark"},{"location":"PMemSpill/User-Guide/#limitations","text":"For the scenario that data will exceed the block cache capacity. Memkind 1.9.0 and kernel 4.18 is recommended to avoid the unexpected issue.","title":"Limitations"},{"location":"PMemSpill/User-Guide/#how-to-contribute","text":"Currently, PMem Spill packages includes all Spark changed files. MemoryMode.java MemoryManager.scala StorageMemoryPool.scala UnifiedMemoryManager.scala MemoryStore.scala BlockManager.scala StorageLevel.scala TestMemoryManager Please make sure your code change in above source code will not break current function. The files from this package should avoid depending on other OAP module except PMem-Common.","title":"How to contribute"},{"location":"RemoteShuffle/","text":"Remote Shuffle Contents Introduction User Guide Introduction Remote Shuffle is a Spark* ShuffleManager plugin, shuffling data through a remote Hadoop-compatible file system, as opposed to vanilla Spark's local-disks. This is an essential part of enabling Spark on disaggregated compute and storage architecture. Installation We have provided a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. If you have finished OAP-Installation-Guide , you can find compiled OAP jars in $HOME/miniconda2/envs/oapenv/oap_jars/ . Developer Guide Build and Deploy We have provided a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. If you have finished OAP-Installation-Guide , you can find compiled remote shuffle jars under $HOME/miniconda2/envs/oapenv/oap_jars . Then just skip this section and jump to User Guide . Build this module using the following command in OAP/oap-shuffle/remote-shuffle folder. This file needs to be deployed on every compute node that runs Spark. Manually place it on all nodes or let resource manager do the work. mvn -DskipTests clean package User Guide Enable Remote Shuffle Add the .jar files to the classpath of Spark driver and executors: Put the following configurations in spark-defaults.conf or Spark submit command line arguments. Note: For DAOS users, DAOS Hadoop/Java API jars should also be included in the classpath as we leverage DAOS Hadoop filesystem. spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/remote-shuffle-<version>.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/remote-shuffle-<version>.jar Enable the remote shuffle manager and specify the Hadoop storage system URI holding shuffle data. spark.shuffle.manager org.apache.spark.shuffle.remote.RemoteShuffleManager spark.shuffle.remote.storageMasterUri daos://default:1 # Or hdfs://namenode:port, file:///my/shuffle/dir Configurations Configurations and tuning parameters that change the behavior of remote shuffle. Most of them should work well under default values. Shuffle Root Directory This is to configure the root directory holding remote shuffle files. For each Spark application, a directory named after application ID is created under this root directory. spark.shuffle.remote.filesRootDirectory /shuffle Index Cache Size This is to configure the cache size for shuffle index files per executor. Shuffle data includes data files and index files. An index file is small but will be read many (the number of reducers) times. On a large scale, constantly reading these small index files from Hadoop Filesystem implementation(i.e. HDFS) is going to cause much overhead and latency. In addition, the shuffle files\u2019 transfer completely relies on the network between compute nodes and storage nodes. But the network inside compute nodes are not fully utilized. The index cache can eliminate the overhead of reading index files from storage cluster multiple times. By enabling index file cache, a reduce task fetches them from the remote executors who write them instead of reading from storage. If the remote executor doesn\u2019t have a desired index file in its cache, it will read the file from storage and cache it locally. The feature can also be disabled by setting the value to zero. spark.shuffle.remote.index.cache.size 30m Number of Threads Reading Data Files This is one of the parameters influencing shuffle read performance. It is to determine number of threads per executor reading shuffle data files from storage. spark.shuffle.remote.numReadThreads 5 Number of Threads Transitioning Index Files (when index cache is enabled) This is one of the parameters influencing shuffle read performance. It is to determine the number of client and server threads that transmit index information from another executor\u2019s cache. It is only valid when the index cache feature is enabled. spark.shuffle.remote.numIndexReadThreads 3 Bypass-merge-sort Threshold This threshold is used to decide using bypass-merge(hash-based) shuffle or not. By default we disable(by setting it to -1) hash-based shuffle writer in remote shuffle, because when memory is relatively sufficient, sort-based shuffle writer is often more efficient than the hash-based one. Hash-based shuffle writer entails a merging process, performing 3x I/Os than total shuffle size: 1 time for read I/Os and 2 times for write I/Os, this can be an even larger overhead under remote shuffle: the 3x shuffle size is gone through network, arriving at a remote storage system. spark.shuffle.remote.bypassMergeThreshold -1 Configurations fetching port for HDFS When the backend storage is HDFS, we contact http://$host:$port/conf to fetch configurations. They were not locally loaded because we assume absence of local storage. spark.shuffle.remote.hdfs.storageMasterUIPort 50070 Inherited Spark Shuffle Configurations These configurations are inherited from upstream Spark, they are still supported in remote shuffle. More explanations can be found in Spark core docs and Spark SQL docs . spark.reducer.maxSizeInFlight spark.reducer.maxReqsInFlight spark.reducer.maxBlocksInFlightPerAddress spark.shuffle.compress spark.shuffle.file.buffer spark.shuffle.io.maxRetries spark.shuffle.io.numConnectionsPerPeer spark.shuffle.io.preferDirectBufs spark.shuffle.io.retryWait spark.shuffle.io.backLog spark.shuffle.spill.compress spark.shuffle.accurateBlockThreshold spark.sql.shuffle.partitions Deprecated Spark Shuffle Configurations These configurations are deprecated and will not take effect. spark.shuffle.sort.bypassMergeThreshold # Replaced by spark.shuffle.remote.bypassMergeThreshold spark.maxRemoteBlockSizeFetchToMem # As we assume no local disks on compute nodes, shuffle blocks are all fetched to memory spark.shuffle.service.enabled # All following configurations are related to External Shuffle Service. ESS & remote shuffle cannot be enabled at the same time, as this remote shuffle facility takes over almost all functionalities of ESS. spark.shuffle.service.port spark.shuffle.service.index.cache.size spark.shuffle.maxChunksBeingTransferred spark.shuffle.registration.timeout spark.shuffle.registration.maxAttempts Performance Evaluation Tool Leverage this tool to evaluate shuffle write/read performance separately under your specific storage system. This tool starts one Java process with #poolSize number of threads, running the specified remote-shuffle writers/readers in this module. Additional Spark configurations can be put in \"./spark-defaults.conf\" and will be loaded.(and printed as part of the summary for recording) Configuration details: * -h or --help : display help messages * -m or --mappers : the number of mappers, default to 5 * -r or --reducers : the number of reducers, default to 5 * -p or --poolSize : the number task threads in write/read thread pool, similar to spark.executor.cores. e.g. if mappers=15, poolSize=5, it takes 3 rounds to finish this job * -n or --rows : the number of rows per mapper, default to 1000 * -b or --shuffleBlockRawSize : the size of each shuffle block, default to 20000 Bytes * -w or --writer : the type of shuffle writers for benchmark, can be one of general, unsafe and bypassmergesort, default to unsafe * -onlyWrite or --onlyWrite : containing this flag then the benchmark only includes shuffle write stage, default behavior is perform both write & read * -uri or --storageMasterUri : Hadoop-compatible storage Master URI, default to file:// * -d or --dir : Shuffle directory, default /tmp * -l or --log : Log level, default to WARN Sample command: java -cp target/remote-shuffle-0.1-SNAPSHOT-test-jar-with-dependencies.jar org.apache.spark.shuffle.remote.PerformanceEvaluationTool -h Sample output unsafe shuffle writer: raw total size: 123 GB compressed size: 135 GB duration: 88.3 seconds throughput(raw): 1429.06843144412 MB/s throughput(storage): 1570.9931870053674 MB/s number of mappers: 210 number of reducers: 70 block size(raw): 8 MB block size(storage): 9 MB properties: spark.reducer.maxSizeInFlight -> 100m, spark.shuffle.remote.numReadThreads -> 8, spark.shuffle.remote.reducer.maxBlocksInFlightPerAddress -> 3 records per mapper: 70 load size per record:9000000 shuffle storage daos://default:1 shuffle folder: /tmp/shuffle ------------------------------------------------------------------------------------------------------------------------- shuffle reader: raw total size: 123 GB compressed size: 135 GB duration: 49.8 seconds throughput(raw): 2533.665772753123 MB/s throughput(storage): 2785.2911586057153 MB/s number of mappers: 210 number of reducers: 70 block size(raw): 8 MB block size(storage): 9 MB properties: spark.reducer.maxSizeInFlight -> 100m, spark.shuffle.remote.numReadThreads -> 8, spark.shuffle.remote.reducer.maxBlocksInFlightPerAddress -> 3 records per mapper: 70 load size per record:9000000 shuffle storage daos://default:1 shuffle folder: /tmp/shuffle","title":"Remote Shuffle"},{"location":"RemoteShuffle/#remote-shuffle","text":"","title":"Remote Shuffle"},{"location":"RemoteShuffle/#contents","text":"Introduction User Guide","title":"Contents"},{"location":"RemoteShuffle/#introduction","text":"Remote Shuffle is a Spark* ShuffleManager plugin, shuffling data through a remote Hadoop-compatible file system, as opposed to vanilla Spark's local-disks. This is an essential part of enabling Spark on disaggregated compute and storage architecture.","title":"Introduction"},{"location":"RemoteShuffle/#installation","text":"We have provided a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. If you have finished OAP-Installation-Guide , you can find compiled OAP jars in $HOME/miniconda2/envs/oapenv/oap_jars/ .","title":"Installation"},{"location":"RemoteShuffle/#developer-guide","text":"","title":"Developer Guide"},{"location":"RemoteShuffle/#build-and-deploy","text":"We have provided a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. If you have finished OAP-Installation-Guide , you can find compiled remote shuffle jars under $HOME/miniconda2/envs/oapenv/oap_jars . Then just skip this section and jump to User Guide . Build this module using the following command in OAP/oap-shuffle/remote-shuffle folder. This file needs to be deployed on every compute node that runs Spark. Manually place it on all nodes or let resource manager do the work. mvn -DskipTests clean package","title":"Build and Deploy"},{"location":"RemoteShuffle/#user-guide","text":"","title":"User Guide"},{"location":"RemoteShuffle/#enable-remote-shuffle","text":"Add the .jar files to the classpath of Spark driver and executors: Put the following configurations in spark-defaults.conf or Spark submit command line arguments. Note: For DAOS users, DAOS Hadoop/Java API jars should also be included in the classpath as we leverage DAOS Hadoop filesystem. spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/remote-shuffle-<version>.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/remote-shuffle-<version>.jar Enable the remote shuffle manager and specify the Hadoop storage system URI holding shuffle data. spark.shuffle.manager org.apache.spark.shuffle.remote.RemoteShuffleManager spark.shuffle.remote.storageMasterUri daos://default:1 # Or hdfs://namenode:port, file:///my/shuffle/dir","title":"Enable Remote Shuffle"},{"location":"RemoteShuffle/#configurations","text":"Configurations and tuning parameters that change the behavior of remote shuffle. Most of them should work well under default values.","title":"Configurations"},{"location":"RemoteShuffle/#shuffle-root-directory","text":"This is to configure the root directory holding remote shuffle files. For each Spark application, a directory named after application ID is created under this root directory. spark.shuffle.remote.filesRootDirectory /shuffle","title":"Shuffle Root Directory"},{"location":"RemoteShuffle/#index-cache-size","text":"This is to configure the cache size for shuffle index files per executor. Shuffle data includes data files and index files. An index file is small but will be read many (the number of reducers) times. On a large scale, constantly reading these small index files from Hadoop Filesystem implementation(i.e. HDFS) is going to cause much overhead and latency. In addition, the shuffle files\u2019 transfer completely relies on the network between compute nodes and storage nodes. But the network inside compute nodes are not fully utilized. The index cache can eliminate the overhead of reading index files from storage cluster multiple times. By enabling index file cache, a reduce task fetches them from the remote executors who write them instead of reading from storage. If the remote executor doesn\u2019t have a desired index file in its cache, it will read the file from storage and cache it locally. The feature can also be disabled by setting the value to zero. spark.shuffle.remote.index.cache.size 30m","title":"Index Cache Size"},{"location":"RemoteShuffle/#number-of-threads-reading-data-files","text":"This is one of the parameters influencing shuffle read performance. It is to determine number of threads per executor reading shuffle data files from storage. spark.shuffle.remote.numReadThreads 5","title":"Number of Threads Reading Data Files"},{"location":"RemoteShuffle/#number-of-threads-transitioning-index-files-when-index-cache-is-enabled","text":"This is one of the parameters influencing shuffle read performance. It is to determine the number of client and server threads that transmit index information from another executor\u2019s cache. It is only valid when the index cache feature is enabled. spark.shuffle.remote.numIndexReadThreads 3","title":"Number of Threads Transitioning Index Files (when index cache is enabled)"},{"location":"RemoteShuffle/#bypass-merge-sort-threshold","text":"This threshold is used to decide using bypass-merge(hash-based) shuffle or not. By default we disable(by setting it to -1) hash-based shuffle writer in remote shuffle, because when memory is relatively sufficient, sort-based shuffle writer is often more efficient than the hash-based one. Hash-based shuffle writer entails a merging process, performing 3x I/Os than total shuffle size: 1 time for read I/Os and 2 times for write I/Os, this can be an even larger overhead under remote shuffle: the 3x shuffle size is gone through network, arriving at a remote storage system. spark.shuffle.remote.bypassMergeThreshold -1","title":"Bypass-merge-sort Threshold"},{"location":"RemoteShuffle/#configurations-fetching-port-for-hdfs","text":"When the backend storage is HDFS, we contact http://$host:$port/conf to fetch configurations. They were not locally loaded because we assume absence of local storage. spark.shuffle.remote.hdfs.storageMasterUIPort 50070","title":"Configurations fetching port for HDFS"},{"location":"RemoteShuffle/#inherited-spark-shuffle-configurations","text":"These configurations are inherited from upstream Spark, they are still supported in remote shuffle. More explanations can be found in Spark core docs and Spark SQL docs . spark.reducer.maxSizeInFlight spark.reducer.maxReqsInFlight spark.reducer.maxBlocksInFlightPerAddress spark.shuffle.compress spark.shuffle.file.buffer spark.shuffle.io.maxRetries spark.shuffle.io.numConnectionsPerPeer spark.shuffle.io.preferDirectBufs spark.shuffle.io.retryWait spark.shuffle.io.backLog spark.shuffle.spill.compress spark.shuffle.accurateBlockThreshold spark.sql.shuffle.partitions","title":"Inherited Spark Shuffle Configurations"},{"location":"RemoteShuffle/#deprecated-spark-shuffle-configurations","text":"These configurations are deprecated and will not take effect. spark.shuffle.sort.bypassMergeThreshold # Replaced by spark.shuffle.remote.bypassMergeThreshold spark.maxRemoteBlockSizeFetchToMem # As we assume no local disks on compute nodes, shuffle blocks are all fetched to memory spark.shuffle.service.enabled # All following configurations are related to External Shuffle Service. ESS & remote shuffle cannot be enabled at the same time, as this remote shuffle facility takes over almost all functionalities of ESS. spark.shuffle.service.port spark.shuffle.service.index.cache.size spark.shuffle.maxChunksBeingTransferred spark.shuffle.registration.timeout spark.shuffle.registration.maxAttempts","title":"Deprecated Spark Shuffle Configurations"},{"location":"RemoteShuffle/#performance-evaluation-tool","text":"Leverage this tool to evaluate shuffle write/read performance separately under your specific storage system. This tool starts one Java process with #poolSize number of threads, running the specified remote-shuffle writers/readers in this module. Additional Spark configurations can be put in \"./spark-defaults.conf\" and will be loaded.(and printed as part of the summary for recording) Configuration details: * -h or --help : display help messages * -m or --mappers : the number of mappers, default to 5 * -r or --reducers : the number of reducers, default to 5 * -p or --poolSize : the number task threads in write/read thread pool, similar to spark.executor.cores. e.g. if mappers=15, poolSize=5, it takes 3 rounds to finish this job * -n or --rows : the number of rows per mapper, default to 1000 * -b or --shuffleBlockRawSize : the size of each shuffle block, default to 20000 Bytes * -w or --writer : the type of shuffle writers for benchmark, can be one of general, unsafe and bypassmergesort, default to unsafe * -onlyWrite or --onlyWrite : containing this flag then the benchmark only includes shuffle write stage, default behavior is perform both write & read * -uri or --storageMasterUri : Hadoop-compatible storage Master URI, default to file:// * -d or --dir : Shuffle directory, default /tmp * -l or --log : Log level, default to WARN Sample command: java -cp target/remote-shuffle-0.1-SNAPSHOT-test-jar-with-dependencies.jar org.apache.spark.shuffle.remote.PerformanceEvaluationTool -h Sample output unsafe shuffle writer: raw total size: 123 GB compressed size: 135 GB duration: 88.3 seconds throughput(raw): 1429.06843144412 MB/s throughput(storage): 1570.9931870053674 MB/s number of mappers: 210 number of reducers: 70 block size(raw): 8 MB block size(storage): 9 MB properties: spark.reducer.maxSizeInFlight -> 100m, spark.shuffle.remote.numReadThreads -> 8, spark.shuffle.remote.reducer.maxBlocksInFlightPerAddress -> 3 records per mapper: 70 load size per record:9000000 shuffle storage daos://default:1 shuffle folder: /tmp/shuffle ------------------------------------------------------------------------------------------------------------------------- shuffle reader: raw total size: 123 GB compressed size: 135 GB duration: 49.8 seconds throughput(raw): 2533.665772753123 MB/s throughput(storage): 2785.2911586057153 MB/s number of mappers: 210 number of reducers: 70 block size(raw): 8 MB block size(storage): 9 MB properties: spark.reducer.maxSizeInFlight -> 100m, spark.shuffle.remote.numReadThreads -> 8, spark.shuffle.remote.reducer.maxBlocksInFlightPerAddress -> 3 records per mapper: 70 load size per record:9000000 shuffle storage daos://default:1 shuffle folder: /tmp/shuffle","title":"Performance Evaluation Tool"},{"location":"RemoteShuffle/OAP-Developer-Guide/","text":"OAP Developer Guide This document contains the instructions & scripts on installing necessary dependencies and building OAP. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Arrow Data Source Native SQL Engine Building OAP Prerequisites for Building OAP is built with Apache Maven and Oracle Java 8, and mainly required tools to install on your cluster are listed below. Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. We provide scripts below to help automatically install dependencies above except RDMA , need change to root account, run: # git clone -b <tag-version> # cd oap-tools # sh dev/install-compile-time-dependencies.sh Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven Building To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building Specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache","title":"OAP Developer Guide"},{"location":"RemoteShuffle/OAP-Developer-Guide/#oap-developer-guide","text":"This document contains the instructions & scripts on installing necessary dependencies and building OAP. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Arrow Data Source Native SQL Engine","title":"OAP Developer Guide"},{"location":"RemoteShuffle/OAP-Developer-Guide/#building-oap","text":"","title":"Building OAP"},{"location":"RemoteShuffle/OAP-Developer-Guide/#prerequisites-for-building","text":"OAP is built with Apache Maven and Oracle Java 8, and mainly required tools to install on your cluster are listed below. Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. We provide scripts below to help automatically install dependencies above except RDMA , need change to root account, run: # git clone -b <tag-version> # cd oap-tools # sh dev/install-compile-time-dependencies.sh Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven","title":"Prerequisites for Building"},{"location":"RemoteShuffle/OAP-Developer-Guide/#building","text":"To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building Specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache","title":"Building"},{"location":"RemoteShuffle/OAP-Installation-Guide/","text":"OAP Installation Guide This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine. Contents Prerequisites Installing OAP Configuration Prerequisites OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly. Installing OAP Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Create a conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.0.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Extra Steps for Shuffle Remote PMem Extension If you use one of OAP features -- PMmem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details. Configuration Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar And then you can follow the corresponding feature documents for more details to use them.","title":"OAP Installation Guide"},{"location":"RemoteShuffle/OAP-Installation-Guide/#oap-installation-guide","text":"This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine.","title":"OAP Installation Guide"},{"location":"RemoteShuffle/OAP-Installation-Guide/#contents","text":"Prerequisites Installing OAP Configuration","title":"Contents"},{"location":"RemoteShuffle/OAP-Installation-Guide/#prerequisites","text":"OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly.","title":"Prerequisites"},{"location":"RemoteShuffle/OAP-Installation-Guide/#installing-oap","text":"Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Create a conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.0.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars","title":"Installing OAP"},{"location":"RemoteShuffle/OAP-Installation-Guide/#extra-steps-for-shuffle-remote-pmem-extension","text":"If you use one of OAP features -- PMmem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details.","title":"Extra Steps for Shuffle Remote PMem Extension"},{"location":"RemoteShuffle/OAP-Installation-Guide/#configuration","text":"Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar And then you can follow the corresponding feature documents for more details to use them.","title":"Configuration"},{"location":"RemoteShuffle/User-Guide/","text":"Remote Shuffle Contents Introduction User Guide Introduction Remote Shuffle is a Spark* ShuffleManager plugin, shuffling data through a remote Hadoop-compatible file system, as opposed to vanilla Spark's local-disks. This is an essential part of enabling Spark on disaggregated compute and storage architecture. Installation We have provided a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. If you have finished OAP-Installation-Guide , you can find compiled OAP jars in $HOME/miniconda2/envs/oapenv/oap_jars/ . Developer Guide Build and Deploy We have provided a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. If you have finished OAP-Installation-Guide , you can find compiled remote shuffle jars under $HOME/miniconda2/envs/oapenv/oap_jars . Then just skip this section and jump to User Guide . Build this module using the following command in OAP/oap-shuffle/remote-shuffle folder. This file needs to be deployed on every compute node that runs Spark. Manually place it on all nodes or let resource manager do the work. mvn -DskipTests clean package User Guide Enable Remote Shuffle Add the .jar files to the classpath of Spark driver and executors: Put the following configurations in spark-defaults.conf or Spark submit command line arguments. Note: For DAOS users, DAOS Hadoop/Java API jars should also be included in the classpath as we leverage DAOS Hadoop filesystem. spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/remote-shuffle-<version>.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/remote-shuffle-<version>.jar Enable the remote shuffle manager and specify the Hadoop storage system URI holding shuffle data. spark.shuffle.manager org.apache.spark.shuffle.remote.RemoteShuffleManager spark.shuffle.remote.storageMasterUri daos://default:1 # Or hdfs://namenode:port, file:///my/shuffle/dir Configurations Configurations and tuning parameters that change the behavior of remote shuffle. Most of them should work well under default values. Shuffle Root Directory This is to configure the root directory holding remote shuffle files. For each Spark application, a directory named after application ID is created under this root directory. spark.shuffle.remote.filesRootDirectory /shuffle Index Cache Size This is to configure the cache size for shuffle index files per executor. Shuffle data includes data files and index files. An index file is small but will be read many (the number of reducers) times. On a large scale, constantly reading these small index files from Hadoop Filesystem implementation(i.e. HDFS) is going to cause much overhead and latency. In addition, the shuffle files\u2019 transfer completely relies on the network between compute nodes and storage nodes. But the network inside compute nodes are not fully utilized. The index cache can eliminate the overhead of reading index files from storage cluster multiple times. By enabling index file cache, a reduce task fetches them from the remote executors who write them instead of reading from storage. If the remote executor doesn\u2019t have a desired index file in its cache, it will read the file from storage and cache it locally. The feature can also be disabled by setting the value to zero. spark.shuffle.remote.index.cache.size 30m Number of Threads Reading Data Files This is one of the parameters influencing shuffle read performance. It is to determine number of threads per executor reading shuffle data files from storage. spark.shuffle.remote.numReadThreads 5 Number of Threads Transitioning Index Files (when index cache is enabled) This is one of the parameters influencing shuffle read performance. It is to determine the number of client and server threads that transmit index information from another executor\u2019s cache. It is only valid when the index cache feature is enabled. spark.shuffle.remote.numIndexReadThreads 3 Bypass-merge-sort Threshold This threshold is used to decide using bypass-merge(hash-based) shuffle or not. By default we disable(by setting it to -1) hash-based shuffle writer in remote shuffle, because when memory is relatively sufficient, sort-based shuffle writer is often more efficient than the hash-based one. Hash-based shuffle writer entails a merging process, performing 3x I/Os than total shuffle size: 1 time for read I/Os and 2 times for write I/Os, this can be an even larger overhead under remote shuffle: the 3x shuffle size is gone through network, arriving at a remote storage system. spark.shuffle.remote.bypassMergeThreshold -1 Configurations fetching port for HDFS When the backend storage is HDFS, we contact http://$host:$port/conf to fetch configurations. They were not locally loaded because we assume absence of local storage. spark.shuffle.remote.hdfs.storageMasterUIPort 50070 Inherited Spark Shuffle Configurations These configurations are inherited from upstream Spark, they are still supported in remote shuffle. More explanations can be found in Spark core docs and Spark SQL docs . spark.reducer.maxSizeInFlight spark.reducer.maxReqsInFlight spark.reducer.maxBlocksInFlightPerAddress spark.shuffle.compress spark.shuffle.file.buffer spark.shuffle.io.maxRetries spark.shuffle.io.numConnectionsPerPeer spark.shuffle.io.preferDirectBufs spark.shuffle.io.retryWait spark.shuffle.io.backLog spark.shuffle.spill.compress spark.shuffle.accurateBlockThreshold spark.sql.shuffle.partitions Deprecated Spark Shuffle Configurations These configurations are deprecated and will not take effect. spark.shuffle.sort.bypassMergeThreshold # Replaced by spark.shuffle.remote.bypassMergeThreshold spark.maxRemoteBlockSizeFetchToMem # As we assume no local disks on compute nodes, shuffle blocks are all fetched to memory spark.shuffle.service.enabled # All following configurations are related to External Shuffle Service. ESS & remote shuffle cannot be enabled at the same time, as this remote shuffle facility takes over almost all functionalities of ESS. spark.shuffle.service.port spark.shuffle.service.index.cache.size spark.shuffle.maxChunksBeingTransferred spark.shuffle.registration.timeout spark.shuffle.registration.maxAttempts Performance Evaluation Tool Leverage this tool to evaluate shuffle write/read performance separately under your specific storage system. This tool starts one Java process with #poolSize number of threads, running the specified remote-shuffle writers/readers in this module. Additional Spark configurations can be put in \"./spark-defaults.conf\" and will be loaded.(and printed as part of the summary for recording) Configuration details: * -h or --help : display help messages * -m or --mappers : the number of mappers, default to 5 * -r or --reducers : the number of reducers, default to 5 * -p or --poolSize : the number task threads in write/read thread pool, similar to spark.executor.cores. e.g. if mappers=15, poolSize=5, it takes 3 rounds to finish this job * -n or --rows : the number of rows per mapper, default to 1000 * -b or --shuffleBlockRawSize : the size of each shuffle block, default to 20000 Bytes * -w or --writer : the type of shuffle writers for benchmark, can be one of general, unsafe and bypassmergesort, default to unsafe * -onlyWrite or --onlyWrite : containing this flag then the benchmark only includes shuffle write stage, default behavior is perform both write & read * -uri or --storageMasterUri : Hadoop-compatible storage Master URI, default to file:// * -d or --dir : Shuffle directory, default /tmp * -l or --log : Log level, default to WARN Sample command: java -cp target/remote-shuffle-0.1-SNAPSHOT-test-jar-with-dependencies.jar org.apache.spark.shuffle.remote.PerformanceEvaluationTool -h Sample output unsafe shuffle writer: raw total size: 123 GB compressed size: 135 GB duration: 88.3 seconds throughput(raw): 1429.06843144412 MB/s throughput(storage): 1570.9931870053674 MB/s number of mappers: 210 number of reducers: 70 block size(raw): 8 MB block size(storage): 9 MB properties: spark.reducer.maxSizeInFlight -> 100m, spark.shuffle.remote.numReadThreads -> 8, spark.shuffle.remote.reducer.maxBlocksInFlightPerAddress -> 3 records per mapper: 70 load size per record:9000000 shuffle storage daos://default:1 shuffle folder: /tmp/shuffle ------------------------------------------------------------------------------------------------------------------------- shuffle reader: raw total size: 123 GB compressed size: 135 GB duration: 49.8 seconds throughput(raw): 2533.665772753123 MB/s throughput(storage): 2785.2911586057153 MB/s number of mappers: 210 number of reducers: 70 block size(raw): 8 MB block size(storage): 9 MB properties: spark.reducer.maxSizeInFlight -> 100m, spark.shuffle.remote.numReadThreads -> 8, spark.shuffle.remote.reducer.maxBlocksInFlightPerAddress -> 3 records per mapper: 70 load size per record:9000000 shuffle storage daos://default:1 shuffle folder: /tmp/shuffle","title":"User Guide"},{"location":"RemoteShuffle/User-Guide/#remote-shuffle","text":"","title":"Remote Shuffle"},{"location":"RemoteShuffle/User-Guide/#contents","text":"Introduction User Guide","title":"Contents"},{"location":"RemoteShuffle/User-Guide/#introduction","text":"Remote Shuffle is a Spark* ShuffleManager plugin, shuffling data through a remote Hadoop-compatible file system, as opposed to vanilla Spark's local-disks. This is an essential part of enabling Spark on disaggregated compute and storage architecture.","title":"Introduction"},{"location":"RemoteShuffle/User-Guide/#installation","text":"We have provided a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. If you have finished OAP-Installation-Guide , you can find compiled OAP jars in $HOME/miniconda2/envs/oapenv/oap_jars/ .","title":"Installation"},{"location":"RemoteShuffle/User-Guide/#developer-guide","text":"","title":"Developer Guide"},{"location":"RemoteShuffle/User-Guide/#build-and-deploy","text":"We have provided a Conda package which will automatically install dependencies needed by OAP, you can refer to OAP-Installation-Guide for more information. If you have finished OAP-Installation-Guide , you can find compiled remote shuffle jars under $HOME/miniconda2/envs/oapenv/oap_jars . Then just skip this section and jump to User Guide . Build this module using the following command in OAP/oap-shuffle/remote-shuffle folder. This file needs to be deployed on every compute node that runs Spark. Manually place it on all nodes or let resource manager do the work. mvn -DskipTests clean package","title":"Build and Deploy"},{"location":"RemoteShuffle/User-Guide/#user-guide","text":"","title":"User Guide"},{"location":"RemoteShuffle/User-Guide/#enable-remote-shuffle","text":"Add the .jar files to the classpath of Spark driver and executors: Put the following configurations in spark-defaults.conf or Spark submit command line arguments. Note: For DAOS users, DAOS Hadoop/Java API jars should also be included in the classpath as we leverage DAOS Hadoop filesystem. spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/remote-shuffle-<version>.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/remote-shuffle-<version>.jar Enable the remote shuffle manager and specify the Hadoop storage system URI holding shuffle data. spark.shuffle.manager org.apache.spark.shuffle.remote.RemoteShuffleManager spark.shuffle.remote.storageMasterUri daos://default:1 # Or hdfs://namenode:port, file:///my/shuffle/dir","title":"Enable Remote Shuffle"},{"location":"RemoteShuffle/User-Guide/#configurations","text":"Configurations and tuning parameters that change the behavior of remote shuffle. Most of them should work well under default values.","title":"Configurations"},{"location":"RemoteShuffle/User-Guide/#shuffle-root-directory","text":"This is to configure the root directory holding remote shuffle files. For each Spark application, a directory named after application ID is created under this root directory. spark.shuffle.remote.filesRootDirectory /shuffle","title":"Shuffle Root Directory"},{"location":"RemoteShuffle/User-Guide/#index-cache-size","text":"This is to configure the cache size for shuffle index files per executor. Shuffle data includes data files and index files. An index file is small but will be read many (the number of reducers) times. On a large scale, constantly reading these small index files from Hadoop Filesystem implementation(i.e. HDFS) is going to cause much overhead and latency. In addition, the shuffle files\u2019 transfer completely relies on the network between compute nodes and storage nodes. But the network inside compute nodes are not fully utilized. The index cache can eliminate the overhead of reading index files from storage cluster multiple times. By enabling index file cache, a reduce task fetches them from the remote executors who write them instead of reading from storage. If the remote executor doesn\u2019t have a desired index file in its cache, it will read the file from storage and cache it locally. The feature can also be disabled by setting the value to zero. spark.shuffle.remote.index.cache.size 30m","title":"Index Cache Size"},{"location":"RemoteShuffle/User-Guide/#number-of-threads-reading-data-files","text":"This is one of the parameters influencing shuffle read performance. It is to determine number of threads per executor reading shuffle data files from storage. spark.shuffle.remote.numReadThreads 5","title":"Number of Threads Reading Data Files"},{"location":"RemoteShuffle/User-Guide/#number-of-threads-transitioning-index-files-when-index-cache-is-enabled","text":"This is one of the parameters influencing shuffle read performance. It is to determine the number of client and server threads that transmit index information from another executor\u2019s cache. It is only valid when the index cache feature is enabled. spark.shuffle.remote.numIndexReadThreads 3","title":"Number of Threads Transitioning Index Files (when index cache is enabled)"},{"location":"RemoteShuffle/User-Guide/#bypass-merge-sort-threshold","text":"This threshold is used to decide using bypass-merge(hash-based) shuffle or not. By default we disable(by setting it to -1) hash-based shuffle writer in remote shuffle, because when memory is relatively sufficient, sort-based shuffle writer is often more efficient than the hash-based one. Hash-based shuffle writer entails a merging process, performing 3x I/Os than total shuffle size: 1 time for read I/Os and 2 times for write I/Os, this can be an even larger overhead under remote shuffle: the 3x shuffle size is gone through network, arriving at a remote storage system. spark.shuffle.remote.bypassMergeThreshold -1","title":"Bypass-merge-sort Threshold"},{"location":"RemoteShuffle/User-Guide/#configurations-fetching-port-for-hdfs","text":"When the backend storage is HDFS, we contact http://$host:$port/conf to fetch configurations. They were not locally loaded because we assume absence of local storage. spark.shuffle.remote.hdfs.storageMasterUIPort 50070","title":"Configurations fetching port for HDFS"},{"location":"RemoteShuffle/User-Guide/#inherited-spark-shuffle-configurations","text":"These configurations are inherited from upstream Spark, they are still supported in remote shuffle. More explanations can be found in Spark core docs and Spark SQL docs . spark.reducer.maxSizeInFlight spark.reducer.maxReqsInFlight spark.reducer.maxBlocksInFlightPerAddress spark.shuffle.compress spark.shuffle.file.buffer spark.shuffle.io.maxRetries spark.shuffle.io.numConnectionsPerPeer spark.shuffle.io.preferDirectBufs spark.shuffle.io.retryWait spark.shuffle.io.backLog spark.shuffle.spill.compress spark.shuffle.accurateBlockThreshold spark.sql.shuffle.partitions","title":"Inherited Spark Shuffle Configurations"},{"location":"RemoteShuffle/User-Guide/#deprecated-spark-shuffle-configurations","text":"These configurations are deprecated and will not take effect. spark.shuffle.sort.bypassMergeThreshold # Replaced by spark.shuffle.remote.bypassMergeThreshold spark.maxRemoteBlockSizeFetchToMem # As we assume no local disks on compute nodes, shuffle blocks are all fetched to memory spark.shuffle.service.enabled # All following configurations are related to External Shuffle Service. ESS & remote shuffle cannot be enabled at the same time, as this remote shuffle facility takes over almost all functionalities of ESS. spark.shuffle.service.port spark.shuffle.service.index.cache.size spark.shuffle.maxChunksBeingTransferred spark.shuffle.registration.timeout spark.shuffle.registration.maxAttempts","title":"Deprecated Spark Shuffle Configurations"},{"location":"RemoteShuffle/User-Guide/#performance-evaluation-tool","text":"Leverage this tool to evaluate shuffle write/read performance separately under your specific storage system. This tool starts one Java process with #poolSize number of threads, running the specified remote-shuffle writers/readers in this module. Additional Spark configurations can be put in \"./spark-defaults.conf\" and will be loaded.(and printed as part of the summary for recording) Configuration details: * -h or --help : display help messages * -m or --mappers : the number of mappers, default to 5 * -r or --reducers : the number of reducers, default to 5 * -p or --poolSize : the number task threads in write/read thread pool, similar to spark.executor.cores. e.g. if mappers=15, poolSize=5, it takes 3 rounds to finish this job * -n or --rows : the number of rows per mapper, default to 1000 * -b or --shuffleBlockRawSize : the size of each shuffle block, default to 20000 Bytes * -w or --writer : the type of shuffle writers for benchmark, can be one of general, unsafe and bypassmergesort, default to unsafe * -onlyWrite or --onlyWrite : containing this flag then the benchmark only includes shuffle write stage, default behavior is perform both write & read * -uri or --storageMasterUri : Hadoop-compatible storage Master URI, default to file:// * -d or --dir : Shuffle directory, default /tmp * -l or --log : Log level, default to WARN Sample command: java -cp target/remote-shuffle-0.1-SNAPSHOT-test-jar-with-dependencies.jar org.apache.spark.shuffle.remote.PerformanceEvaluationTool -h Sample output unsafe shuffle writer: raw total size: 123 GB compressed size: 135 GB duration: 88.3 seconds throughput(raw): 1429.06843144412 MB/s throughput(storage): 1570.9931870053674 MB/s number of mappers: 210 number of reducers: 70 block size(raw): 8 MB block size(storage): 9 MB properties: spark.reducer.maxSizeInFlight -> 100m, spark.shuffle.remote.numReadThreads -> 8, spark.shuffle.remote.reducer.maxBlocksInFlightPerAddress -> 3 records per mapper: 70 load size per record:9000000 shuffle storage daos://default:1 shuffle folder: /tmp/shuffle ------------------------------------------------------------------------------------------------------------------------- shuffle reader: raw total size: 123 GB compressed size: 135 GB duration: 49.8 seconds throughput(raw): 2533.665772753123 MB/s throughput(storage): 2785.2911586057153 MB/s number of mappers: 210 number of reducers: 70 block size(raw): 8 MB block size(storage): 9 MB properties: spark.reducer.maxSizeInFlight -> 100m, spark.shuffle.remote.numReadThreads -> 8, spark.shuffle.remote.reducer.maxBlocksInFlightPerAddress -> 3 records per mapper: 70 load size per record:9000000 shuffle storage daos://default:1 shuffle folder: /tmp/shuffle","title":"Performance Evaluation Tool"},{"location":"SQLDSCache/","text":"User Guide Prerequisites Getting Started Configuration for YARN Cluster Mode Configuration for Spark Standalone Mode Working with SQL Index Working with SQL Data Source Cache Run TPC-DS Benchmark Advanced Configuration Prerequisites SQL Index and Data Source Cache on Spark 3.0.0 requires a working Hadoop cluster with YARN and Spark. Running Spark on YARN requires a binary distribution of Spark, which is built with YARN support. Getting Started Building We have provided a Conda package which will automatically install dependencies and build OAP jars, please follow OAP-Installation-Guide and you can find compiled OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars once finished the installation. If you\u2019d like to build from source code, please refer to Developer Guide for the detailed steps. Spark Configurations Users usually test and run Spark SQL or Scala scripts in Spark Shell, which launches Spark applications on YRAN with client mode. In this section, we will start with Spark Shell then introduce other use scenarios. Before you run $SPARK_HOME/bin/spark-shell , you need to configure Spark for integration. You need to add or update the following configurations in the Spark configuration file $SPARK_HOME/conf/spark-defaults.conf on your working node. spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar Verify Integration After configuration, you can follow these steps to verify the OAP integration is working using Spark Shell. Create a test data path on your HDFS. hdfs:///user/oap/ for example. hadoop fs -mkdir /user/oap/ Launch Spark Shell using the following command on your working node. $SPARK_HOME/bin/spark-shell Execute the following commands in Spark Shell to test OAP integration. > spark.sql(s\"\"\"CREATE TABLE oap_test (a INT, b STRING) USING parquet OPTIONS (path 'hdfs:///user/oap/')\"\"\".stripMargin) > val data = (1 to 30000).map { i => (i, s\"this is test $i\") }.toDF().createOrReplaceTempView(\"t\") > spark.sql(\"insert overwrite table oap_test select * from t\") > spark.sql(\"create oindex index1 on oap_test (a)\") > spark.sql(\"show oindex from oap_test\").show() This test creates an index for a table and then shows it. If there are no errors, the OAP .jar is working with the configuration. The picture below is an example of a successfully run. Configuration for YARN Cluster Mode Spark Shell, Spark SQL CLI and Thrift Sever run Spark application in client mode. While Spark Submit tool can run Spark application in client or cluster mode, which is decided by --deploy-mode parameter. Getting Started session has shown the configurations needed for client mode. If you are running Spark Submit tool in cluster mode, you need to follow the below configuration steps instead. Add the following OAP configuration settings to $SPARK_HOME/conf/spark-defaults.conf on your working node before running spark-submit in cluster mode. spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path on your working node spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.driver.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar Configuration for Spark Standalone Mode In addition to running on the YARN cluster manager, Spark also provides a simple standalone deploy mode. If you are using Spark in Spark Standalone mode: Make sure the OAP .jar at the same path of all the worker nodes. Add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf on the working node. spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path on worker nodes spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # absolute path on worker nodes spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar Working with SQL Index After a successful OAP integration, you can use OAP SQL DDL to manage table indexes. The DDL operations include index create , drop , refresh , and show . Test these functions using the following examples in Spark Shell. > spark.sql(s\"\"\"CREATE TABLE oap_test (a INT, b STRING) USING parquet OPTIONS (path 'hdfs:///user/oap/')\"\"\".stripMargin) > val data = (1 to 30000).map { i => (i, s\"this is test $i\") }.toDF().createOrReplaceTempView(\"t\") > spark.sql(\"insert overwrite table oap_test select * from t\") Index Creation Use the CREATE OINDEX DDL command to create a B+ Tree index or bitmap index. CREATE OINDEX index_name ON table_name (column_name) USING [BTREE, BITMAP] The following example creates a B+ Tree index on column \"a\" of the oap_test table. > spark.sql(\"create oindex index1 on oap_test (a)\") Use SHOW OINDEX command to show all the created indexes on a specified table. > spark.sql(\"show oindex from oap_test\").show() Use Index Using index in a query is transparent. When SQL queries have filter conditions on the column(s) which can take advantage of the index to filter the data scan, the index will automatically be applied to the execution of Spark SQL. The following example will automatically use the underlayer index created on column \"a\". > spark.sql(\"SELECT * FROM oap_test WHERE a = 1\").show() Drop index Use DROP OINDEX command to drop a named index. > spark.sql(\"drop oindex index1 on oap_test\") Working with SQL Data Source Cache Data Source Cache can provide input data cache functionality to the executor. When using the cache data among different SQL queries, configure cache to allow different SQL queries to use the same executor process. Do this by running your queries through the Spark ThriftServer as shown below. For cache media, we support both DRAM and Intel PMem which means you can choose to cache data in DRAM or Intel PMem if you have PMem configured in hardware. Use DRAM Cache Make the following configuration changes in Spark configuration file $SPARK_HOME/conf/spark-defaults.conf . spark.memory.offHeap.enabled false spark.oap.cache.strategy guava spark.sql.oap.cache.memory.manager offheap # according to the resource of cluster spark.executor.memoryOverhead 50g # equal to the size of executor.memoryOverhead spark.executor.sql.oap.cache.offheap.memory.size 50g # for parquet fileformat, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for orc fileformat, enable binary cache spark.sql.oap.orc.binary.cache.enabled true NOTE : Change spark.executor.sql.oap.cache.offheap.memory.size based on the availability of DRAM capacity to cache data, and its size is equal to spark.executor.memoryOverhead Launch Spark ThriftServer Launch Spark Thrift Server, and use the Beeline command line tool to connect to the Thrift Server to execute DDL or DML operations. The data cache will automatically take effect for Parquet or ORC file sources. The rest of this section will show you how to do a quick verification of cache functionality. It will reuse the database metastore created in the Working with SQL Index section, which creates the oap_test table definition. In production, Spark Thrift Server will have its own metastore database directory or metastore service and use DDL's through Beeline for creating your tables. When you run spark-shell to create the oap_test table, metastore_db will be created in the directory where you ran '$SPARK_HOME/bin/spark-shell'. Go to that directory and execute the following command to launch Thrift JDBC server and run queries. $SPARK_HOME/sbin/start-thriftserver.sh Use Beeline and connect to the Thrift JDBC server, replacing the hostname (mythriftserver) with your own Thrift Server hostname. . $SPARK_HOME/bin/beeline -u jdbc:hive2://<mythriftserver>:10000 After the connection is established, execute the following commands to check the metastore is initialized correctly. > SHOW databases; > USE default; > SHOW tables; Run queries on the table that will use the cache automatically. For example, > SELECT * FROM oap_test WHERE a = 1; > SELECT * FROM oap_test WHERE a = 2; > SELECT * FROM oap_test WHERE a = 3; ... Open the Spark History Web UI and go to the OAP tab page to verify the cache metrics. The following picture is an example. Use PMem Cache Prerequisites The following steps are required to configure OAP to use PMem cache with external cache strategy. PMem hardware is successfully deployed on each node in cluster. Besides, when enabling SQL Data Source Cache with external cache using Plasma, PMem could get noticeable performance gain with BIOS configuration settings below, especially on cross socket write path. Socket Configuration -> Memory Configuration -> NGN Configuration -> Snoopy mode for AD : Enabled Socket Configuration -> Intel UPI General Configuration -> Stale AtoS : Disabled It's strongly advised to use Linux device mapper to interleave PMem across sockets and get maximum size for Plasma. // use ipmctl command to show topology and dimm info of PMem ipmctl show -topology ipmctl show -dimm // provision PMem in app direct mode ipmctl create -goal PersistentMemoryType=AppDirect // reboot system to make configuration take affect reboot // check capacity provisioned for app direct mode(AppDirectCapacity) ipmctl show -memoryresources // show the PMem region information ipmctl show -region // create namespace based on the region, multi namespaces can be created on a single region ndctl create-namespace -m fsdax -r region0 ndctl create-namespace -m fsdax -r region1 // show the created namespaces fdisk -l // create and mount file system sudo dmsetup create striped-pmem mkfs.ext4 -b 4096 -E stride=512 -F /dev/mapper/striped-pmem mkdir -p /mnt/pmem mount -o dax /dev/mapper/striped-pmem /mnt/pmem For more information you can refer to Quick Start Guide: Provision Intel\u00ae Optane\u2122 DC Persistent Memory SQL Data Source Cache uses Plasma as a node-level external cache service, the benefit of using external cache is data could be shared across process boundaries. Plasma is a high-performance shared-memory object store and a component of Apache Arrow . We have modified Plasma to support PMem, and make it open source on Intel-bigdata Arrow repo. If you have finished OAP Installation Guide , Plasma will be automatically installed and then you just need copy arrow-plasma-0.17.0.jar to $SPARK_HOME/jars . For manual building and installation steps you can refer to Plasma installation . Refer to configuration below to apply external cache strategy and start Plasma service on each node and start your workload. Configuration for NUMA Install numactl to bind the executor to the PMem device on the same NUMA node. yum install numactl -y We recommend you use NUMA-patched Spark to achieve better performance gain for the external strategy compared with Community Spark. Build Spark from source to enable NUMA-binding support, refer to Enabling-NUMA-binding-for-PMem-in-Spark . Configuration for enabling PMem cache Add the following configuration to $SPARK_HOME/conf/spark-defaults.conf . # 2x number of your worker nodes spark.executor.instances 6 # enable numa spark.yarn.numa.enabled true # enable SQL Index and Data Source Cache extension in Spark spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node, when in Yarn client mode spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir, when in Yarn client mode spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node,when in Yarn client mode spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # for parquet file format, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for ORC file format, enable binary cache spark.sql.oap.orc.binary.cache.enabled true # enable external cache strategy spark.oap.cache.strategy external spark.sql.oap.dcpmm.free.wait.threshold 50000000000 # according to your executor core number spark.executor.sql.oap.cache.external.client.pool.size 10 Start Plasma service manually Plasma config parameters: -m how much Bytes share memory Plasma will use -s Unix Domain sockcet path -d PMem directory Start Plasma service on each node with following command, then run your workload. If you install OAP by Conda, you can find plasma-store-server in the path $HOME/miniconda2/envs/oapenv/bin/ . ./plasma-store-server -m 15000000000 -s /tmp/plasmaStore -d /mnt/pmem Remember to kill plasma-store-server process if you no longer need cache, and you should delete /tmp/plasmaStore which is a Unix domain socket. Use Yarn to start Plamsa service When using Yarn(Hadoop version >= 3.1) to start Plasma service, you should provide a json file as below. { \"name\": \"plasma-store-service\", \"version\": 1, \"components\" : [ { \"name\": \"plasma-store-service\", \"number_of_containers\": 3, \"launch_command\": \"plasma-store-server -m 15000000000 -s /tmp/plasmaStore -d /mnt/pmem\", \"resource\": { \"cpus\": 1, \"memory\": 512 } } ] } Run command yarn app -launch plasma-store-service /tmp/plasmaLaunch.json to start Plasma server. Run yarn app -stop plasma-store-service to stop it. Run yarn app -destroy plasma-store-service to destroy it. Verify PMem cache functionality After finishing configuration, restart Spark Thrift Server for the configuration changes to take effect. Start at step 2 of the Use DRAM Cache guide to verify that cache is working correctly. Check PMem cache size by checking disk space with df -h . Run TPC-DS Benchmark This section provides instructions and tools for running TPC-DS queries to evaluate the cache performance of various configurations. The TPC-DS suite has many queries and we select 9 I/O intensive queries to simplify performance evaluation. We created some tool scripts oap-benchmark-tool.zip to simplify running the workload. If you are already familiar with TPC-DS data generation and running a TPC-DS tool suite, skip our tool and use the TPC-DS tool suite directly. Prerequisites Python 2.7+ is required on the working node. Prepare the Tool Download oap-benchmark-tool.zip and unzip to a folder (for example, oap-benchmark-tool folder) on your working node. Copy oap-benchmark-tool/tools/tpcds-kits to ALL worker nodes under the same folder (for example, /home/oap/tpcds-kits ). Generate TPC-DS Data Update the values for the following variables in oap-benchmark-tool/scripts/tool.conf based on your environment and needs. SPARK_HOME: Point to the Spark home directory of your Spark setup. TPCDS_KITS_DIR: The tpcds-kits directory you coped to the worker nodes in the above prepare process. For example, /home/oap/tpcds-kits NAMENODE_ADDRESS: Your HDFS Namenode address in the format of host:port. THRIFT_SERVER_ADDRESS: Your working node address on which you will run Thrift Server. DATA_SCALE: The data scale to be generated in GB DATA_FORMAT: The data file format. You can specify parquet or orc For example: export SPARK_HOME=/home/oap/spark-3.0.0 export TPCDS_KITS_DIR=/home/oap/tpcds-kits export NAMENODE_ADDRESS=mynamenode:9000 export THRIFT_SERVER_ADDRESS=mythriftserver export DATA_SCALE=1024 export DATA_FORMAT=parquet Start data generation. In the root directory of this tool ( oap-benchmark-tool ), run scripts/run_gen_data.sh to start the data generation process. cd oap-benchmark-tool sh ./scripts/run_gen_data.sh Once finished, the $scale data will be generated in the HDFS folder genData$scale . And a database called tpcds_$format$scale will contain the TPC-DS tables. Start Spark Thrift Server Start the Thrift Server in the tool root folder, which is the same folder you run data generation scripts. Use either the PMem or DRAM script to start the Thrift Server. Use PMem as Cache Media Update the configuration values in scripts/spark_thrift_server_yarn_with_PMem.sh to reflect your environment. Normally, you need to update the following configuration values to cache to PMem. --num-executors --driver-memory --executor-memory --executor-cores --conf spark.oap.cache.strategy --conf spark.sql.oap.dcpmm.free.wait.threshold --conf spark.executor.sql.oap.cache.external.client.pool.size These settings will override the values specified in Spark configuration file ( spark-defaults.conf ). After the configuration is done, you can execute the following command to start Thrift Server. cd oap-benchmark-tool sh ./scripts/spark_thrift_server_yarn_with_PMem.sh start In this script, we use external as cache strategy for Parquet Binary data cache. Use DRAM as Cache Media Update the configuration values in scripts/spark_thrift_server_yarn_with_DRAM.sh to reflect your environment. Normally, you need to update the following configuration values to cache to DRAM. --num-executors --driver-memory --executor-memory --executor-cores --conf spark.executor.sql.oap.cache.offheap.memory.size --conf spark.executor.memoryOverhead These settings will override the values specified in Spark configuration file ( spark-defaults.conf ). After the configuration is done, you can execute the following command to start Thrift Server. cd oap-benchmark-tool sh ./scripts/spark_thrift_server_yarn_with_DRAM.sh start Run Queries Execute the following command to start to run queries. If you use external cache strategy, also need start plasma service manually as above. cd oap-benchmark-tool sh ./scripts/run_tpcds.sh When all the queries are done, you will see the result.json file in the current directory. You will find in the 2nd and 3rd round, cache feature takes effect and query time becomes less. And the Spark webUI OAP tab has more specific OAP cache metrics just as section step 5. Advanced Configuration Additional Cache Strategies In addition to external cache strategy, SQL Data Source Cache also supports 3 other cache strategies: guava , noevict and vmemcache . Index and Data Cache Separation To optimize the cache media utilization, SQL Data Source Cache supports cache separation of data and index, by using same or different cache media with DRAM and PMem. Cache Hot Tables Data Source Cache also supports caching specific tables according to actual situations, these tables are usually hot tables. Column Vector Cache This document above uses binary cache as example for Parquet file format, if your cluster memory resources is abundant enough, you can choose ColumnVector data cache instead of binary cache for Parquet to spare computation time. Large Scale and Heterogeneous Cluster Support Introduce an external database to store cache locality info to support large-scale and heterogeneous clusters. For more information and configuration details, please refer to Advanced Configuration .","title":"User Guide"},{"location":"SQLDSCache/#user-guide","text":"Prerequisites Getting Started Configuration for YARN Cluster Mode Configuration for Spark Standalone Mode Working with SQL Index Working with SQL Data Source Cache Run TPC-DS Benchmark Advanced Configuration","title":"User Guide"},{"location":"SQLDSCache/#prerequisites","text":"SQL Index and Data Source Cache on Spark 3.0.0 requires a working Hadoop cluster with YARN and Spark. Running Spark on YARN requires a binary distribution of Spark, which is built with YARN support.","title":"Prerequisites"},{"location":"SQLDSCache/#getting-started","text":"","title":"Getting Started"},{"location":"SQLDSCache/#building","text":"We have provided a Conda package which will automatically install dependencies and build OAP jars, please follow OAP-Installation-Guide and you can find compiled OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars once finished the installation. If you\u2019d like to build from source code, please refer to Developer Guide for the detailed steps.","title":"Building"},{"location":"SQLDSCache/#spark-configurations","text":"Users usually test and run Spark SQL or Scala scripts in Spark Shell, which launches Spark applications on YRAN with client mode. In this section, we will start with Spark Shell then introduce other use scenarios. Before you run $SPARK_HOME/bin/spark-shell , you need to configure Spark for integration. You need to add or update the following configurations in the Spark configuration file $SPARK_HOME/conf/spark-defaults.conf on your working node. spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar","title":"Spark Configurations"},{"location":"SQLDSCache/#verify-integration","text":"After configuration, you can follow these steps to verify the OAP integration is working using Spark Shell. Create a test data path on your HDFS. hdfs:///user/oap/ for example. hadoop fs -mkdir /user/oap/ Launch Spark Shell using the following command on your working node. $SPARK_HOME/bin/spark-shell Execute the following commands in Spark Shell to test OAP integration. > spark.sql(s\"\"\"CREATE TABLE oap_test (a INT, b STRING) USING parquet OPTIONS (path 'hdfs:///user/oap/')\"\"\".stripMargin) > val data = (1 to 30000).map { i => (i, s\"this is test $i\") }.toDF().createOrReplaceTempView(\"t\") > spark.sql(\"insert overwrite table oap_test select * from t\") > spark.sql(\"create oindex index1 on oap_test (a)\") > spark.sql(\"show oindex from oap_test\").show() This test creates an index for a table and then shows it. If there are no errors, the OAP .jar is working with the configuration. The picture below is an example of a successfully run.","title":"Verify Integration"},{"location":"SQLDSCache/#configuration-for-yarn-cluster-mode","text":"Spark Shell, Spark SQL CLI and Thrift Sever run Spark application in client mode. While Spark Submit tool can run Spark application in client or cluster mode, which is decided by --deploy-mode parameter. Getting Started session has shown the configurations needed for client mode. If you are running Spark Submit tool in cluster mode, you need to follow the below configuration steps instead. Add the following OAP configuration settings to $SPARK_HOME/conf/spark-defaults.conf on your working node before running spark-submit in cluster mode. spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path on your working node spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.driver.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar","title":"Configuration for YARN Cluster Mode"},{"location":"SQLDSCache/#configuration-for-spark-standalone-mode","text":"In addition to running on the YARN cluster manager, Spark also provides a simple standalone deploy mode. If you are using Spark in Spark Standalone mode: Make sure the OAP .jar at the same path of all the worker nodes. Add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf on the working node. spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path on worker nodes spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # absolute path on worker nodes spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar","title":"Configuration for Spark Standalone Mode"},{"location":"SQLDSCache/#working-with-sql-index","text":"After a successful OAP integration, you can use OAP SQL DDL to manage table indexes. The DDL operations include index create , drop , refresh , and show . Test these functions using the following examples in Spark Shell. > spark.sql(s\"\"\"CREATE TABLE oap_test (a INT, b STRING) USING parquet OPTIONS (path 'hdfs:///user/oap/')\"\"\".stripMargin) > val data = (1 to 30000).map { i => (i, s\"this is test $i\") }.toDF().createOrReplaceTempView(\"t\") > spark.sql(\"insert overwrite table oap_test select * from t\")","title":"Working with SQL Index"},{"location":"SQLDSCache/#index-creation","text":"Use the CREATE OINDEX DDL command to create a B+ Tree index or bitmap index. CREATE OINDEX index_name ON table_name (column_name) USING [BTREE, BITMAP] The following example creates a B+ Tree index on column \"a\" of the oap_test table. > spark.sql(\"create oindex index1 on oap_test (a)\") Use SHOW OINDEX command to show all the created indexes on a specified table. > spark.sql(\"show oindex from oap_test\").show()","title":"Index Creation"},{"location":"SQLDSCache/#use-index","text":"Using index in a query is transparent. When SQL queries have filter conditions on the column(s) which can take advantage of the index to filter the data scan, the index will automatically be applied to the execution of Spark SQL. The following example will automatically use the underlayer index created on column \"a\". > spark.sql(\"SELECT * FROM oap_test WHERE a = 1\").show()","title":"Use Index"},{"location":"SQLDSCache/#drop-index","text":"Use DROP OINDEX command to drop a named index. > spark.sql(\"drop oindex index1 on oap_test\")","title":"Drop index"},{"location":"SQLDSCache/#working-with-sql-data-source-cache","text":"Data Source Cache can provide input data cache functionality to the executor. When using the cache data among different SQL queries, configure cache to allow different SQL queries to use the same executor process. Do this by running your queries through the Spark ThriftServer as shown below. For cache media, we support both DRAM and Intel PMem which means you can choose to cache data in DRAM or Intel PMem if you have PMem configured in hardware.","title":"Working with SQL Data Source Cache"},{"location":"SQLDSCache/#use-dram-cache","text":"Make the following configuration changes in Spark configuration file $SPARK_HOME/conf/spark-defaults.conf . spark.memory.offHeap.enabled false spark.oap.cache.strategy guava spark.sql.oap.cache.memory.manager offheap # according to the resource of cluster spark.executor.memoryOverhead 50g # equal to the size of executor.memoryOverhead spark.executor.sql.oap.cache.offheap.memory.size 50g # for parquet fileformat, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for orc fileformat, enable binary cache spark.sql.oap.orc.binary.cache.enabled true NOTE : Change spark.executor.sql.oap.cache.offheap.memory.size based on the availability of DRAM capacity to cache data, and its size is equal to spark.executor.memoryOverhead Launch Spark ThriftServer Launch Spark Thrift Server, and use the Beeline command line tool to connect to the Thrift Server to execute DDL or DML operations. The data cache will automatically take effect for Parquet or ORC file sources. The rest of this section will show you how to do a quick verification of cache functionality. It will reuse the database metastore created in the Working with SQL Index section, which creates the oap_test table definition. In production, Spark Thrift Server will have its own metastore database directory or metastore service and use DDL's through Beeline for creating your tables. When you run spark-shell to create the oap_test table, metastore_db will be created in the directory where you ran '$SPARK_HOME/bin/spark-shell'. Go to that directory and execute the following command to launch Thrift JDBC server and run queries. $SPARK_HOME/sbin/start-thriftserver.sh Use Beeline and connect to the Thrift JDBC server, replacing the hostname (mythriftserver) with your own Thrift Server hostname. . $SPARK_HOME/bin/beeline -u jdbc:hive2://<mythriftserver>:10000 After the connection is established, execute the following commands to check the metastore is initialized correctly. > SHOW databases; > USE default; > SHOW tables; Run queries on the table that will use the cache automatically. For example, > SELECT * FROM oap_test WHERE a = 1; > SELECT * FROM oap_test WHERE a = 2; > SELECT * FROM oap_test WHERE a = 3; ... Open the Spark History Web UI and go to the OAP tab page to verify the cache metrics. The following picture is an example.","title":"Use DRAM Cache"},{"location":"SQLDSCache/#use-pmem-cache","text":"","title":"Use PMem Cache"},{"location":"SQLDSCache/#prerequisites_1","text":"The following steps are required to configure OAP to use PMem cache with external cache strategy. PMem hardware is successfully deployed on each node in cluster. Besides, when enabling SQL Data Source Cache with external cache using Plasma, PMem could get noticeable performance gain with BIOS configuration settings below, especially on cross socket write path. Socket Configuration -> Memory Configuration -> NGN Configuration -> Snoopy mode for AD : Enabled Socket Configuration -> Intel UPI General Configuration -> Stale AtoS : Disabled It's strongly advised to use Linux device mapper to interleave PMem across sockets and get maximum size for Plasma. // use ipmctl command to show topology and dimm info of PMem ipmctl show -topology ipmctl show -dimm // provision PMem in app direct mode ipmctl create -goal PersistentMemoryType=AppDirect // reboot system to make configuration take affect reboot // check capacity provisioned for app direct mode(AppDirectCapacity) ipmctl show -memoryresources // show the PMem region information ipmctl show -region // create namespace based on the region, multi namespaces can be created on a single region ndctl create-namespace -m fsdax -r region0 ndctl create-namespace -m fsdax -r region1 // show the created namespaces fdisk -l // create and mount file system sudo dmsetup create striped-pmem mkfs.ext4 -b 4096 -E stride=512 -F /dev/mapper/striped-pmem mkdir -p /mnt/pmem mount -o dax /dev/mapper/striped-pmem /mnt/pmem For more information you can refer to Quick Start Guide: Provision Intel\u00ae Optane\u2122 DC Persistent Memory SQL Data Source Cache uses Plasma as a node-level external cache service, the benefit of using external cache is data could be shared across process boundaries. Plasma is a high-performance shared-memory object store and a component of Apache Arrow . We have modified Plasma to support PMem, and make it open source on Intel-bigdata Arrow repo. If you have finished OAP Installation Guide , Plasma will be automatically installed and then you just need copy arrow-plasma-0.17.0.jar to $SPARK_HOME/jars . For manual building and installation steps you can refer to Plasma installation . Refer to configuration below to apply external cache strategy and start Plasma service on each node and start your workload.","title":"Prerequisites"},{"location":"SQLDSCache/#configuration-for-numa","text":"Install numactl to bind the executor to the PMem device on the same NUMA node. yum install numactl -y We recommend you use NUMA-patched Spark to achieve better performance gain for the external strategy compared with Community Spark. Build Spark from source to enable NUMA-binding support, refer to Enabling-NUMA-binding-for-PMem-in-Spark .","title":"Configuration for NUMA"},{"location":"SQLDSCache/#configuration-for-enabling-pmem-cache","text":"Add the following configuration to $SPARK_HOME/conf/spark-defaults.conf . # 2x number of your worker nodes spark.executor.instances 6 # enable numa spark.yarn.numa.enabled true # enable SQL Index and Data Source Cache extension in Spark spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node, when in Yarn client mode spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir, when in Yarn client mode spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node,when in Yarn client mode spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # for parquet file format, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for ORC file format, enable binary cache spark.sql.oap.orc.binary.cache.enabled true # enable external cache strategy spark.oap.cache.strategy external spark.sql.oap.dcpmm.free.wait.threshold 50000000000 # according to your executor core number spark.executor.sql.oap.cache.external.client.pool.size 10 Start Plasma service manually Plasma config parameters: -m how much Bytes share memory Plasma will use -s Unix Domain sockcet path -d PMem directory Start Plasma service on each node with following command, then run your workload. If you install OAP by Conda, you can find plasma-store-server in the path $HOME/miniconda2/envs/oapenv/bin/ . ./plasma-store-server -m 15000000000 -s /tmp/plasmaStore -d /mnt/pmem Remember to kill plasma-store-server process if you no longer need cache, and you should delete /tmp/plasmaStore which is a Unix domain socket. Use Yarn to start Plamsa service When using Yarn(Hadoop version >= 3.1) to start Plasma service, you should provide a json file as below. { \"name\": \"plasma-store-service\", \"version\": 1, \"components\" : [ { \"name\": \"plasma-store-service\", \"number_of_containers\": 3, \"launch_command\": \"plasma-store-server -m 15000000000 -s /tmp/plasmaStore -d /mnt/pmem\", \"resource\": { \"cpus\": 1, \"memory\": 512 } } ] } Run command yarn app -launch plasma-store-service /tmp/plasmaLaunch.json to start Plasma server. Run yarn app -stop plasma-store-service to stop it. Run yarn app -destroy plasma-store-service to destroy it.","title":"Configuration for enabling PMem cache"},{"location":"SQLDSCache/#verify-pmem-cache-functionality","text":"After finishing configuration, restart Spark Thrift Server for the configuration changes to take effect. Start at step 2 of the Use DRAM Cache guide to verify that cache is working correctly. Check PMem cache size by checking disk space with df -h .","title":"Verify PMem cache functionality"},{"location":"SQLDSCache/#run-tpc-ds-benchmark","text":"This section provides instructions and tools for running TPC-DS queries to evaluate the cache performance of various configurations. The TPC-DS suite has many queries and we select 9 I/O intensive queries to simplify performance evaluation. We created some tool scripts oap-benchmark-tool.zip to simplify running the workload. If you are already familiar with TPC-DS data generation and running a TPC-DS tool suite, skip our tool and use the TPC-DS tool suite directly.","title":"Run TPC-DS Benchmark"},{"location":"SQLDSCache/#prerequisites_2","text":"Python 2.7+ is required on the working node.","title":"Prerequisites"},{"location":"SQLDSCache/#prepare-the-tool","text":"Download oap-benchmark-tool.zip and unzip to a folder (for example, oap-benchmark-tool folder) on your working node. Copy oap-benchmark-tool/tools/tpcds-kits to ALL worker nodes under the same folder (for example, /home/oap/tpcds-kits ).","title":"Prepare the Tool"},{"location":"SQLDSCache/#generate-tpc-ds-data","text":"Update the values for the following variables in oap-benchmark-tool/scripts/tool.conf based on your environment and needs. SPARK_HOME: Point to the Spark home directory of your Spark setup. TPCDS_KITS_DIR: The tpcds-kits directory you coped to the worker nodes in the above prepare process. For example, /home/oap/tpcds-kits NAMENODE_ADDRESS: Your HDFS Namenode address in the format of host:port. THRIFT_SERVER_ADDRESS: Your working node address on which you will run Thrift Server. DATA_SCALE: The data scale to be generated in GB DATA_FORMAT: The data file format. You can specify parquet or orc For example: export SPARK_HOME=/home/oap/spark-3.0.0 export TPCDS_KITS_DIR=/home/oap/tpcds-kits export NAMENODE_ADDRESS=mynamenode:9000 export THRIFT_SERVER_ADDRESS=mythriftserver export DATA_SCALE=1024 export DATA_FORMAT=parquet Start data generation. In the root directory of this tool ( oap-benchmark-tool ), run scripts/run_gen_data.sh to start the data generation process. cd oap-benchmark-tool sh ./scripts/run_gen_data.sh Once finished, the $scale data will be generated in the HDFS folder genData$scale . And a database called tpcds_$format$scale will contain the TPC-DS tables.","title":"Generate TPC-DS Data"},{"location":"SQLDSCache/#start-spark-thrift-server","text":"Start the Thrift Server in the tool root folder, which is the same folder you run data generation scripts. Use either the PMem or DRAM script to start the Thrift Server.","title":"Start Spark Thrift Server"},{"location":"SQLDSCache/#use-pmem-as-cache-media","text":"Update the configuration values in scripts/spark_thrift_server_yarn_with_PMem.sh to reflect your environment. Normally, you need to update the following configuration values to cache to PMem. --num-executors --driver-memory --executor-memory --executor-cores --conf spark.oap.cache.strategy --conf spark.sql.oap.dcpmm.free.wait.threshold --conf spark.executor.sql.oap.cache.external.client.pool.size These settings will override the values specified in Spark configuration file ( spark-defaults.conf ). After the configuration is done, you can execute the following command to start Thrift Server. cd oap-benchmark-tool sh ./scripts/spark_thrift_server_yarn_with_PMem.sh start In this script, we use external as cache strategy for Parquet Binary data cache.","title":"Use PMem as Cache Media"},{"location":"SQLDSCache/#use-dram-as-cache-media","text":"Update the configuration values in scripts/spark_thrift_server_yarn_with_DRAM.sh to reflect your environment. Normally, you need to update the following configuration values to cache to DRAM. --num-executors --driver-memory --executor-memory --executor-cores --conf spark.executor.sql.oap.cache.offheap.memory.size --conf spark.executor.memoryOverhead These settings will override the values specified in Spark configuration file ( spark-defaults.conf ). After the configuration is done, you can execute the following command to start Thrift Server. cd oap-benchmark-tool sh ./scripts/spark_thrift_server_yarn_with_DRAM.sh start","title":"Use DRAM as Cache Media"},{"location":"SQLDSCache/#run-queries","text":"Execute the following command to start to run queries. If you use external cache strategy, also need start plasma service manually as above. cd oap-benchmark-tool sh ./scripts/run_tpcds.sh When all the queries are done, you will see the result.json file in the current directory. You will find in the 2nd and 3rd round, cache feature takes effect and query time becomes less. And the Spark webUI OAP tab has more specific OAP cache metrics just as section step 5.","title":"Run Queries"},{"location":"SQLDSCache/#advanced-configuration","text":"Additional Cache Strategies In addition to external cache strategy, SQL Data Source Cache also supports 3 other cache strategies: guava , noevict and vmemcache . Index and Data Cache Separation To optimize the cache media utilization, SQL Data Source Cache supports cache separation of data and index, by using same or different cache media with DRAM and PMem. Cache Hot Tables Data Source Cache also supports caching specific tables according to actual situations, these tables are usually hot tables. Column Vector Cache This document above uses binary cache as example for Parquet file format, if your cluster memory resources is abundant enough, you can choose ColumnVector data cache instead of binary cache for Parquet to spare computation time. Large Scale and Heterogeneous Cluster Support Introduce an external database to store cache locality info to support large-scale and heterogeneous clusters. For more information and configuration details, please refer to Advanced Configuration .","title":"Advanced Configuration"},{"location":"SQLDSCache/Advanced-Configuration/","text":"Advanced Configuration In addition to usage information provided in User Guide , we provide more strategies for SQL Index and Data Source Cache in this section. Their needed dependencies like Memkind , Vmemcache and Plasma can be automatically installed when following OAP Installation Guide , corresponding feature jars can be found under $HOME/miniconda2/envs/oapenv/oap_jars . Additional Cache Strategies In addition to external cache strategy, SQL Data Source Cache also supports 3 other cache strategies: guava , noevict and vmemcache . Index and Data Cache Separation To optimize the cache media utilization, SQL Data Source Cache supports cache separation of data and index, by using same or different cache media with DRAM and PMem. Cache Hot Tables Data Source Cache also supports caching specific tables according to actual situations, these tables are usually hot tables. Column Vector Cache This document above uses binary cache as example for Parquet file format, if your cluster memory resources is abundant enough, you can choose ColumnVector data cache instead of binary cache for Parquet to spare computation time. Large Scale and Heterogeneous Cluster Support Introduce an external database to store cache locality info to support large-scale and heterogeneous clusters. Additional Cache Strategies Following table shows features of 4 cache strategies on PMem. guava noevict vmemcache external cache Use memkind lib to operate on PMem and guava cache strategy when data eviction happens. Use memkind lib to operate on PMem and doesn't allow data eviction. Use vmemcache lib to operate on PMem and LRU cache strategy when data eviction happens. Use Plasma/dlmalloc to operate on PMem and LRU cache strategy when data eviction happens. Need numa patch in Spark for better performance. Need numa patch in Spark for better performance. Need numa patch in Spark for better performance. Doesn't need numa patch. Suggest using 2 executors one node to keep aligned with PMem paths and numa nodes number. Suggest using 2 executors one node to keep aligned with PMem paths and numa nodes number. Suggest using 2 executors one node to keep aligned with PMem paths and numa nodes number. Node-level cache so there are no limitation for executor number. Cache data cleaned once executors exited. Cache data cleaned once executors exited. Cache data cleaned once executors exited. No data loss when executors exit thus is friendly to dynamic allocation. But currently it has performance overhead than other cache solutions. For cache solution guava/noevict , make sure Memkind library installed on every cluster worker node. If you have finished OAP Installation Guide , libmemkind will be installed. Or manually build and install it following memkind-installation , then place libmemkind.so.0 under /lib64/ on each worker node. For cache solution vmemcahe/external cache, make sure Vmemcache library has been installed on every cluster worker node. If you have finished OAP Installation Guide , libvmemcache will be installed. Or you can follow the vmemcache-installation steps and make sure libvmemcache.so.0 exist under /lib64/ directory on each worker node. If you have followed OAP Installation Guide , Memkind , Vmemcache and Plasma will be automatically installed. Or you can refer to OAP Developer-Guide , there is a shell script to help you install these dependencies automatically. Use PMem Cache Prerequisites The following are required to configure OAP to use PMem cache. PMem hardware is successfully deployed on each node in cluster. Directories exposing PMem hardware on each socket. For example, on a two socket system the mounted PMem directories should appear as /mnt/pmem0 and /mnt/pmem1 . Correctly installed PMem must be formatted and mounted on every cluster worker node. You can follow these commands to destroy interleaved PMem device which you set in User-Guide : # destroy interleaved PMem device which you set when using external cache strategy umount /mnt/pmem dmsetup remove striped-pmem echo y | mkfs.ext4 /dev/pmem0 echo y | mkfs.ext4 /dev/pmem1 mkdir -p /mnt/pmem0 mkdir -p /mnt/pmem1 mount -o dax /dev/pmem0 /mnt/pmem0 mount -o dax /dev/pmem1 /mnt/pmem1 In this case file systems are generated for 2 NUMA nodes, which can be checked by \"numactl --hardware\". For a different number of NUMA nodes, a corresponding number of namespaces should be created to assure correct file system paths mapping to NUMA nodes. For more information you can refer to Quick Start Guide: Provision Intel\u00ae Optane\u2122 DC Persistent Memory Configuration for NUMA Install numactl to bind the executor to the PMem device on the same NUMA node. yum install numactl -y We strongly recommend you use NUMA-patched Spark to achieve better performance gain for the following 3 cache strategies. Besides, currently using Community Spark occasionally has the problem of two executors being bound to the same PMem path. Build Spark from source to enable NUMA-binding support, refer to Enabling-NUMA-binding-for-PMem-in-Spark . Configuration for PMem Create persistent-memory.xml under $SPARK_HOME/conf if it doesn't exist. Use the following template and change the initialPath to your mounted paths for PMem devices. <persistentMemoryPool> <!--The numa id--> <numanode id=\"0\"> <!--The initial path for Intel Optane DC persistent memory--> <initialPath>/mnt/pmem0</initialPath> </numanode> <numanode id=\"1\"> <initialPath>/mnt/pmem1</initialPath> </numanode> </persistentMemoryPool> Guava cache Guava cache is based on memkind library, built on top of jemalloc and provides memory characteristics. To use it in your workload, follow prerequisites to set up PMem hardware correctly, also make sure memkind library installed. Then follow configurations below. NOTE : spark.executor.sql.oap.cache.persistent.memory.reserved.size : When we use PMem as memory through memkind library, some portion of the space needs to be reserved for memory management overhead, such as memory segmentation. We suggest reserving 20% - 25% of the available PMem capacity to avoid memory allocation failure. But even with an allocation failure, OAP will continue the operation to read data from original input data and will not cache the data block. # enable numa spark.yarn.numa.enabled true spark.executorEnv.MEMKIND_ARENA_NUM_PER_KIND 1 # for Parquet file format, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for ORC file format, enable binary cache spark.sql.oap.orc.binary.cache.enabled true spark.sql.oap.cache.memory.manager pm spark.oap.cache.strategy guava # PMem capacity per executor, according to your cluster spark.executor.sql.oap.cache.persistent.memory.initial.size 256g # Reserved space per executor, according to your cluster spark.executor.sql.oap.cache.persistent.memory.reserved.size 50g # enable SQL Index and Data Source Cache jar in Spark spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node, when in Yarn client mode spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir, when in Yarn client mode spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node,when in Yarn client mode spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar Memkind library also support DAX KMEM mode. Refer to Kernel , this chapter will guide how to configure PMem as system RAM. Or Memkind support for KMEM DAX option for more details. Please note that DAX KMEM mode need kernel version 5.x and memkind version 1.10 or above. If you choose KMEM mode, change memory manager from pm to kmem as below. spark.sql.oap.cache.memory.manager kmem Noevict cache The noevict cache strategy is also supported in OAP based on the memkind library for PMem. To use it in your workload, follow prerequisites to set up PMem hardware correctly, also make sure memkind library installed. Then follow the configuration below. # enable numa spark.yarn.numa.enabled true spark.executorEnv.MEMKIND_ARENA_NUM_PER_KIND 1 # for Parquet file format, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for ORC file format, enable binary cache spark.sql.oap.orc.binary.cache.enabled true spark.oap.cache.strategy noevict spark.executor.sql.oap.cache.persistent.memory.initial.size 256g # Enable OAP extension in Spark spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node, when in Yarn client mode spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir, when in Yarn client mode spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node,when in Yarn client mode spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar Vmemcache Make sure Vmemcache library has been installed on every cluster worker node if vmemcache strategy is chosen for PMem cache. If you have finished OAP-Installation-Guide , vmemcache library will be automatically installed by Conda. Or you can follow the build/install steps and make sure libvmemcache.so exist in /lib64 directory on each worker node. - To use it in your workload, follow prerequisites to set up PMem hardware correctly. Configure to enable PMem cache Make the following configuration changes in $SPARK_HOME/conf/spark-defaults.conf . # 2x number of your worker nodes spark.executor.instances 6 # enable numa spark.yarn.numa.enabled true # Enable OAP extension in Spark spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node, when in Yarn client mode spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir, when in Yarn client mode spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node,when in Yarn client mode spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # for parquet file format, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for ORC file format, enable binary cache spark.sql.oap.orc.binary.cache.enabled true # enable vmemcache strategy spark.oap.cache.strategy vmem spark.executor.sql.oap.cache.persistent.memory.initial.size 256g # according to your cluster spark.executor.sql.oap.cache.guardian.memory.size 10g The vmem cache strategy is based on libvmemcache (buffer based LRU cache), which provides a key-value store API. Follow these steps to enable vmemcache support in Data Source Cache. spark.executor.instances : We suggest setting the value to 2X the number of worker nodes when NUMA binding is enabled. Each worker node runs two executors, each executor is bound to one of the two sockets, and accesses the corresponding PMem device on that socket. spark.executor.sql.oap.cache.persistent.memory.initial.size : It is configured to the available PMem capacity to be used as data cache per exectutor. NOTE : If \"PendingFiber Size\" (on spark web-UI OAP page) is large, or some tasks fail with \"cache guardian use too much memory\" error, set spark.executor.sql.oap.cache.guardian.memory.size to a larger number as the default size is 10GB. The user could also increase spark.sql.oap.cache.guardian.free.thread.nums or decrease spark.sql.oap.cache.dispose.timeout.ms to free memory more quickly. Verify PMem cache functionality After finishing configuration, restart Spark Thrift Server for the configuration changes to take effect. Start at step 2 of the Use DRAM Cache guide to verify that cache is working correctly. Verify NUMA binding status by confirming keywords like numactl --cpubind=1 --membind=1 contained in executor launch command. Check PMem cache size by checking disk space with df -h .For vmemcache strategy, disk usage will reach the initial cache size once the PMem cache is initialized and will not change during workload execution. For Guava/Noevict strategies, the command will show disk space usage increases along with workload execution. Index and Data Cache Separation SQL Index and Data Source Cache now supports different cache strategies for DRAM and PMem. To optimize the cache media utilization, you can enable cache separation of data and index with same or different cache media. When Sharing same media, data cache and index cache will use different fiber cache ratio. Here we list 4 different kinds of configuration for index/cache separation, if you choose one of them, please add corresponding configuration to spark-defaults.conf . 1. DRAM as cache media, guava strategy as index & data cache backend. spark.sql.oap.index.data.cache.separation.enabled true spark.oap.cache.strategy mix spark.sql.oap.cache.memory.manager offheap The rest configuration you can refer to Use DRAM Cache PMem as cache media, external strategy as index & data cache backend. spark.sql.oap.index.data.cache.separation.enabled true spark.oap.cache.strategy mix spark.sql.oap.cache.memory.manager tmp spark.sql.oap.mix.data.cache.backend external spark.sql.oap.mix.index.cache.backend external The rest configurations can refer to the configurations of PMem Cache and External cache DRAM( offheap )/ guava as index cache media and backend, PMem( tmp )/ external as data cache media and backend. spark.sql.oap.index.data.cache.separation.enabled true spark.oap.cache.strategy mix spark.sql.oap.cache.memory.manager mix spark.sql.oap.mix.data.cache.backend external # 2x number of your worker nodes spark.executor.instances 6 # enable numa spark.yarn.numa.enabled true spark.memory.offHeap.enabled false spark.sql.oap.dcpmm.free.wait.threshold 50000000000 # according to your executor core number spark.executor.sql.oap.cache.external.client.pool.size 10 # equal to the size of executor.memoryOverhead spark.executor.sql.oap.cache.offheap.memory.size 50g # according to the resource of cluster spark.executor.memoryOverhead 50g # for ORC file format spark.sql.oap.orc.binary.cache.enabled true # for Parquet file format spark.sql.oap.parquet.binary.cache.enabled true DRAM( offheap )/ guava as index cache media and backend, PMem( pm )/ guava as data cache media and backend. spark.sql.oap.index.data.cache.separation.enabled true spark.oap.cache.strategy mix spark.sql.oap.cache.memory.manager mix # 2x number of your worker nodes spark.executor.instances 6 # enable numa spark.yarn.numa.enabled true spark.executorEnv.MEMKIND_ARENA_NUM_PER_KIND 1 spark.memory.offHeap.enabled false # PMem capacity per executor spark.executor.sql.oap.cache.persistent.memory.initial.size 256g # Reserved space per executor spark.executor.sql.oap.cache.persistent.memory.reserved.size 50g # equal to the size of executor.memoryOverhead spark.executor.sql.oap.cache.offheap.memory.size 50g # according to the resource of cluster spark.executor.memoryOverhead 50g # for ORC file format spark.sql.oap.orc.binary.cache.enabled true # for Parquet file format spark.sql.oap.parquet.binary.cache.enabled true Cache Hot Tables Data Source Cache also supports caching specific tables by configuring items according to actual situations, these tables are usually hot tables. To enable caching specific hot tables, you can add the configuration below to spark-defaults.conf . # enable table lists fiberCache spark.sql.oap.cache.table.list.enabled true # Table lists using fiberCache actively spark.sql.oap.cache.table.list <databasename>.<tablename1>;<databasename>.<tablename2> Column Vector Cache This document above use binary cache for Parquet as example, cause binary cache can improve cache space utilization compared to ColumnVector cache. When your cluster memory resources are abundant enough, you can choose ColumnVector cache to spare computation time. To enable ColumnVector data cache for Parquet file format, you should add the configuration below to spark-defaults.conf . # for parquet file format, disable binary cache spark.sql.oap.parquet.binary.cache.enabled false # for parquet file format, enable ColumnVector cache spark.sql.oap.parquet.data.cache.enabled true Large Scale and Heterogeneous Cluster Support NOTE: Only works with external cache OAP influences Spark to schedule tasks according to cache locality info. This info could be of large amount in a large scale cluster , and how to schedule tasks in a heterogeneous cluster (some nodes with PMem, some without) could also be challenging. We introduce an external DB to store cache locality info. If there's no cache available, Spark will fall back to schedule respecting HDFS locality. Currently we support Redis as external DB service. Please download and launch a redis-server before running Spark with OAP. Please add the following configurations to spark-defaults.conf . spark.sql.oap.external.cache.metaDB.enabled true # Redis-server address spark.sql.oap.external.cache.metaDB.address 10.1.2.12 spark.sql.oap.external.cache.metaDB.impl org.apache.spark.sql.execution.datasources.RedisClient","title":"Advanced Configuration"},{"location":"SQLDSCache/Advanced-Configuration/#advanced-configuration","text":"In addition to usage information provided in User Guide , we provide more strategies for SQL Index and Data Source Cache in this section. Their needed dependencies like Memkind , Vmemcache and Plasma can be automatically installed when following OAP Installation Guide , corresponding feature jars can be found under $HOME/miniconda2/envs/oapenv/oap_jars . Additional Cache Strategies In addition to external cache strategy, SQL Data Source Cache also supports 3 other cache strategies: guava , noevict and vmemcache . Index and Data Cache Separation To optimize the cache media utilization, SQL Data Source Cache supports cache separation of data and index, by using same or different cache media with DRAM and PMem. Cache Hot Tables Data Source Cache also supports caching specific tables according to actual situations, these tables are usually hot tables. Column Vector Cache This document above uses binary cache as example for Parquet file format, if your cluster memory resources is abundant enough, you can choose ColumnVector data cache instead of binary cache for Parquet to spare computation time. Large Scale and Heterogeneous Cluster Support Introduce an external database to store cache locality info to support large-scale and heterogeneous clusters.","title":"Advanced Configuration"},{"location":"SQLDSCache/Advanced-Configuration/#additional-cache-strategies","text":"Following table shows features of 4 cache strategies on PMem. guava noevict vmemcache external cache Use memkind lib to operate on PMem and guava cache strategy when data eviction happens. Use memkind lib to operate on PMem and doesn't allow data eviction. Use vmemcache lib to operate on PMem and LRU cache strategy when data eviction happens. Use Plasma/dlmalloc to operate on PMem and LRU cache strategy when data eviction happens. Need numa patch in Spark for better performance. Need numa patch in Spark for better performance. Need numa patch in Spark for better performance. Doesn't need numa patch. Suggest using 2 executors one node to keep aligned with PMem paths and numa nodes number. Suggest using 2 executors one node to keep aligned with PMem paths and numa nodes number. Suggest using 2 executors one node to keep aligned with PMem paths and numa nodes number. Node-level cache so there are no limitation for executor number. Cache data cleaned once executors exited. Cache data cleaned once executors exited. Cache data cleaned once executors exited. No data loss when executors exit thus is friendly to dynamic allocation. But currently it has performance overhead than other cache solutions. For cache solution guava/noevict , make sure Memkind library installed on every cluster worker node. If you have finished OAP Installation Guide , libmemkind will be installed. Or manually build and install it following memkind-installation , then place libmemkind.so.0 under /lib64/ on each worker node. For cache solution vmemcahe/external cache, make sure Vmemcache library has been installed on every cluster worker node. If you have finished OAP Installation Guide , libvmemcache will be installed. Or you can follow the vmemcache-installation steps and make sure libvmemcache.so.0 exist under /lib64/ directory on each worker node. If you have followed OAP Installation Guide , Memkind , Vmemcache and Plasma will be automatically installed. Or you can refer to OAP Developer-Guide , there is a shell script to help you install these dependencies automatically.","title":"Additional Cache Strategies"},{"location":"SQLDSCache/Advanced-Configuration/#use-pmem-cache","text":"","title":"Use PMem Cache"},{"location":"SQLDSCache/Advanced-Configuration/#prerequisites","text":"The following are required to configure OAP to use PMem cache. PMem hardware is successfully deployed on each node in cluster. Directories exposing PMem hardware on each socket. For example, on a two socket system the mounted PMem directories should appear as /mnt/pmem0 and /mnt/pmem1 . Correctly installed PMem must be formatted and mounted on every cluster worker node. You can follow these commands to destroy interleaved PMem device which you set in User-Guide : # destroy interleaved PMem device which you set when using external cache strategy umount /mnt/pmem dmsetup remove striped-pmem echo y | mkfs.ext4 /dev/pmem0 echo y | mkfs.ext4 /dev/pmem1 mkdir -p /mnt/pmem0 mkdir -p /mnt/pmem1 mount -o dax /dev/pmem0 /mnt/pmem0 mount -o dax /dev/pmem1 /mnt/pmem1 In this case file systems are generated for 2 NUMA nodes, which can be checked by \"numactl --hardware\". For a different number of NUMA nodes, a corresponding number of namespaces should be created to assure correct file system paths mapping to NUMA nodes. For more information you can refer to Quick Start Guide: Provision Intel\u00ae Optane\u2122 DC Persistent Memory","title":"Prerequisites"},{"location":"SQLDSCache/Advanced-Configuration/#configuration-for-numa","text":"Install numactl to bind the executor to the PMem device on the same NUMA node. yum install numactl -y We strongly recommend you use NUMA-patched Spark to achieve better performance gain for the following 3 cache strategies. Besides, currently using Community Spark occasionally has the problem of two executors being bound to the same PMem path. Build Spark from source to enable NUMA-binding support, refer to Enabling-NUMA-binding-for-PMem-in-Spark .","title":"Configuration for NUMA"},{"location":"SQLDSCache/Advanced-Configuration/#configuration-for-pmem","text":"Create persistent-memory.xml under $SPARK_HOME/conf if it doesn't exist. Use the following template and change the initialPath to your mounted paths for PMem devices. <persistentMemoryPool> <!--The numa id--> <numanode id=\"0\"> <!--The initial path for Intel Optane DC persistent memory--> <initialPath>/mnt/pmem0</initialPath> </numanode> <numanode id=\"1\"> <initialPath>/mnt/pmem1</initialPath> </numanode> </persistentMemoryPool>","title":"Configuration for PMem"},{"location":"SQLDSCache/Advanced-Configuration/#guava-cache","text":"Guava cache is based on memkind library, built on top of jemalloc and provides memory characteristics. To use it in your workload, follow prerequisites to set up PMem hardware correctly, also make sure memkind library installed. Then follow configurations below. NOTE : spark.executor.sql.oap.cache.persistent.memory.reserved.size : When we use PMem as memory through memkind library, some portion of the space needs to be reserved for memory management overhead, such as memory segmentation. We suggest reserving 20% - 25% of the available PMem capacity to avoid memory allocation failure. But even with an allocation failure, OAP will continue the operation to read data from original input data and will not cache the data block. # enable numa spark.yarn.numa.enabled true spark.executorEnv.MEMKIND_ARENA_NUM_PER_KIND 1 # for Parquet file format, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for ORC file format, enable binary cache spark.sql.oap.orc.binary.cache.enabled true spark.sql.oap.cache.memory.manager pm spark.oap.cache.strategy guava # PMem capacity per executor, according to your cluster spark.executor.sql.oap.cache.persistent.memory.initial.size 256g # Reserved space per executor, according to your cluster spark.executor.sql.oap.cache.persistent.memory.reserved.size 50g # enable SQL Index and Data Source Cache jar in Spark spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node, when in Yarn client mode spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir, when in Yarn client mode spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node,when in Yarn client mode spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar Memkind library also support DAX KMEM mode. Refer to Kernel , this chapter will guide how to configure PMem as system RAM. Or Memkind support for KMEM DAX option for more details. Please note that DAX KMEM mode need kernel version 5.x and memkind version 1.10 or above. If you choose KMEM mode, change memory manager from pm to kmem as below. spark.sql.oap.cache.memory.manager kmem","title":"Guava cache"},{"location":"SQLDSCache/Advanced-Configuration/#noevict-cache","text":"The noevict cache strategy is also supported in OAP based on the memkind library for PMem. To use it in your workload, follow prerequisites to set up PMem hardware correctly, also make sure memkind library installed. Then follow the configuration below. # enable numa spark.yarn.numa.enabled true spark.executorEnv.MEMKIND_ARENA_NUM_PER_KIND 1 # for Parquet file format, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for ORC file format, enable binary cache spark.sql.oap.orc.binary.cache.enabled true spark.oap.cache.strategy noevict spark.executor.sql.oap.cache.persistent.memory.initial.size 256g # Enable OAP extension in Spark spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node, when in Yarn client mode spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir, when in Yarn client mode spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node,when in Yarn client mode spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar","title":"Noevict cache"},{"location":"SQLDSCache/Advanced-Configuration/#vmemcache","text":"Make sure Vmemcache library has been installed on every cluster worker node if vmemcache strategy is chosen for PMem cache. If you have finished OAP-Installation-Guide , vmemcache library will be automatically installed by Conda. Or you can follow the build/install steps and make sure libvmemcache.so exist in /lib64 directory on each worker node. - To use it in your workload, follow prerequisites to set up PMem hardware correctly.","title":"Vmemcache"},{"location":"SQLDSCache/Advanced-Configuration/#configure-to-enable-pmem-cache","text":"Make the following configuration changes in $SPARK_HOME/conf/spark-defaults.conf . # 2x number of your worker nodes spark.executor.instances 6 # enable numa spark.yarn.numa.enabled true # Enable OAP extension in Spark spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node, when in Yarn client mode spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir, when in Yarn client mode spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node,when in Yarn client mode spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # for parquet file format, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for ORC file format, enable binary cache spark.sql.oap.orc.binary.cache.enabled true # enable vmemcache strategy spark.oap.cache.strategy vmem spark.executor.sql.oap.cache.persistent.memory.initial.size 256g # according to your cluster spark.executor.sql.oap.cache.guardian.memory.size 10g The vmem cache strategy is based on libvmemcache (buffer based LRU cache), which provides a key-value store API. Follow these steps to enable vmemcache support in Data Source Cache. spark.executor.instances : We suggest setting the value to 2X the number of worker nodes when NUMA binding is enabled. Each worker node runs two executors, each executor is bound to one of the two sockets, and accesses the corresponding PMem device on that socket. spark.executor.sql.oap.cache.persistent.memory.initial.size : It is configured to the available PMem capacity to be used as data cache per exectutor. NOTE : If \"PendingFiber Size\" (on spark web-UI OAP page) is large, or some tasks fail with \"cache guardian use too much memory\" error, set spark.executor.sql.oap.cache.guardian.memory.size to a larger number as the default size is 10GB. The user could also increase spark.sql.oap.cache.guardian.free.thread.nums or decrease spark.sql.oap.cache.dispose.timeout.ms to free memory more quickly.","title":"Configure to enable PMem cache"},{"location":"SQLDSCache/Advanced-Configuration/#verify-pmem-cache-functionality","text":"After finishing configuration, restart Spark Thrift Server for the configuration changes to take effect. Start at step 2 of the Use DRAM Cache guide to verify that cache is working correctly. Verify NUMA binding status by confirming keywords like numactl --cpubind=1 --membind=1 contained in executor launch command. Check PMem cache size by checking disk space with df -h .For vmemcache strategy, disk usage will reach the initial cache size once the PMem cache is initialized and will not change during workload execution. For Guava/Noevict strategies, the command will show disk space usage increases along with workload execution.","title":"Verify PMem cache functionality"},{"location":"SQLDSCache/Advanced-Configuration/#index-and-data-cache-separation","text":"SQL Index and Data Source Cache now supports different cache strategies for DRAM and PMem. To optimize the cache media utilization, you can enable cache separation of data and index with same or different cache media. When Sharing same media, data cache and index cache will use different fiber cache ratio. Here we list 4 different kinds of configuration for index/cache separation, if you choose one of them, please add corresponding configuration to spark-defaults.conf . 1. DRAM as cache media, guava strategy as index & data cache backend. spark.sql.oap.index.data.cache.separation.enabled true spark.oap.cache.strategy mix spark.sql.oap.cache.memory.manager offheap The rest configuration you can refer to Use DRAM Cache PMem as cache media, external strategy as index & data cache backend. spark.sql.oap.index.data.cache.separation.enabled true spark.oap.cache.strategy mix spark.sql.oap.cache.memory.manager tmp spark.sql.oap.mix.data.cache.backend external spark.sql.oap.mix.index.cache.backend external The rest configurations can refer to the configurations of PMem Cache and External cache DRAM( offheap )/ guava as index cache media and backend, PMem( tmp )/ external as data cache media and backend. spark.sql.oap.index.data.cache.separation.enabled true spark.oap.cache.strategy mix spark.sql.oap.cache.memory.manager mix spark.sql.oap.mix.data.cache.backend external # 2x number of your worker nodes spark.executor.instances 6 # enable numa spark.yarn.numa.enabled true spark.memory.offHeap.enabled false spark.sql.oap.dcpmm.free.wait.threshold 50000000000 # according to your executor core number spark.executor.sql.oap.cache.external.client.pool.size 10 # equal to the size of executor.memoryOverhead spark.executor.sql.oap.cache.offheap.memory.size 50g # according to the resource of cluster spark.executor.memoryOverhead 50g # for ORC file format spark.sql.oap.orc.binary.cache.enabled true # for Parquet file format spark.sql.oap.parquet.binary.cache.enabled true DRAM( offheap )/ guava as index cache media and backend, PMem( pm )/ guava as data cache media and backend. spark.sql.oap.index.data.cache.separation.enabled true spark.oap.cache.strategy mix spark.sql.oap.cache.memory.manager mix # 2x number of your worker nodes spark.executor.instances 6 # enable numa spark.yarn.numa.enabled true spark.executorEnv.MEMKIND_ARENA_NUM_PER_KIND 1 spark.memory.offHeap.enabled false # PMem capacity per executor spark.executor.sql.oap.cache.persistent.memory.initial.size 256g # Reserved space per executor spark.executor.sql.oap.cache.persistent.memory.reserved.size 50g # equal to the size of executor.memoryOverhead spark.executor.sql.oap.cache.offheap.memory.size 50g # according to the resource of cluster spark.executor.memoryOverhead 50g # for ORC file format spark.sql.oap.orc.binary.cache.enabled true # for Parquet file format spark.sql.oap.parquet.binary.cache.enabled true","title":"Index and Data Cache Separation"},{"location":"SQLDSCache/Advanced-Configuration/#cache-hot-tables","text":"Data Source Cache also supports caching specific tables by configuring items according to actual situations, these tables are usually hot tables. To enable caching specific hot tables, you can add the configuration below to spark-defaults.conf . # enable table lists fiberCache spark.sql.oap.cache.table.list.enabled true # Table lists using fiberCache actively spark.sql.oap.cache.table.list <databasename>.<tablename1>;<databasename>.<tablename2>","title":"Cache Hot Tables"},{"location":"SQLDSCache/Advanced-Configuration/#column-vector-cache","text":"This document above use binary cache for Parquet as example, cause binary cache can improve cache space utilization compared to ColumnVector cache. When your cluster memory resources are abundant enough, you can choose ColumnVector cache to spare computation time. To enable ColumnVector data cache for Parquet file format, you should add the configuration below to spark-defaults.conf . # for parquet file format, disable binary cache spark.sql.oap.parquet.binary.cache.enabled false # for parquet file format, enable ColumnVector cache spark.sql.oap.parquet.data.cache.enabled true","title":"Column Vector Cache"},{"location":"SQLDSCache/Advanced-Configuration/#large-scale-and-heterogeneous-cluster-support","text":"NOTE: Only works with external cache OAP influences Spark to schedule tasks according to cache locality info. This info could be of large amount in a large scale cluster , and how to schedule tasks in a heterogeneous cluster (some nodes with PMem, some without) could also be challenging. We introduce an external DB to store cache locality info. If there's no cache available, Spark will fall back to schedule respecting HDFS locality. Currently we support Redis as external DB service. Please download and launch a redis-server before running Spark with OAP. Please add the following configurations to spark-defaults.conf . spark.sql.oap.external.cache.metaDB.enabled true # Redis-server address spark.sql.oap.external.cache.metaDB.address 10.1.2.12 spark.sql.oap.external.cache.metaDB.impl org.apache.spark.sql.execution.datasources.RedisClient","title":"Large Scale and Heterogeneous Cluster Support"},{"location":"SQLDSCache/Architect-Overview/","text":"Architecture Overview Introduction Usage Scenarios Architecture Features Introduction Apache Spark is a unified analytics engine for large-scale data processing, and Spark SQL is a Spark module widely used to process structured data in data center. However, Spark SQL still faces the challenge of stability and performance in a highly dynamic environment with ultra-large-scale data. SQL Index and Data Source Cache are designed to leverage users' defined indices and smart fine-grained in-memory data caching to boost Spark SQL performance, and can address the performance issues of some use cases. Usage Scenarios Usage Scenario 1 -- Interactive queries Most customers adopt Spark SQL as a batch processing engine. Unfortunately, customers would find it hard to separate batch processing and interactive use cases. Interactive queries need to return the data in seconds or even sub-seconds instead of the minutes or hours of batch processing. This is a big challenge for the current Spark SQL implementation. For example, the following interactive query attempts to filter out a very small result set from a huge fact table. select ss_sold_date_sk, ss_sold_time_sk, ss_item_sk, ss_cdemo_sk, ss_store_sk, ss_ticket_number, ss_ext_discount_amt, ss_ext_wholesale_cost, ss_ext_tax from fact.ss_sales where (date='20200801' and ss_customer='xxx' and ss_item_sk='806486\u2019) limit 10 Interactive queries usually process a large data set but return a small portion of data filtering for a specific condition. By creating and storing a full B+ Tree index for key columns and using a smart fine-grained in-memory data caching strategy, we can boost Spark SQL interactive queries to seconds and even sub-seconds. By properly using index and cache, the performance of some interactive queries can possibly be improved by order of magnitude. Usage Scenario 2 -- Batch processing jobs Customers usually use Spark SQL for Business Analytics in Data Warehousing. SQL Data Source Cache can speed up batch processing jobs with 2 cache strategies: Automatically cache hot data. Specifically cache hot tables. Users can choose either strategy based on their need. Architecture The following diagram shows the design architecture. SQL Index and Data Source Cache acts as a .jar plug-in for Spark SQL. We designed the compatible adapter layer for three columnar storage file formats: Parquet ORC oap(Parquet-like file format defined by OAP). SQL Index and Data Source Cache have a Unified Cache Representation for different columnar storage formats and a fine-grained cache unit for one column of a RowGroup. 2 major optimization functions (indexing and caching) are based on unified representation and the adapter. Indices can be created on one or multiple columns of a data file. Data Source Cache can cache both decompressed and decoded vectorized data and binary raw data. Generally, the server's DRAM is used as the cache medium. PMem can also be used as the cache medium as it will provide a more cost effective solution for the requirements of a high performance environment. Both indexing and caching as Optimizer & Execution are transparent for users. See the Features section for details. Spark ThriftServer * is a good use case for OAP, because ThriftServer launches Spark Applications which can cache hot data for a long time in the background, and it also accepts query requests from different clients at the same time. bin/spark-sql , bin/spark-shell or bin/pyspark can certainly be used by SQL Index and Data Source Cache, but usually only for interactive test situations. Features Use indexing and caching to improve Spark SQL performance on ad-hoc queries and batch processing jobs. Indexing Users can use SQL DDL(create/drop/refresh/check/show index) to use indexing. Once users create indices using DDL, index files are generated in a specific directory and mainly composed of index data and statistics. When queries are executed, analyzing index files to boost performance is transparent to users. BTREE, BITMAP Index is an optimization that is widely used in traditional databases. We also adopt these two index types in the project. BTREE indexing is intended for datasets that have a lot of distinct values, and are distributed randomly, such as telephone numbers or ID numbers. BitMap index is intended for datasets with a limited total amount of distinct values, such as state or age. Statistics are located in the index file after all the index data are written into the index file. Sometimes, reading indices could bring extra cost for some queries. So we also support 4 statistics (MinMax, Bloom Filter, SampleBase and PartByValue) to help filter. With statistics, we can make sure we only use indices if we can possibly improve the execution. Caching Caching is another core feature of OAP. It is also transparent to users. Data Source Cache can automatically load frequently queried (hot) data, and evict data automatically according to the LRU policy when cache is full. Data Source Cache has the following characteristics: Off-Heap memory. The Data Source Cache uses off-heap memory and avoids the JVM GC. It can also use PMem as high-performance, high-capacity, low-cost memory Cache-Locality. Data Source Cache can schedule computing tasks to the executor which holds needed data in cache, by implementing a cache aware mechanism based on Spark driver and executors communication. Cache granularity. A column in one RowGroup (equivalent to Stripe in ORC) of a column-oriented storage format file is loaded into a basic cache unit which is called a \"Fiber\" in OAP. Cache Eviction. Data Source Cache cache eviction uses LRU policy, and automatically caches and evicts data transparently to end user. Cache configured tables. Data Source Cache also supports caching specific tables by configuring items according to actual situations, these tables are usually hot tables. *Other names and brands may be claimed as the property of others.","title":"Architecture Overview"},{"location":"SQLDSCache/Architect-Overview/#architecture-overview","text":"Introduction Usage Scenarios Architecture Features","title":"Architecture Overview"},{"location":"SQLDSCache/Architect-Overview/#introduction","text":"Apache Spark is a unified analytics engine for large-scale data processing, and Spark SQL is a Spark module widely used to process structured data in data center. However, Spark SQL still faces the challenge of stability and performance in a highly dynamic environment with ultra-large-scale data. SQL Index and Data Source Cache are designed to leverage users' defined indices and smart fine-grained in-memory data caching to boost Spark SQL performance, and can address the performance issues of some use cases.","title":"Introduction"},{"location":"SQLDSCache/Architect-Overview/#usage-scenarios","text":"","title":"Usage Scenarios"},{"location":"SQLDSCache/Architect-Overview/#usage-scenario-1-interactive-queries","text":"Most customers adopt Spark SQL as a batch processing engine. Unfortunately, customers would find it hard to separate batch processing and interactive use cases. Interactive queries need to return the data in seconds or even sub-seconds instead of the minutes or hours of batch processing. This is a big challenge for the current Spark SQL implementation. For example, the following interactive query attempts to filter out a very small result set from a huge fact table. select ss_sold_date_sk, ss_sold_time_sk, ss_item_sk, ss_cdemo_sk, ss_store_sk, ss_ticket_number, ss_ext_discount_amt, ss_ext_wholesale_cost, ss_ext_tax from fact.ss_sales where (date='20200801' and ss_customer='xxx' and ss_item_sk='806486\u2019) limit 10 Interactive queries usually process a large data set but return a small portion of data filtering for a specific condition. By creating and storing a full B+ Tree index for key columns and using a smart fine-grained in-memory data caching strategy, we can boost Spark SQL interactive queries to seconds and even sub-seconds. By properly using index and cache, the performance of some interactive queries can possibly be improved by order of magnitude.","title":"Usage Scenario 1 -- Interactive queries"},{"location":"SQLDSCache/Architect-Overview/#usage-scenario-2-batch-processing-jobs","text":"Customers usually use Spark SQL for Business Analytics in Data Warehousing. SQL Data Source Cache can speed up batch processing jobs with 2 cache strategies: Automatically cache hot data. Specifically cache hot tables. Users can choose either strategy based on their need.","title":"Usage Scenario 2 -- Batch processing jobs"},{"location":"SQLDSCache/Architect-Overview/#architecture","text":"The following diagram shows the design architecture. SQL Index and Data Source Cache acts as a .jar plug-in for Spark SQL. We designed the compatible adapter layer for three columnar storage file formats: Parquet ORC oap(Parquet-like file format defined by OAP). SQL Index and Data Source Cache have a Unified Cache Representation for different columnar storage formats and a fine-grained cache unit for one column of a RowGroup. 2 major optimization functions (indexing and caching) are based on unified representation and the adapter. Indices can be created on one or multiple columns of a data file. Data Source Cache can cache both decompressed and decoded vectorized data and binary raw data. Generally, the server's DRAM is used as the cache medium. PMem can also be used as the cache medium as it will provide a more cost effective solution for the requirements of a high performance environment. Both indexing and caching as Optimizer & Execution are transparent for users. See the Features section for details. Spark ThriftServer * is a good use case for OAP, because ThriftServer launches Spark Applications which can cache hot data for a long time in the background, and it also accepts query requests from different clients at the same time. bin/spark-sql , bin/spark-shell or bin/pyspark can certainly be used by SQL Index and Data Source Cache, but usually only for interactive test situations.","title":"Architecture"},{"location":"SQLDSCache/Architect-Overview/#features","text":"Use indexing and caching to improve Spark SQL performance on ad-hoc queries and batch processing jobs.","title":"Features"},{"location":"SQLDSCache/Architect-Overview/#indexing","text":"Users can use SQL DDL(create/drop/refresh/check/show index) to use indexing. Once users create indices using DDL, index files are generated in a specific directory and mainly composed of index data and statistics. When queries are executed, analyzing index files to boost performance is transparent to users. BTREE, BITMAP Index is an optimization that is widely used in traditional databases. We also adopt these two index types in the project. BTREE indexing is intended for datasets that have a lot of distinct values, and are distributed randomly, such as telephone numbers or ID numbers. BitMap index is intended for datasets with a limited total amount of distinct values, such as state or age. Statistics are located in the index file after all the index data are written into the index file. Sometimes, reading indices could bring extra cost for some queries. So we also support 4 statistics (MinMax, Bloom Filter, SampleBase and PartByValue) to help filter. With statistics, we can make sure we only use indices if we can possibly improve the execution.","title":"Indexing"},{"location":"SQLDSCache/Architect-Overview/#caching","text":"Caching is another core feature of OAP. It is also transparent to users. Data Source Cache can automatically load frequently queried (hot) data, and evict data automatically according to the LRU policy when cache is full. Data Source Cache has the following characteristics: Off-Heap memory. The Data Source Cache uses off-heap memory and avoids the JVM GC. It can also use PMem as high-performance, high-capacity, low-cost memory Cache-Locality. Data Source Cache can schedule computing tasks to the executor which holds needed data in cache, by implementing a cache aware mechanism based on Spark driver and executors communication. Cache granularity. A column in one RowGroup (equivalent to Stripe in ORC) of a column-oriented storage format file is loaded into a basic cache unit which is called a \"Fiber\" in OAP. Cache Eviction. Data Source Cache cache eviction uses LRU policy, and automatically caches and evicts data transparently to end user. Cache configured tables. Data Source Cache also supports caching specific tables by configuring items according to actual situations, these tables are usually hot tables.","title":"Caching"},{"location":"SQLDSCache/Architect-Overview/#other-names-and-brands-may-be-claimed-as-the-property-of-others","text":"","title":"*Other names and brands may be claimed as the property of others."},{"location":"SQLDSCache/Developer-Guide/","text":"Developer Guide This document is a supplement to the whole OAP Developer Guide for SQL Index and Data Source Cache. After following that document, you can continue more details for SQL Index and Data Source Cache. Building Enabling NUMA binding for Intel\u00ae Optane\u2122 DC Persistent Memory in Spark Building Building SQL Index and Data Source Cache Building with Apache Maven* . Clone the OAP project: git clone -b <tag-version> https://github.com/Intel-bigdata/OAP.git cd OAP Build the oap-cache package: mvn clean -pl com.intel.oap:oap-cache -am package Running Tests Run all the tests: mvn clean -pl com.intel.oap:oap-cache -am test Run a specific test suite, for example OapDDLSuite : mvn -pl com.intel.oap:oap-cache -am -DwildcardSuites=org.apache.spark.sql.execution.datasources.oap.OapDDLSuite test NOTE : Log level of unit tests currently default to ERROR, please override oap-cache/oap/src/test/resources/log4j.properties if needed. Building with Intel\u00ae Optane\u2122 DC Persistent Memory Module Prerequisites for building with PMem support Install the required packages on the build system: cmake memkind vmemcache Plasma memkind installation The memkind library depends on libnuma at the runtime, so it must already exist in the worker node system. Build the latest memkind lib from source: git clone -b v1.10.1 https://github.com/memkind/memkind cd memkind ./autogen.sh ./configure make make install vmemcache installation To build vmemcache library from source, you can (for RPM-based linux as example): git clone https://github.com/pmem/vmemcache cd vmemcache mkdir build cd build cmake .. -DCMAKE_INSTALL_PREFIX=/usr -DCPACK_GENERATOR=rpm make package sudo rpm -i libvmemcache*.rpm Plasma installation To use optimized Plasma cache with OAP, you need following components: (1) libarrow.so , libplasma.so , libplasma_java.so : dynamic libraries, will be used in Plasma client. (2) plasma-store-server : executable file, Plasma cache service. (3) arrow-plasma-0.17.0.jar : will be used when compile oap and spark runtime also need it. .so file and binary file Clone code from Intel-arrow repo and run following commands, this will install libplasma.so , libarrow.so , libplasma_java.so and plasma-store-server to your system path( /usr/lib64 by default). And if you are using Spark in a cluster environment, you can copy these files to all nodes in your cluster if the OS or distribution are same, otherwise, you need compile it on each node. cd /tmp git clone https://github.com/Intel-bigdata/arrow.git cd arrow && git checkout branch-0.17.0-oap-1.0 cd cpp mkdir release cd release #build libarrow, libplasma, libplasma_java cmake -DCMAKE_INSTALL_PREFIX=/usr/ -DCMAKE_BUILD_TYPE=Release -DARROW_BUILD_TESTS=on -DARROW_PLASMA_JAVA_CLIENT=on -DARROW_PLASMA=on -DARROW_DEPENDENCY_SOURCE=BUNDLED .. make -j$(nproc) sudo make install -j$(nproc) arrow-plasma-0.17.0.jar arrow-plasma-0.17.0.jar is provided in Maven central repo, you can download it and copy to $SPARK_HOME/jars dir. Or you can manually install it, run following command, this will install arrow jars to your local maven repo. Besides, you need copy arrow-plasma-0.17.0.jar to $SPARK_HOME/jars/ dir, cause this jar is needed when using external cache. cd /tmp/arrow/java mvn clean -q -pl plasma -DskipTests install Building the package You need to add -Ppersistent-memory to build with PMem support. For noevict cache strategy, you also need to build with -Ppersistent-memory parameter. mvn clean -q -pl com.intel.oap:oap-cache -am -Ppersistent-memory -DskipTests package For vmemcache cache strategy, please build with command: mvn clean -q -pl com.intel.oap:oap-cache -am -Pvmemcache -DskipTests package Build with this command to use all of them: mvn clean -q -pl com.intel.oap:oap-cache -am -Ppersistent-memory -Pvmemcache -DskipTests package Enabling NUMA binding for PMem in Spark Rebuilding Spark packages with NUMA binding patch When using PMem as a cache medium apply the NUMA binding patch numa-binding-spark-3.0.0.patch to Spark source code for best performance. Download src for Spark-3.0.0 and clone the src from github. Apply this patch and rebuild the Spark package. git apply numa-binding-spark-3.0.0.patch Add these configuration items to the Spark configuration file $SPARK_HOME/conf/spark-defaults.conf to enable NUMA binding. spark.yarn.numa.enabled true NOTE : If you are using a customized Spark, you will need to manually resolve the conflicts. *Other names and brands may be claimed as the property of others.","title":"Developer Guide"},{"location":"SQLDSCache/Developer-Guide/#developer-guide","text":"This document is a supplement to the whole OAP Developer Guide for SQL Index and Data Source Cache. After following that document, you can continue more details for SQL Index and Data Source Cache. Building Enabling NUMA binding for Intel\u00ae Optane\u2122 DC Persistent Memory in Spark","title":"Developer Guide"},{"location":"SQLDSCache/Developer-Guide/#building","text":"","title":"Building"},{"location":"SQLDSCache/Developer-Guide/#building-sql-index-and-data-source-cache","text":"Building with Apache Maven* . Clone the OAP project: git clone -b <tag-version> https://github.com/Intel-bigdata/OAP.git cd OAP Build the oap-cache package: mvn clean -pl com.intel.oap:oap-cache -am package","title":"Building SQL Index and  Data Source Cache"},{"location":"SQLDSCache/Developer-Guide/#running-tests","text":"Run all the tests: mvn clean -pl com.intel.oap:oap-cache -am test Run a specific test suite, for example OapDDLSuite : mvn -pl com.intel.oap:oap-cache -am -DwildcardSuites=org.apache.spark.sql.execution.datasources.oap.OapDDLSuite test NOTE : Log level of unit tests currently default to ERROR, please override oap-cache/oap/src/test/resources/log4j.properties if needed.","title":"Running Tests"},{"location":"SQLDSCache/Developer-Guide/#building-with-intel-optanetm-dc-persistent-memory-module","text":"","title":"Building with Intel\u00ae Optane\u2122 DC Persistent Memory Module"},{"location":"SQLDSCache/Developer-Guide/#prerequisites-for-building-with-pmem-support","text":"Install the required packages on the build system: cmake memkind vmemcache Plasma","title":"Prerequisites for building with PMem support"},{"location":"SQLDSCache/Developer-Guide/#memkind-installation","text":"The memkind library depends on libnuma at the runtime, so it must already exist in the worker node system. Build the latest memkind lib from source: git clone -b v1.10.1 https://github.com/memkind/memkind cd memkind ./autogen.sh ./configure make make install","title":"memkind installation"},{"location":"SQLDSCache/Developer-Guide/#vmemcache-installation","text":"To build vmemcache library from source, you can (for RPM-based linux as example): git clone https://github.com/pmem/vmemcache cd vmemcache mkdir build cd build cmake .. -DCMAKE_INSTALL_PREFIX=/usr -DCPACK_GENERATOR=rpm make package sudo rpm -i libvmemcache*.rpm","title":"vmemcache installation"},{"location":"SQLDSCache/Developer-Guide/#plasma-installation","text":"To use optimized Plasma cache with OAP, you need following components: (1) libarrow.so , libplasma.so , libplasma_java.so : dynamic libraries, will be used in Plasma client. (2) plasma-store-server : executable file, Plasma cache service. (3) arrow-plasma-0.17.0.jar : will be used when compile oap and spark runtime also need it. .so file and binary file Clone code from Intel-arrow repo and run following commands, this will install libplasma.so , libarrow.so , libplasma_java.so and plasma-store-server to your system path( /usr/lib64 by default). And if you are using Spark in a cluster environment, you can copy these files to all nodes in your cluster if the OS or distribution are same, otherwise, you need compile it on each node. cd /tmp git clone https://github.com/Intel-bigdata/arrow.git cd arrow && git checkout branch-0.17.0-oap-1.0 cd cpp mkdir release cd release #build libarrow, libplasma, libplasma_java cmake -DCMAKE_INSTALL_PREFIX=/usr/ -DCMAKE_BUILD_TYPE=Release -DARROW_BUILD_TESTS=on -DARROW_PLASMA_JAVA_CLIENT=on -DARROW_PLASMA=on -DARROW_DEPENDENCY_SOURCE=BUNDLED .. make -j$(nproc) sudo make install -j$(nproc) arrow-plasma-0.17.0.jar arrow-plasma-0.17.0.jar is provided in Maven central repo, you can download it and copy to $SPARK_HOME/jars dir. Or you can manually install it, run following command, this will install arrow jars to your local maven repo. Besides, you need copy arrow-plasma-0.17.0.jar to $SPARK_HOME/jars/ dir, cause this jar is needed when using external cache. cd /tmp/arrow/java mvn clean -q -pl plasma -DskipTests install","title":"Plasma installation"},{"location":"SQLDSCache/Developer-Guide/#building-the-package","text":"You need to add -Ppersistent-memory to build with PMem support. For noevict cache strategy, you also need to build with -Ppersistent-memory parameter. mvn clean -q -pl com.intel.oap:oap-cache -am -Ppersistent-memory -DskipTests package For vmemcache cache strategy, please build with command: mvn clean -q -pl com.intel.oap:oap-cache -am -Pvmemcache -DskipTests package Build with this command to use all of them: mvn clean -q -pl com.intel.oap:oap-cache -am -Ppersistent-memory -Pvmemcache -DskipTests package","title":"Building the package"},{"location":"SQLDSCache/Developer-Guide/#enabling-numa-binding-for-pmem-in-spark","text":"","title":"Enabling NUMA binding for PMem in Spark"},{"location":"SQLDSCache/Developer-Guide/#rebuilding-spark-packages-with-numa-binding-patch","text":"When using PMem as a cache medium apply the NUMA binding patch numa-binding-spark-3.0.0.patch to Spark source code for best performance. Download src for Spark-3.0.0 and clone the src from github. Apply this patch and rebuild the Spark package. git apply numa-binding-spark-3.0.0.patch Add these configuration items to the Spark configuration file $SPARK_HOME/conf/spark-defaults.conf to enable NUMA binding. spark.yarn.numa.enabled true NOTE : If you are using a customized Spark, you will need to manually resolve the conflicts.","title":"Rebuilding Spark packages with NUMA binding patch"},{"location":"SQLDSCache/Developer-Guide/#other-names-and-brands-may-be-claimed-as-the-property-of-others","text":"","title":"*Other names and brands may be claimed as the property of others."},{"location":"SQLDSCache/OAP-Developer-Guide/","text":"OAP Developer Guide This document contains the instructions & scripts on installing necessary dependencies and building OAP. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Arrow Data Source Native SQL Engine Building OAP Prerequisites for Building OAP is built with Apache Maven and Oracle Java 8, and mainly required tools to install on your cluster are listed below. Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. We provide scripts below to help automatically install dependencies above except RDMA , need change to root account, run: # git clone -b <tag-version> # cd oap-tools # sh dev/install-compile-time-dependencies.sh Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven Building To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building Specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache","title":"OAP Developer Guide"},{"location":"SQLDSCache/OAP-Developer-Guide/#oap-developer-guide","text":"This document contains the instructions & scripts on installing necessary dependencies and building OAP. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Arrow Data Source Native SQL Engine","title":"OAP Developer Guide"},{"location":"SQLDSCache/OAP-Developer-Guide/#building-oap","text":"","title":"Building OAP"},{"location":"SQLDSCache/OAP-Developer-Guide/#prerequisites-for-building","text":"OAP is built with Apache Maven and Oracle Java 8, and mainly required tools to install on your cluster are listed below. Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. We provide scripts below to help automatically install dependencies above except RDMA , need change to root account, run: # git clone -b <tag-version> # cd oap-tools # sh dev/install-compile-time-dependencies.sh Run the following command to learn more. # sh dev/scripts/prepare_oap_env.sh --help Run the following command to automatically install specific dependency such as Maven. # sh dev/scripts/prepare_oap_env.sh --prepare_maven","title":"Prerequisites for Building"},{"location":"SQLDSCache/OAP-Developer-Guide/#building","text":"To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package . $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh Building Specified OAP Module, such as sql-ds-cache , run: $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache","title":"Building"},{"location":"SQLDSCache/OAP-Installation-Guide/","text":"OAP Installation Guide This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine. Contents Prerequisites Installing OAP Configuration Prerequisites OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly. Installing OAP Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Create a conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.0.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Extra Steps for Shuffle Remote PMem Extension If you use one of OAP features -- PMmem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details. Configuration Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar And then you can follow the corresponding feature documents for more details to use them.","title":"OAP Installation Guide"},{"location":"SQLDSCache/OAP-Installation-Guide/#oap-installation-guide","text":"This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine.","title":"OAP Installation Guide"},{"location":"SQLDSCache/OAP-Installation-Guide/#contents","text":"Prerequisites Installing OAP Configuration","title":"Contents"},{"location":"SQLDSCache/OAP-Installation-Guide/#prerequisites","text":"OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly.","title":"Prerequisites"},{"location":"SQLDSCache/OAP-Installation-Guide/#installing-oap","text":"Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Create a conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.0.0 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars","title":"Installing OAP"},{"location":"SQLDSCache/OAP-Installation-Guide/#extra-steps-for-shuffle-remote-pmem-extension","text":"If you use one of OAP features -- PMmem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details.","title":"Extra Steps for Shuffle Remote PMem Extension"},{"location":"SQLDSCache/OAP-Installation-Guide/#configuration","text":"Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar And then you can follow the corresponding feature documents for more details to use them.","title":"Configuration"},{"location":"SQLDSCache/User-Guide/","text":"User Guide Prerequisites Getting Started Configuration for YARN Cluster Mode Configuration for Spark Standalone Mode Working with SQL Index Working with SQL Data Source Cache Run TPC-DS Benchmark Advanced Configuration Prerequisites SQL Index and Data Source Cache on Spark 3.0.0 requires a working Hadoop cluster with YARN and Spark. Running Spark on YARN requires a binary distribution of Spark, which is built with YARN support. Getting Started Building We have provided a Conda package which will automatically install dependencies and build OAP jars, please follow OAP-Installation-Guide and you can find compiled OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars once finished the installation. If you\u2019d like to build from source code, please refer to Developer Guide for the detailed steps. Spark Configurations Users usually test and run Spark SQL or Scala scripts in Spark Shell, which launches Spark applications on YRAN with client mode. In this section, we will start with Spark Shell then introduce other use scenarios. Before you run $SPARK_HOME/bin/spark-shell , you need to configure Spark for integration. You need to add or update the following configurations in the Spark configuration file $SPARK_HOME/conf/spark-defaults.conf on your working node. spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar Verify Integration After configuration, you can follow these steps to verify the OAP integration is working using Spark Shell. Create a test data path on your HDFS. hdfs:///user/oap/ for example. hadoop fs -mkdir /user/oap/ Launch Spark Shell using the following command on your working node. $SPARK_HOME/bin/spark-shell Execute the following commands in Spark Shell to test OAP integration. > spark.sql(s\"\"\"CREATE TABLE oap_test (a INT, b STRING) USING parquet OPTIONS (path 'hdfs:///user/oap/')\"\"\".stripMargin) > val data = (1 to 30000).map { i => (i, s\"this is test $i\") }.toDF().createOrReplaceTempView(\"t\") > spark.sql(\"insert overwrite table oap_test select * from t\") > spark.sql(\"create oindex index1 on oap_test (a)\") > spark.sql(\"show oindex from oap_test\").show() This test creates an index for a table and then shows it. If there are no errors, the OAP .jar is working with the configuration. The picture below is an example of a successfully run. Configuration for YARN Cluster Mode Spark Shell, Spark SQL CLI and Thrift Sever run Spark application in client mode. While Spark Submit tool can run Spark application in client or cluster mode, which is decided by --deploy-mode parameter. Getting Started session has shown the configurations needed for client mode. If you are running Spark Submit tool in cluster mode, you need to follow the below configuration steps instead. Add the following OAP configuration settings to $SPARK_HOME/conf/spark-defaults.conf on your working node before running spark-submit in cluster mode. spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path on your working node spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.driver.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar Configuration for Spark Standalone Mode In addition to running on the YARN cluster manager, Spark also provides a simple standalone deploy mode. If you are using Spark in Spark Standalone mode: Make sure the OAP .jar at the same path of all the worker nodes. Add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf on the working node. spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path on worker nodes spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # absolute path on worker nodes spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar Working with SQL Index After a successful OAP integration, you can use OAP SQL DDL to manage table indexes. The DDL operations include index create , drop , refresh , and show . Test these functions using the following examples in Spark Shell. > spark.sql(s\"\"\"CREATE TABLE oap_test (a INT, b STRING) USING parquet OPTIONS (path 'hdfs:///user/oap/')\"\"\".stripMargin) > val data = (1 to 30000).map { i => (i, s\"this is test $i\") }.toDF().createOrReplaceTempView(\"t\") > spark.sql(\"insert overwrite table oap_test select * from t\") Index Creation Use the CREATE OINDEX DDL command to create a B+ Tree index or bitmap index. CREATE OINDEX index_name ON table_name (column_name) USING [BTREE, BITMAP] The following example creates a B+ Tree index on column \"a\" of the oap_test table. > spark.sql(\"create oindex index1 on oap_test (a)\") Use SHOW OINDEX command to show all the created indexes on a specified table. > spark.sql(\"show oindex from oap_test\").show() Use Index Using index in a query is transparent. When SQL queries have filter conditions on the column(s) which can take advantage of the index to filter the data scan, the index will automatically be applied to the execution of Spark SQL. The following example will automatically use the underlayer index created on column \"a\". > spark.sql(\"SELECT * FROM oap_test WHERE a = 1\").show() Drop index Use DROP OINDEX command to drop a named index. > spark.sql(\"drop oindex index1 on oap_test\") Working with SQL Data Source Cache Data Source Cache can provide input data cache functionality to the executor. When using the cache data among different SQL queries, configure cache to allow different SQL queries to use the same executor process. Do this by running your queries through the Spark ThriftServer as shown below. For cache media, we support both DRAM and Intel PMem which means you can choose to cache data in DRAM or Intel PMem if you have PMem configured in hardware. Use DRAM Cache Make the following configuration changes in Spark configuration file $SPARK_HOME/conf/spark-defaults.conf . spark.memory.offHeap.enabled false spark.oap.cache.strategy guava spark.sql.oap.cache.memory.manager offheap # according to the resource of cluster spark.executor.memoryOverhead 50g # equal to the size of executor.memoryOverhead spark.executor.sql.oap.cache.offheap.memory.size 50g # for parquet fileformat, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for orc fileformat, enable binary cache spark.sql.oap.orc.binary.cache.enabled true NOTE : Change spark.executor.sql.oap.cache.offheap.memory.size based on the availability of DRAM capacity to cache data, and its size is equal to spark.executor.memoryOverhead Launch Spark ThriftServer Launch Spark Thrift Server, and use the Beeline command line tool to connect to the Thrift Server to execute DDL or DML operations. The data cache will automatically take effect for Parquet or ORC file sources. The rest of this section will show you how to do a quick verification of cache functionality. It will reuse the database metastore created in the Working with SQL Index section, which creates the oap_test table definition. In production, Spark Thrift Server will have its own metastore database directory or metastore service and use DDL's through Beeline for creating your tables. When you run spark-shell to create the oap_test table, metastore_db will be created in the directory where you ran '$SPARK_HOME/bin/spark-shell'. Go to that directory and execute the following command to launch Thrift JDBC server and run queries. $SPARK_HOME/sbin/start-thriftserver.sh Use Beeline and connect to the Thrift JDBC server, replacing the hostname (mythriftserver) with your own Thrift Server hostname. . $SPARK_HOME/bin/beeline -u jdbc:hive2://<mythriftserver>:10000 After the connection is established, execute the following commands to check the metastore is initialized correctly. > SHOW databases; > USE default; > SHOW tables; Run queries on the table that will use the cache automatically. For example, > SELECT * FROM oap_test WHERE a = 1; > SELECT * FROM oap_test WHERE a = 2; > SELECT * FROM oap_test WHERE a = 3; ... Open the Spark History Web UI and go to the OAP tab page to verify the cache metrics. The following picture is an example. Use PMem Cache Prerequisites The following steps are required to configure OAP to use PMem cache with external cache strategy. PMem hardware is successfully deployed on each node in cluster. Besides, when enabling SQL Data Source Cache with external cache using Plasma, PMem could get noticeable performance gain with BIOS configuration settings below, especially on cross socket write path. Socket Configuration -> Memory Configuration -> NGN Configuration -> Snoopy mode for AD : Enabled Socket Configuration -> Intel UPI General Configuration -> Stale AtoS : Disabled It's strongly advised to use Linux device mapper to interleave PMem across sockets and get maximum size for Plasma. // use ipmctl command to show topology and dimm info of PMem ipmctl show -topology ipmctl show -dimm // provision PMem in app direct mode ipmctl create -goal PersistentMemoryType=AppDirect // reboot system to make configuration take affect reboot // check capacity provisioned for app direct mode(AppDirectCapacity) ipmctl show -memoryresources // show the PMem region information ipmctl show -region // create namespace based on the region, multi namespaces can be created on a single region ndctl create-namespace -m fsdax -r region0 ndctl create-namespace -m fsdax -r region1 // show the created namespaces fdisk -l // create and mount file system sudo dmsetup create striped-pmem mkfs.ext4 -b 4096 -E stride=512 -F /dev/mapper/striped-pmem mkdir -p /mnt/pmem mount -o dax /dev/mapper/striped-pmem /mnt/pmem For more information you can refer to Quick Start Guide: Provision Intel\u00ae Optane\u2122 DC Persistent Memory SQL Data Source Cache uses Plasma as a node-level external cache service, the benefit of using external cache is data could be shared across process boundaries. Plasma is a high-performance shared-memory object store and a component of Apache Arrow . We have modified Plasma to support PMem, and make it open source on Intel-bigdata Arrow repo. If you have finished OAP Installation Guide , Plasma will be automatically installed and then you just need copy arrow-plasma-0.17.0.jar to $SPARK_HOME/jars . For manual building and installation steps you can refer to Plasma installation . Refer to configuration below to apply external cache strategy and start Plasma service on each node and start your workload. Configuration for NUMA Install numactl to bind the executor to the PMem device on the same NUMA node. yum install numactl -y We recommend you use NUMA-patched Spark to achieve better performance gain for the external strategy compared with Community Spark. Build Spark from source to enable NUMA-binding support, refer to Enabling-NUMA-binding-for-PMem-in-Spark . Configuration for enabling PMem cache Add the following configuration to $SPARK_HOME/conf/spark-defaults.conf . # 2x number of your worker nodes spark.executor.instances 6 # enable numa spark.yarn.numa.enabled true # enable SQL Index and Data Source Cache extension in Spark spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node, when in Yarn client mode spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir, when in Yarn client mode spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node,when in Yarn client mode spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # for parquet file format, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for ORC file format, enable binary cache spark.sql.oap.orc.binary.cache.enabled true # enable external cache strategy spark.oap.cache.strategy external spark.sql.oap.dcpmm.free.wait.threshold 50000000000 # according to your executor core number spark.executor.sql.oap.cache.external.client.pool.size 10 Start Plasma service manually Plasma config parameters: -m how much Bytes share memory Plasma will use -s Unix Domain sockcet path -d PMem directory Start Plasma service on each node with following command, then run your workload. If you install OAP by Conda, you can find plasma-store-server in the path $HOME/miniconda2/envs/oapenv/bin/ . ./plasma-store-server -m 15000000000 -s /tmp/plasmaStore -d /mnt/pmem Remember to kill plasma-store-server process if you no longer need cache, and you should delete /tmp/plasmaStore which is a Unix domain socket. Use Yarn to start Plamsa service When using Yarn(Hadoop version >= 3.1) to start Plasma service, you should provide a json file as below. { \"name\": \"plasma-store-service\", \"version\": 1, \"components\" : [ { \"name\": \"plasma-store-service\", \"number_of_containers\": 3, \"launch_command\": \"plasma-store-server -m 15000000000 -s /tmp/plasmaStore -d /mnt/pmem\", \"resource\": { \"cpus\": 1, \"memory\": 512 } } ] } Run command yarn app -launch plasma-store-service /tmp/plasmaLaunch.json to start Plasma server. Run yarn app -stop plasma-store-service to stop it. Run yarn app -destroy plasma-store-service to destroy it. Verify PMem cache functionality After finishing configuration, restart Spark Thrift Server for the configuration changes to take effect. Start at step 2 of the Use DRAM Cache guide to verify that cache is working correctly. Check PMem cache size by checking disk space with df -h . Run TPC-DS Benchmark This section provides instructions and tools for running TPC-DS queries to evaluate the cache performance of various configurations. The TPC-DS suite has many queries and we select 9 I/O intensive queries to simplify performance evaluation. We created some tool scripts oap-benchmark-tool.zip to simplify running the workload. If you are already familiar with TPC-DS data generation and running a TPC-DS tool suite, skip our tool and use the TPC-DS tool suite directly. Prerequisites Python 2.7+ is required on the working node. Prepare the Tool Download oap-benchmark-tool.zip and unzip to a folder (for example, oap-benchmark-tool folder) on your working node. Copy oap-benchmark-tool/tools/tpcds-kits to ALL worker nodes under the same folder (for example, /home/oap/tpcds-kits ). Generate TPC-DS Data Update the values for the following variables in oap-benchmark-tool/scripts/tool.conf based on your environment and needs. SPARK_HOME: Point to the Spark home directory of your Spark setup. TPCDS_KITS_DIR: The tpcds-kits directory you coped to the worker nodes in the above prepare process. For example, /home/oap/tpcds-kits NAMENODE_ADDRESS: Your HDFS Namenode address in the format of host:port. THRIFT_SERVER_ADDRESS: Your working node address on which you will run Thrift Server. DATA_SCALE: The data scale to be generated in GB DATA_FORMAT: The data file format. You can specify parquet or orc For example: export SPARK_HOME=/home/oap/spark-3.0.0 export TPCDS_KITS_DIR=/home/oap/tpcds-kits export NAMENODE_ADDRESS=mynamenode:9000 export THRIFT_SERVER_ADDRESS=mythriftserver export DATA_SCALE=1024 export DATA_FORMAT=parquet Start data generation. In the root directory of this tool ( oap-benchmark-tool ), run scripts/run_gen_data.sh to start the data generation process. cd oap-benchmark-tool sh ./scripts/run_gen_data.sh Once finished, the $scale data will be generated in the HDFS folder genData$scale . And a database called tpcds_$format$scale will contain the TPC-DS tables. Start Spark Thrift Server Start the Thrift Server in the tool root folder, which is the same folder you run data generation scripts. Use either the PMem or DRAM script to start the Thrift Server. Use PMem as Cache Media Update the configuration values in scripts/spark_thrift_server_yarn_with_PMem.sh to reflect your environment. Normally, you need to update the following configuration values to cache to PMem. --num-executors --driver-memory --executor-memory --executor-cores --conf spark.oap.cache.strategy --conf spark.sql.oap.dcpmm.free.wait.threshold --conf spark.executor.sql.oap.cache.external.client.pool.size These settings will override the values specified in Spark configuration file ( spark-defaults.conf ). After the configuration is done, you can execute the following command to start Thrift Server. cd oap-benchmark-tool sh ./scripts/spark_thrift_server_yarn_with_PMem.sh start In this script, we use external as cache strategy for Parquet Binary data cache. Use DRAM as Cache Media Update the configuration values in scripts/spark_thrift_server_yarn_with_DRAM.sh to reflect your environment. Normally, you need to update the following configuration values to cache to DRAM. --num-executors --driver-memory --executor-memory --executor-cores --conf spark.executor.sql.oap.cache.offheap.memory.size --conf spark.executor.memoryOverhead These settings will override the values specified in Spark configuration file ( spark-defaults.conf ). After the configuration is done, you can execute the following command to start Thrift Server. cd oap-benchmark-tool sh ./scripts/spark_thrift_server_yarn_with_DRAM.sh start Run Queries Execute the following command to start to run queries. If you use external cache strategy, also need start plasma service manually as above. cd oap-benchmark-tool sh ./scripts/run_tpcds.sh When all the queries are done, you will see the result.json file in the current directory. You will find in the 2nd and 3rd round, cache feature takes effect and query time becomes less. And the Spark webUI OAP tab has more specific OAP cache metrics just as section step 5. Advanced Configuration Additional Cache Strategies In addition to external cache strategy, SQL Data Source Cache also supports 3 other cache strategies: guava , noevict and vmemcache . Index and Data Cache Separation To optimize the cache media utilization, SQL Data Source Cache supports cache separation of data and index, by using same or different cache media with DRAM and PMem. Cache Hot Tables Data Source Cache also supports caching specific tables according to actual situations, these tables are usually hot tables. Column Vector Cache This document above uses binary cache as example for Parquet file format, if your cluster memory resources is abundant enough, you can choose ColumnVector data cache instead of binary cache for Parquet to spare computation time. Large Scale and Heterogeneous Cluster Support Introduce an external database to store cache locality info to support large-scale and heterogeneous clusters. For more information and configuration details, please refer to Advanced Configuration .","title":"User Guide"},{"location":"SQLDSCache/User-Guide/#user-guide","text":"Prerequisites Getting Started Configuration for YARN Cluster Mode Configuration for Spark Standalone Mode Working with SQL Index Working with SQL Data Source Cache Run TPC-DS Benchmark Advanced Configuration","title":"User Guide"},{"location":"SQLDSCache/User-Guide/#prerequisites","text":"SQL Index and Data Source Cache on Spark 3.0.0 requires a working Hadoop cluster with YARN and Spark. Running Spark on YARN requires a binary distribution of Spark, which is built with YARN support.","title":"Prerequisites"},{"location":"SQLDSCache/User-Guide/#getting-started","text":"","title":"Getting Started"},{"location":"SQLDSCache/User-Guide/#building","text":"We have provided a Conda package which will automatically install dependencies and build OAP jars, please follow OAP-Installation-Guide and you can find compiled OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars once finished the installation. If you\u2019d like to build from source code, please refer to Developer Guide for the detailed steps.","title":"Building"},{"location":"SQLDSCache/User-Guide/#spark-configurations","text":"Users usually test and run Spark SQL or Scala scripts in Spark Shell, which launches Spark applications on YRAN with client mode. In this section, we will start with Spark Shell then introduce other use scenarios. Before you run $SPARK_HOME/bin/spark-shell , you need to configure Spark for integration. You need to add or update the following configurations in the Spark configuration file $SPARK_HOME/conf/spark-defaults.conf on your working node. spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar","title":"Spark Configurations"},{"location":"SQLDSCache/User-Guide/#verify-integration","text":"After configuration, you can follow these steps to verify the OAP integration is working using Spark Shell. Create a test data path on your HDFS. hdfs:///user/oap/ for example. hadoop fs -mkdir /user/oap/ Launch Spark Shell using the following command on your working node. $SPARK_HOME/bin/spark-shell Execute the following commands in Spark Shell to test OAP integration. > spark.sql(s\"\"\"CREATE TABLE oap_test (a INT, b STRING) USING parquet OPTIONS (path 'hdfs:///user/oap/')\"\"\".stripMargin) > val data = (1 to 30000).map { i => (i, s\"this is test $i\") }.toDF().createOrReplaceTempView(\"t\") > spark.sql(\"insert overwrite table oap_test select * from t\") > spark.sql(\"create oindex index1 on oap_test (a)\") > spark.sql(\"show oindex from oap_test\").show() This test creates an index for a table and then shows it. If there are no errors, the OAP .jar is working with the configuration. The picture below is an example of a successfully run.","title":"Verify Integration"},{"location":"SQLDSCache/User-Guide/#configuration-for-yarn-cluster-mode","text":"Spark Shell, Spark SQL CLI and Thrift Sever run Spark application in client mode. While Spark Submit tool can run Spark application in client or cluster mode, which is decided by --deploy-mode parameter. Getting Started session has shown the configurations needed for client mode. If you are running Spark Submit tool in cluster mode, you need to follow the below configuration steps instead. Add the following OAP configuration settings to $SPARK_HOME/conf/spark-defaults.conf on your working node before running spark-submit in cluster mode. spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path on your working node spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.driver.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar","title":"Configuration for YARN Cluster Mode"},{"location":"SQLDSCache/User-Guide/#configuration-for-spark-standalone-mode","text":"In addition to running on the YARN cluster manager, Spark also provides a simple standalone deploy mode. If you are using Spark in Spark Standalone mode: Make sure the OAP .jar at the same path of all the worker nodes. Add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf on the working node. spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path on worker nodes spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # absolute path on worker nodes spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar","title":"Configuration for Spark Standalone Mode"},{"location":"SQLDSCache/User-Guide/#working-with-sql-index","text":"After a successful OAP integration, you can use OAP SQL DDL to manage table indexes. The DDL operations include index create , drop , refresh , and show . Test these functions using the following examples in Spark Shell. > spark.sql(s\"\"\"CREATE TABLE oap_test (a INT, b STRING) USING parquet OPTIONS (path 'hdfs:///user/oap/')\"\"\".stripMargin) > val data = (1 to 30000).map { i => (i, s\"this is test $i\") }.toDF().createOrReplaceTempView(\"t\") > spark.sql(\"insert overwrite table oap_test select * from t\")","title":"Working with SQL Index"},{"location":"SQLDSCache/User-Guide/#index-creation","text":"Use the CREATE OINDEX DDL command to create a B+ Tree index or bitmap index. CREATE OINDEX index_name ON table_name (column_name) USING [BTREE, BITMAP] The following example creates a B+ Tree index on column \"a\" of the oap_test table. > spark.sql(\"create oindex index1 on oap_test (a)\") Use SHOW OINDEX command to show all the created indexes on a specified table. > spark.sql(\"show oindex from oap_test\").show()","title":"Index Creation"},{"location":"SQLDSCache/User-Guide/#use-index","text":"Using index in a query is transparent. When SQL queries have filter conditions on the column(s) which can take advantage of the index to filter the data scan, the index will automatically be applied to the execution of Spark SQL. The following example will automatically use the underlayer index created on column \"a\". > spark.sql(\"SELECT * FROM oap_test WHERE a = 1\").show()","title":"Use Index"},{"location":"SQLDSCache/User-Guide/#drop-index","text":"Use DROP OINDEX command to drop a named index. > spark.sql(\"drop oindex index1 on oap_test\")","title":"Drop index"},{"location":"SQLDSCache/User-Guide/#working-with-sql-data-source-cache","text":"Data Source Cache can provide input data cache functionality to the executor. When using the cache data among different SQL queries, configure cache to allow different SQL queries to use the same executor process. Do this by running your queries through the Spark ThriftServer as shown below. For cache media, we support both DRAM and Intel PMem which means you can choose to cache data in DRAM or Intel PMem if you have PMem configured in hardware.","title":"Working with SQL Data Source Cache"},{"location":"SQLDSCache/User-Guide/#use-dram-cache","text":"Make the following configuration changes in Spark configuration file $SPARK_HOME/conf/spark-defaults.conf . spark.memory.offHeap.enabled false spark.oap.cache.strategy guava spark.sql.oap.cache.memory.manager offheap # according to the resource of cluster spark.executor.memoryOverhead 50g # equal to the size of executor.memoryOverhead spark.executor.sql.oap.cache.offheap.memory.size 50g # for parquet fileformat, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for orc fileformat, enable binary cache spark.sql.oap.orc.binary.cache.enabled true NOTE : Change spark.executor.sql.oap.cache.offheap.memory.size based on the availability of DRAM capacity to cache data, and its size is equal to spark.executor.memoryOverhead Launch Spark ThriftServer Launch Spark Thrift Server, and use the Beeline command line tool to connect to the Thrift Server to execute DDL or DML operations. The data cache will automatically take effect for Parquet or ORC file sources. The rest of this section will show you how to do a quick verification of cache functionality. It will reuse the database metastore created in the Working with SQL Index section, which creates the oap_test table definition. In production, Spark Thrift Server will have its own metastore database directory or metastore service and use DDL's through Beeline for creating your tables. When you run spark-shell to create the oap_test table, metastore_db will be created in the directory where you ran '$SPARK_HOME/bin/spark-shell'. Go to that directory and execute the following command to launch Thrift JDBC server and run queries. $SPARK_HOME/sbin/start-thriftserver.sh Use Beeline and connect to the Thrift JDBC server, replacing the hostname (mythriftserver) with your own Thrift Server hostname. . $SPARK_HOME/bin/beeline -u jdbc:hive2://<mythriftserver>:10000 After the connection is established, execute the following commands to check the metastore is initialized correctly. > SHOW databases; > USE default; > SHOW tables; Run queries on the table that will use the cache automatically. For example, > SELECT * FROM oap_test WHERE a = 1; > SELECT * FROM oap_test WHERE a = 2; > SELECT * FROM oap_test WHERE a = 3; ... Open the Spark History Web UI and go to the OAP tab page to verify the cache metrics. The following picture is an example.","title":"Use DRAM Cache"},{"location":"SQLDSCache/User-Guide/#use-pmem-cache","text":"","title":"Use PMem Cache"},{"location":"SQLDSCache/User-Guide/#prerequisites_1","text":"The following steps are required to configure OAP to use PMem cache with external cache strategy. PMem hardware is successfully deployed on each node in cluster. Besides, when enabling SQL Data Source Cache with external cache using Plasma, PMem could get noticeable performance gain with BIOS configuration settings below, especially on cross socket write path. Socket Configuration -> Memory Configuration -> NGN Configuration -> Snoopy mode for AD : Enabled Socket Configuration -> Intel UPI General Configuration -> Stale AtoS : Disabled It's strongly advised to use Linux device mapper to interleave PMem across sockets and get maximum size for Plasma. // use ipmctl command to show topology and dimm info of PMem ipmctl show -topology ipmctl show -dimm // provision PMem in app direct mode ipmctl create -goal PersistentMemoryType=AppDirect // reboot system to make configuration take affect reboot // check capacity provisioned for app direct mode(AppDirectCapacity) ipmctl show -memoryresources // show the PMem region information ipmctl show -region // create namespace based on the region, multi namespaces can be created on a single region ndctl create-namespace -m fsdax -r region0 ndctl create-namespace -m fsdax -r region1 // show the created namespaces fdisk -l // create and mount file system sudo dmsetup create striped-pmem mkfs.ext4 -b 4096 -E stride=512 -F /dev/mapper/striped-pmem mkdir -p /mnt/pmem mount -o dax /dev/mapper/striped-pmem /mnt/pmem For more information you can refer to Quick Start Guide: Provision Intel\u00ae Optane\u2122 DC Persistent Memory SQL Data Source Cache uses Plasma as a node-level external cache service, the benefit of using external cache is data could be shared across process boundaries. Plasma is a high-performance shared-memory object store and a component of Apache Arrow . We have modified Plasma to support PMem, and make it open source on Intel-bigdata Arrow repo. If you have finished OAP Installation Guide , Plasma will be automatically installed and then you just need copy arrow-plasma-0.17.0.jar to $SPARK_HOME/jars . For manual building and installation steps you can refer to Plasma installation . Refer to configuration below to apply external cache strategy and start Plasma service on each node and start your workload.","title":"Prerequisites"},{"location":"SQLDSCache/User-Guide/#configuration-for-numa","text":"Install numactl to bind the executor to the PMem device on the same NUMA node. yum install numactl -y We recommend you use NUMA-patched Spark to achieve better performance gain for the external strategy compared with Community Spark. Build Spark from source to enable NUMA-binding support, refer to Enabling-NUMA-binding-for-PMem-in-Spark .","title":"Configuration for NUMA"},{"location":"SQLDSCache/User-Guide/#configuration-for-enabling-pmem-cache","text":"Add the following configuration to $SPARK_HOME/conf/spark-defaults.conf . # 2x number of your worker nodes spark.executor.instances 6 # enable numa spark.yarn.numa.enabled true # enable SQL Index and Data Source Cache extension in Spark spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node, when in Yarn client mode spark.files $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir, when in Yarn client mode spark.executor.extraClassPath ./oap-cache-<version>-with-spark-<version>.jar:./oap-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node,when in Yarn client mode spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-<version>-with-spark-<version>.jar # for parquet file format, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for ORC file format, enable binary cache spark.sql.oap.orc.binary.cache.enabled true # enable external cache strategy spark.oap.cache.strategy external spark.sql.oap.dcpmm.free.wait.threshold 50000000000 # according to your executor core number spark.executor.sql.oap.cache.external.client.pool.size 10 Start Plasma service manually Plasma config parameters: -m how much Bytes share memory Plasma will use -s Unix Domain sockcet path -d PMem directory Start Plasma service on each node with following command, then run your workload. If you install OAP by Conda, you can find plasma-store-server in the path $HOME/miniconda2/envs/oapenv/bin/ . ./plasma-store-server -m 15000000000 -s /tmp/plasmaStore -d /mnt/pmem Remember to kill plasma-store-server process if you no longer need cache, and you should delete /tmp/plasmaStore which is a Unix domain socket. Use Yarn to start Plamsa service When using Yarn(Hadoop version >= 3.1) to start Plasma service, you should provide a json file as below. { \"name\": \"plasma-store-service\", \"version\": 1, \"components\" : [ { \"name\": \"plasma-store-service\", \"number_of_containers\": 3, \"launch_command\": \"plasma-store-server -m 15000000000 -s /tmp/plasmaStore -d /mnt/pmem\", \"resource\": { \"cpus\": 1, \"memory\": 512 } } ] } Run command yarn app -launch plasma-store-service /tmp/plasmaLaunch.json to start Plasma server. Run yarn app -stop plasma-store-service to stop it. Run yarn app -destroy plasma-store-service to destroy it.","title":"Configuration for enabling PMem cache"},{"location":"SQLDSCache/User-Guide/#verify-pmem-cache-functionality","text":"After finishing configuration, restart Spark Thrift Server for the configuration changes to take effect. Start at step 2 of the Use DRAM Cache guide to verify that cache is working correctly. Check PMem cache size by checking disk space with df -h .","title":"Verify PMem cache functionality"},{"location":"SQLDSCache/User-Guide/#run-tpc-ds-benchmark","text":"This section provides instructions and tools for running TPC-DS queries to evaluate the cache performance of various configurations. The TPC-DS suite has many queries and we select 9 I/O intensive queries to simplify performance evaluation. We created some tool scripts oap-benchmark-tool.zip to simplify running the workload. If you are already familiar with TPC-DS data generation and running a TPC-DS tool suite, skip our tool and use the TPC-DS tool suite directly.","title":"Run TPC-DS Benchmark"},{"location":"SQLDSCache/User-Guide/#prerequisites_2","text":"Python 2.7+ is required on the working node.","title":"Prerequisites"},{"location":"SQLDSCache/User-Guide/#prepare-the-tool","text":"Download oap-benchmark-tool.zip and unzip to a folder (for example, oap-benchmark-tool folder) on your working node. Copy oap-benchmark-tool/tools/tpcds-kits to ALL worker nodes under the same folder (for example, /home/oap/tpcds-kits ).","title":"Prepare the Tool"},{"location":"SQLDSCache/User-Guide/#generate-tpc-ds-data","text":"Update the values for the following variables in oap-benchmark-tool/scripts/tool.conf based on your environment and needs. SPARK_HOME: Point to the Spark home directory of your Spark setup. TPCDS_KITS_DIR: The tpcds-kits directory you coped to the worker nodes in the above prepare process. For example, /home/oap/tpcds-kits NAMENODE_ADDRESS: Your HDFS Namenode address in the format of host:port. THRIFT_SERVER_ADDRESS: Your working node address on which you will run Thrift Server. DATA_SCALE: The data scale to be generated in GB DATA_FORMAT: The data file format. You can specify parquet or orc For example: export SPARK_HOME=/home/oap/spark-3.0.0 export TPCDS_KITS_DIR=/home/oap/tpcds-kits export NAMENODE_ADDRESS=mynamenode:9000 export THRIFT_SERVER_ADDRESS=mythriftserver export DATA_SCALE=1024 export DATA_FORMAT=parquet Start data generation. In the root directory of this tool ( oap-benchmark-tool ), run scripts/run_gen_data.sh to start the data generation process. cd oap-benchmark-tool sh ./scripts/run_gen_data.sh Once finished, the $scale data will be generated in the HDFS folder genData$scale . And a database called tpcds_$format$scale will contain the TPC-DS tables.","title":"Generate TPC-DS Data"},{"location":"SQLDSCache/User-Guide/#start-spark-thrift-server","text":"Start the Thrift Server in the tool root folder, which is the same folder you run data generation scripts. Use either the PMem or DRAM script to start the Thrift Server.","title":"Start Spark Thrift Server"},{"location":"SQLDSCache/User-Guide/#use-pmem-as-cache-media","text":"Update the configuration values in scripts/spark_thrift_server_yarn_with_PMem.sh to reflect your environment. Normally, you need to update the following configuration values to cache to PMem. --num-executors --driver-memory --executor-memory --executor-cores --conf spark.oap.cache.strategy --conf spark.sql.oap.dcpmm.free.wait.threshold --conf spark.executor.sql.oap.cache.external.client.pool.size These settings will override the values specified in Spark configuration file ( spark-defaults.conf ). After the configuration is done, you can execute the following command to start Thrift Server. cd oap-benchmark-tool sh ./scripts/spark_thrift_server_yarn_with_PMem.sh start In this script, we use external as cache strategy for Parquet Binary data cache.","title":"Use PMem as Cache Media"},{"location":"SQLDSCache/User-Guide/#use-dram-as-cache-media","text":"Update the configuration values in scripts/spark_thrift_server_yarn_with_DRAM.sh to reflect your environment. Normally, you need to update the following configuration values to cache to DRAM. --num-executors --driver-memory --executor-memory --executor-cores --conf spark.executor.sql.oap.cache.offheap.memory.size --conf spark.executor.memoryOverhead These settings will override the values specified in Spark configuration file ( spark-defaults.conf ). After the configuration is done, you can execute the following command to start Thrift Server. cd oap-benchmark-tool sh ./scripts/spark_thrift_server_yarn_with_DRAM.sh start","title":"Use DRAM as Cache Media"},{"location":"SQLDSCache/User-Guide/#run-queries","text":"Execute the following command to start to run queries. If you use external cache strategy, also need start plasma service manually as above. cd oap-benchmark-tool sh ./scripts/run_tpcds.sh When all the queries are done, you will see the result.json file in the current directory. You will find in the 2nd and 3rd round, cache feature takes effect and query time becomes less. And the Spark webUI OAP tab has more specific OAP cache metrics just as section step 5.","title":"Run Queries"},{"location":"SQLDSCache/User-Guide/#advanced-configuration","text":"Additional Cache Strategies In addition to external cache strategy, SQL Data Source Cache also supports 3 other cache strategies: guava , noevict and vmemcache . Index and Data Cache Separation To optimize the cache media utilization, SQL Data Source Cache supports cache separation of data and index, by using same or different cache media with DRAM and PMem. Cache Hot Tables Data Source Cache also supports caching specific tables according to actual situations, these tables are usually hot tables. Column Vector Cache This document above uses binary cache as example for Parquet file format, if your cluster memory resources is abundant enough, you can choose ColumnVector data cache instead of binary cache for Parquet to spare computation time. Large Scale and Heterogeneous Cluster Support Introduce an external database to store cache locality info to support large-scale and heterogeneous clusters. For more information and configuration details, please refer to Advanced Configuration .","title":"Advanced Configuration"}]}